{
  "/": {
    "title": "_index",
    "content": "+++\ntitle = 'Home'\ndate = 2023-01-01T08:00:00-07:00\ncategories = []\ntags = []\ntoc = true\nmath = true\n+++\n\nI've graduated from the [International Institute of Information Technology - Hyderabad](https://www.iiit.ac.in/) and will be starting my professional career at [Databricks](https://www.databricks.com/). My interests span performance optimization, algorithms research, compilers, quantitative finance and competitive programming. I've contributed to projects on implementing C++ stdlib experimental extensions and worked on optimizing BLAS and bio-informatics algorithms like Needleman-Wunsch and the STAR DNA sequence aligner.    \n\u003cbr/\u003e\nLately, I've been exploring options pricing theory and ML theory. I intermittently write up my thoughts and notes on things I've learnt in my blog. If you are interested in my work, or would like to casually chat about research or CP or one of my other interests, feel free to reach out to me!    \n\n\u003cbr/\u003e\n\n# My Vault \n\n\u003cbr/\u003e\n",
    "lastmodified": "2024-05-30T05:07:16.540301761+05:30",
    "tags": []
  },
  "/blog/a-deep-dive-into-the-knapsack-problem": {
    "title": "A Deep Dive Into the Knapsack Problem",
    "content": "After discussing [DP as DAGs,  Shortest path on DAGs \u0026 LIS in O(nlogn)](/blog/dp-as-dags-shortest-path-on-dags-lis-in-o-nlogn), [Levenshtein Edit Distance](/blog/levenshtein-edit-distance) \u0026 [Chain Matrix Multiplication](/blog/chain-matrix-multiplication) we are finally here.\n# The Knapsack Problem\nThe Knapsack is probably one of the most famous problems used to introduce Dynamic Programming to new learners. It asks the following question, _\"Given a list of $n$ elements, each of which have some value $v_i$ and weight $w_i$ associated with them, what is the maximum value of elements I can fit into my Knapsack given that my Knapsack can only hold at max a weight of $W$ capacity?\"_\n\nThere are two variations of the above problem as well. The simpler one assumes that we have an infinite quantity of each element. That is, we can pick an element as many times as we wish. The harder version does not assume this. Each element can only be picked once.\n\n## A toy example\n\nFor the sake of illustration, we'll assume we are attempting to solve the Knapsack for the given inputs\n\n![knapsack-1](/images/knapsack-1.png)\n\n\nWe have 4 items with their respective $v_i$ and $w_i$ values. Our Knapsack has a maximum capacity of $W = 10$.\n\n### With repetition\n\nIf repetition is allowed, we can solve the problem using a very simple approach. All we need to observe is that to compute the maximum value for a bag of capacity $W$, we can simply brute force over all elements with a simple recurrence.\n\nLet $F(W)$ be the maximum value obtainable for a bag of capacity $W$. Then,\n\n$$ \n\\begin{aligned}\nF(W) = max(v_1+F(W-w_1), \\ \\dots \\, v_n+F(W-w_n)) \\\\ \\text{In our example, this corresponds to the following computation } \\implies \\\\ F(10) = max(30+F(10-6), 14+F(10-3), 16+F(10-4), 9+F(10-2)) \\\\ \\implies F(10) = max(30 + F(4), 14+F(7), 16+F(6), 9+F(8)) \n\\end{aligned}\n$$\n\nThe idea behind this recurrence is as follows. At any capacity $W$, we are simply picking every possible element and asking what is the maximum value I can achieve **after** picking each element. It's more or less just a brute force that considers picking every element for each capacity $W$.\n\nIt is easy to see that we are computing the answer for $W$ such sub-problems from $W_i = 1 \\to W$. And at each sub-problem, we are iterating over $n$ elements.\n\nIt is also important to note that we do not consider including the element in our brute force when we reach a state where $W-w_i \\lt 0$. This is an impossible/unreachable state. The base case is when we no longer have any elements which we can fit into the bag.\n\n1. Hence we have $W$ sub-problems.\n    \n2. We are doing $O(n)$ computation at every node.\n    \n3. The recurrence is as described above.\n    \n4. The DAG structure is also easy to reason about. It's simply just a linear chain from state\n    \n    $W_i = 1 \\to 2 \\to \\dots \\to W$\n    \n5. Therefore, our final algorithm will have $O(nW)$ complexity.\n    \n\nFurther, since there are only $O(W)$ subproblems, we only need $O(W)$ space to store the DP table.\n\n### Without repetition\n\nNotice that our previous solution will not work here. Because we cannot choose elements multiple times. However, the order of choosing the elements does not matter either. But because of this condition, notice that it is not enough to simply consider subproblems defined by just one characteristic.\n\nThat is, a subproblem in the previous case was simply identified by $W$, the size of the Knapsack. Here, this is no longer the case. A \"state\" or \"subproblem\" has at **least** two changing variables. Both the number of elements we are including into the Knapsack **and** the weight of the Knapsack.\n\n#### The new DP state\n\nThat is, we must change the definition of our DP to a 2-d DP where $DP[i][j]$ represents the state where we are considering the **first** $i$ elements among the list of available elements and our Knapsack is of size $j$.\n\n1. **Number of subproblems**\n    \n    Since we have $n$ possible prefixes which we will consider and $W$ possible values for the weight, we have of the order $O(nW)$ subproblems to compute\n    \n2. **Finding the solution at some node**\n    \n    Notice that since we changed the definition of our DP to storing the best possible answer to the problem given that our Knapsack has size $W$ and we are only considering the first $i$ elements, when computing $DP[i][j]$, notice that we are only trying to **include** the $i^{th}$ element wherever it maximizes our answer.\n    \n    This has the important implication that we do not need to brute force over $n$ elements at some state $[i, j]$. We only need to check the states $[i-1, W-w_i]$. This is $O(1)$ computation at every node.\n    \n3. **Coming up with the recurrence**\n    \n    We are essentially trying to answer the question\n    \n    _\"At some capacity $W$, when considering the $i^{th}$ element, does including it in the Knapsack help increase the previously obtained score at capacity $W$ when considering only $i-1$ elements?\"_\n    \n    Writing this recurrence formally,\n    \n    $F(i, W) = max \\{ F(i-1, W), F(i-1, W-w_i) \\}$\n    \n    The first term in the max represents the previously obtained score at capacity $W$. The second term is the value we would get if we tried including element $i$ when considering a bag of size $W$.\n    \n4. The **DAG structure** for this problem is very similar to the structure obtained when solving the Edit distance problem. It is simply a graph where each state $[i, W]$ depends on the state $[i-1, W-w_i]$.\n    \n5. We have an algorithm that requires us to perform $O(1)$ computation for each of the $O(nW)$ subproblems. Hence the total running time will be $O(nW)$. However, since there are $nW$ subproblems, we will also require $O(nW)$ space.\n    \n\n#### Can we do better?\n\nThis time, we actually can! Notice that just like how we did in the Edit distance problem, the DP state at any $[i, W]$ is **ONLY** dependent on the DP states exactly one level below it. That is, every DP value in row $i$ is only dependent on the DP values in row $i-1$.\n\nThis means that again, we can do the exact same thing and use **Single Row Optimization** to reduce the space complexity of our DP from $O(nW)$ to just $O(W)$. For small values of $W$, we might even consider this linear!\n\n# Pseudo-polynomial-time algorithms\n\nAt first glance, it is very easy to write off the Knapsack problem as belonging to the $P$ complexity class ([Introduction to Complexity Theory](/blog/introduction-to-complexity-theory)). After all, it seems to just be quadratic right?\n\nBut this is not true. We define the complexity of algorithms based on input size $n$.\n\nTo be more precise: _Time complexity measures the time that an algorithm takes as a function of the **length in bits** of its input._\n\nHowever, notice that in this case, the complexity of our algorithm relies on both $n$ and $W$. $W$ is the **value** of an input. If we consider $W$ in binary, we would require $log_2(W)$ bits to represent $W$. If the input is in binary, the algorithm becomes **exponential.**\n\nWhy?\n\nWe will try to explain this by means of a nice example.\n\n1. Let's say we are trying to solve the problem for $n = 3$ and $W = 8$. Keep in mind that $W = 1000$ in binary. That is, $W$ is **4 bits** long.\n    \n    Hence total complexity = $O(nW) \\implies O(3 \\times 8) = O(24)$\n    \n2. Now let's increase $n$ to $n = 6$. We have linearly multiplied it by $2$. Notice that this still gives us\n    \n    Time complexity: $O(nW) \\implies O(6 \\times 8) = O(48)$. It is the expected increase by 2.\n    \n3. Now let us increase $W$ by a factor of 2. **Notice that this means we double the length of W in bits. Not the value of W itself.** This means $W$ will now be represented by $W = 8$ bits. This means $W$ is now equal to $10000000$ in binary.\n    \n    This gives us a complexity of $O(nW) \\implies O(3 \\times 2^8) = O(768)$. That is, there is an exponential increase in complexity for a linear increase in $W$ .\n    \n\n## Knapsack is NP-Complete\n\nThe Knapsack problem is in fact, an **NP-Complete** problem. There exists no known polynomial-time algorithm for this problem. However, it is nice to know that is it often classes as _\"Weakly np-complete.\"_\n\nThat is, for small values of $W$we can indeed solve the optimization problem in polynomial time. If we give input $W$ in the form of smaller integers, it is weakly NP-Complete. But if the value $W$ is given as rational numbers, it is no longer the case.\n\n# Alternate Version of the Knapsack problem\n\nWhile we solved the Knapsack problem in the standard manner by defining $DP[i][j]$ as the maximum value achievable when considering the first $i$ elements and a bag of capacity $j$, what do we do if the value of $W$ is large, but the value of $\\sum_{i}^{n}v_i$ is small?\n\nConsider the following two problems from the [AtCoder Educational DP contest.](https://atcoder.jp/contests/dp/tasks)\n\n## Knapsack - 1\n\nThe [first problem](https://atcoder.jp/contests/dp/tasks/dp_d) is simply the standard Knapsack problem.\n\nThe constraints for it were as follows,\n\n$$\n\\begin{aligned}\n1 \\leq N \\leq 100 \\\\ 1 \\leq W \\leq 10^5 \\\\ 1 \\leq w_i \\leq W \\\\ 1 \\leq v_i \\leq 10^9 \n\\end{aligned}\n$$\n\nA $O(nW)$ solution would take around $1e7$ operations which should pass comfortably.\n\nHere's a link to my submission: [Submission Link](https://atcoder.jp/contests/dp/submissions/19493344)\n\n## Knapsack - 2\n\nThe [second problem](https://atcoder.jp/contests/dp/tasks/dp_e) is a little different. It asks the same question, but for different constraints.\n\n$$ \n\\begin{aligned}\n1 \\leq N \\leq 100 \\\\ 1 \\leq W \\leq 10^9 \\\\ 1 \\leq w_i \\leq W \\\\ 1 \\leq v_i \\leq 10^3 \n\\end{aligned}\n$$\n\nNotice that $W$ is now $10^9$. $O(nW)$ would now take 1e11 operations. This would practically have a very slow running time in comparison to our previous ~1e7 operation solution.\n\nWe will have to think of something different.\n\nNotice that for this problem, the values $v_i$ are much smaller. In fact, considering $n=100$ elements, the maximum value obtainable is just $max(v_i)\\times n = 10^5$.\n\nNow, we can exploit this by doing the same Knapsack DP, but this time, instead of storing the maximum value achievable in max capacity $j$ when considering the first $i$ elements, we redefine the dp as follows.\n\n$DP[i][j]$ will now store the minimum weight required to achieve value $j$ when considering just the first $i$ elements. We can now simply pick the maximum $j$ in row $i=n$ which satisfies the condition $DP[i][j] \\leq W$.\n\nThis solution runs in $O(n \\times \\sum_{i}^{n}v_i)$ which gives us $\\approx1e5$ operations. This is much faster than the standard approach.\n\nHere's a link to my submission: [Submission Link](https://atcoder.jp/contests/dp/submissions/19494460)\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.212025181+05:30",
    "tags": []
  },
  "/blog/activity-selection-huffman-encoding": {
    "title": "Activity Selection \u0026 Huffman Encoding",
    "content": "# Greedy Algorithms\n\nAs discussed previously, greedy algorithms are an amazing choice when we can prove that they do indeed give the optimal solution to some given problem. This signifies the importance of being able to prove the optimality of greedy solutions. In general, if the following two conditions hold, we can certainly say that a greedy strategy will give us the globally optimal answer.\n\n## Locally optimum choice\n\nGiven some problems, can we focus on its local state and solve for a solution that produces the most locally optimal solution at that state? In other words, we should be able to take _one step_ towards the optimum solution.\n\n## Optimum substructure property\n\nOnce this step is taken, even after the change in state after taking that step, are we able to restate the problem such that the new problem is the same as the original problem, albeit for a smaller input?\n\nNotice that if the answer to the above two questions is **yes**, then it is possible to prove that repeatedly taking the locally optimal choice will indeed give us the optimal solution. This is easily proven via induction.\n\nTake the optimal step at any given step $i$, now restate the problem as a smaller version of the original problem, and again take the locally optimal step at $i+1$. We can inductively repeat till this is the final state where we can again take the optimal choice. Since we can solve each subproblem independently simply by taking the best choice at each step, the solution **must** be optimal.\n\n# Activity Selection\n\nConsider the famous activity selection problem. The problem is as follows,\n\n_Given a set of activities $S=\\{a_1, a_2,\\dots,a_n\\}$, where activity $a_i$ takes time $[s_i, f_i)$ to finish. Find the maximum number of activities that can be picked such that there are zero overlaps, i.e., pick the subset of the maximum size where all activities are disjoint._\n\nThe naïve solution would be to brute force over all $n!$ different permutations in linear time to find the optimal answer. This is obviously far too slow to be of much use to us. So, how can we do better? Would a **greedy** solution work?\n\n## Greedy #1\n\n**Sort the intervals by duration $|f_i-s_i|$ and greedily pick the shortest ones**\n\nDoes this satisfy our two properties? The answer is... no. Notice that by picking the shortest interval activity, we cannot restate the problem for a smaller input the same way. We do not have optimum substructure. Consider the below case.\n\n![greedy-1](/images/greedy-1.png)\n\n\nGreedily we would pick the middle activity, but this removes two activities for the next step. This problem has no optimum substructure. The optimal solution would be to pick both the large intervals.\n\n## Greedy #2\n\n**Greedily pick the activities that start the earliest**\n\nThat approach follows neither property. Consider this case,\n\n![greedy-2](/images/greedy-2.png)\n\n\nWe are neither picking a locally optimum choice nor maintaining an optimum substructure. The greedy solution gives 1 whereas the answer is clearly, 3.\n\n## Greedy #3\n\n**Greedily pick the activities that end the earliest**\n\nDoes this approach satisfy both criteria? The answer is... yes.\n\nLet us pick the activity that ends the earliest. If this is not part of the optimal solution and the activity it overlaps with is part of the optimal solution, notice that because the activity we picked ends earlier, our activity cannot have any other overlap. Both contribute +1 to the solution and hence our activity is locally optimal. Further, since we have picked the earliest ending activity (which is optimal) we can cross off overlaps and restate the problem for smaller input. This approach maintains both properties! It **must** be right.\n\n### A more formal proof\n\nLet us suppose that we know the answer. Let the answer be $A$. Let us sort $A$ by finish time such that $\\forall a_{i\u003cn}\\in A$, $f_i \\lt f_{i+1}$\n\nNow, let our optimal choice activity be $x$. By virtue of our greedy choice, we know that\n\n$f_{x} \\leq f_{a_i} \\forall a_i \\in A$\n\nConsider $f_{a_0}$. If $x = a_0$, we are done. But if $x \\neq a_0$, notice that $f_x \\leq f_{a_0}$. This means that $x$ cannot overlap with any more activities in the set $A$ than $a_0$. And the set $A$ is disjoint by definition. Our solution can be written as\n\n$$ B = A-\\{x\\}\\cup \\{a_0\\} $$\n\nNotice that $x$ cannot overlap with any element in $A$. This is because they're the first choice to be picked, there is no overlap on the left. And $f_x \\leq f_{a_0}$ implies there is no overlap on the right and both provide a $+1$ to the final answer. Hence $x$ **must** be an optimal choice.\n\nThis solution is **much better** than our $O(n!)$ solution and can find the optimal answer in just $O(nlogn)$. The $nlogn$ comes from the sorting requirement.\n\n# Huffman Encoding\n\n## The compression problem\n\nLet's think about how computers store text. A lot of the text on machines is stored in ASCII. ASCII is a character encoding used by our computers to represent the alphabet, punctuations, numbers, escape sequence characters, etc. Each and every ASCII character takes up _exactly_ one byte or 8 bits. The encoding chart can be found [here](https://asciichart.com/)\n\nOftentimes, computers need to communicate with one another, and sending large volumes of text is not an uncommon occurrence. Communication over networks, however, have their own cost and speed disadvantages that make sending smaller chunks of data a _very_ favorable option. This is one of the times when ranking an algorithm by **space** is preferred over ranking algorithms by **time**. As our metric for comparison between algorithms changes, so does our problem statement.\n\n_\"What is the most optimal way to losslessly compress data such that it takes up minimum space?\"_\n\nNotice that unlike video or audio compression, ASCII text compression must be **lossless**. If we lose _any_ data, we have also lost the character. This means we can no longer figure out what the original ASCII character was. These requirements give us a few basic requirements that our algorithm **must** meet.\n\n### Prefix-free property\n\nThe idea of compression is to reduce the size of the data being compressed. But ASCII requires 8 bytes. This means that we must try to encode data in fewer than 8 bytes based on the frequency of occurrence. This will allow us to dedicate fewer bits for more commonly occurring characters and more bits for characters that occur almost never, thus helping us compress our data. However, this implies that we need some form of **variable-length** encoding for our characters. One variable-length encoding that might work is the binary system.\n\nHowever, notice that the following assignment will fail.\n\n$$ Space \\to 0 \\\\ e \\to 1 \\\\ t \\to 00 \\\\ \\dots $$\n\nWhen we encounter the encoding $00$ in the compressed data, we no longer know whether it is \"two spaces\" or one \"t\" character. We have lost information in our attempt to compress data. This implies that our algorithm **must** fulfill the prefix-free property. That is while reading the compressed data, based on the prefix, we must be able to **uniquely** identify the character that it is representing. If this is not possible then we will not have an injective mapping and data will be lost.\n\n## A little detour to information theory\n\nBack in the world of information theory, Shannon laid out the 4 key axioms regarding information.\n\n1. **Information $I(x)$ and probability $P(x)$ are inversely related to each other**\n    \n    Consider the following thought experiment.\n    \n    1. The kings of the Middle East are rich\n    2. The man washing cars here is a rich man\n    \n    The second sentence conveys a lot more information than the first. The first statement is highly probable and hence does not convey as much information as the second.\n    \n2. **$I(x) \\geq 0$**\n    \n    Observing an event never causes a loss in information\n    \n3. $P(x)=1\\implies I(x) = 0$\n    \n    If an event is 100% certain to occur then there is no information to be gained from it\n    \n4. $P(x\\cap y)=P(x).P(y) \\implies I(x\\cap y)=I(x)+I(y)$\n    \n    Two independent events if observed separately, give information equal to the sum of observing each one individually\n    \n\nIt can be proven that the only set of functions that satisfy the above criteria are\n\n$$ I(x) = log_b(\\frac{1}{P(x)})=-log_bP(x) $$\n\nHe then went on to define a term called Information Entropy. It is a quantity that aims to model how \"unpredictable\" a distribution is. It is defined as the weighted average of the self-information of each event.\n\n$$ H(x) = \\sum_{i=1}^{n}P(x_i).I(x_i) = \\sum_{i=1}^{n}-P(x_i).log_2P(x_i) $$\n\nAn intuitive way to think of it is as follows. If an event that has a high self-information value has a high frequency, then this will increase the entropy. This makes sense as we are essentially saying that there is some event that is hard to predict which occurs frequently. Vice versa, if low self-information (something predictable) has a high frequency then the entropy of the distribution is lesser.\n\n\u003e An interesting fact to note behind the coining of the term \"Entropy\" in information theory. Shannon initially planned on calling it \"uncertainty.\" But after an encounter with John von Neumann who told him \"No one knows what entropy really is, so in a debate, you'll always have the advantage.\" he changed the term to \"Entropy\"\n## Back to algorithms!\n\nLet's say we have some encoding $E$ for our data $D$. We can measure the compression of our data by the \"Average expected length per symbol.\" This quantity is essentially just the weighted average of the lengths of each symbol in $D$ in our encoding $E$. Let's call the average length per symbol $L$.\n\nShannon discovered that the fundamental lower bound on $L$ is given as $L \\geq H(x)$. No matter what we do, we cannot compress the data to an average length lower than the information entropy of each data point occurrence.\n\nConsider the case where the letters `A`, `B`, `C`, `D` occur in our data with a frequency of $0.25$ each. We can divide the decoding process into a simple decision tree as follows,\n\n![huffman-1](/images/huffman-1.png)\n\n\n### Representing the encoding as binary trees\n\nIn the above image, if we replace every **left** branch with 1 and every **right** branch with 0, we get a very interesting encoding. We get a **prefix-free** encoding that maps every character to a unique encoding. Given some bit string, all we have to do is start at the node and follow the bit string along the tree till we reach a leaf node. Every path to a leaf node in a tree is unique and hence our encoding is unique. Further, since it is on a tree and we stop only after reaching the leaf node, there can be **no ambiguity**. This means that the encoding is prefix-free!\n\nIn fact, for the above data, we can do no better than the encoding above. However, when we get to work with varying probabilities, things change. Shannon and Fano came up with an encoding that used the same concept of representing the encoding on binary trees to ensure they maintain the uniqueness and prefix-free requirements.\n\nTheir algorithm began by sorting the frequency of every event and then splitting the tree into two halves such that the prefix and suffix sum on either side of our division was as close to each other as possible. This had the intended effect of relegating lesser used symbols to the bottom (greater depth and hence longer encoding) and more frequently used symbols to shorter encodings. This was a big achievement and was known as the Shannon-Fano encoding for a long period of time. It was a good heuristic and performed well but it was **not** optimal.\n\nNotice that with this greedy strategy, we **cannot** prove that it is taking the most optimal choice at the local level. This algorithm is **not** optimal.\n\nAt the same time, the Shannon-Fano encoding achieved both a unique representation of our data and more importantly, a prefix-free encoding that performed really well. Perhaps we can build upon their idea to obtain a prefix-free encoding with optimal compression.\n\n### Enter Huffman\n\nContrasting the top-down approach used by Shannon and Fano, Huffman viewed the problem with a _slight_ change in perspective. Instead of trying to start at the root, he claimed that if we picked the least two probable events, then they **must** be at the bottom of the tree.\n\n### Locally optimal choice\n\nWe want lesser used symbols to have longer encodings. If the above was not true, then that would imply that there is a symbol with a higher frequency of occurrence that is now given a longer encoding. This increases the size of the compression and is hence not an optimal choice. We now know for a fact that the least used symbols must belong to the bottom of the tree.\n\n### Optimum Substructure\n\nWe can write $L = \\sum_{i=1}^{n} p_i.d_i$ where $d_i$ is the depth of the $ith$ node in the tree. Note that this quantity $L$ is actually the same as the sum of the probabilities of every node except the root node in our tree. Consider the following example, notice that in the expanded view, the probability of each symbol gets included as many times as its depth in the tree.\n\n![huffman-2](/images/huffman-2.png)\n\n\nRemember that our goal is to minimize L. Let our symbols have probabilities/frequency $p_1, p_2, \\dots, p_k$ each and let us assume $p_1\\leq p_2\\leq\\dots \\leq p_k$. Using our optimal greedy choice, we can choose the bottommost nodes as $p_1+p_2$ and then restate the equation as follows.\n\n$$ L(p_1,\\dots,p_k) = p_1+p_2+L((p_1+p_2), p_3,\\dots, p_k) $$\n\nThat is, we have managed to express the next problem as a smaller version of the original problem for which we realize that again, the greedy choice holds. We have managed to obtain the optimum substructure in our problem.\n\nThis implies that therefore, our greedy algorithm is indeed correct. **This** is the Huffman encoding.\n\nGiven some text data in the form of $(data, frequency/probability)$ tuples we can build the Huffman tree by using the greedy logic described above. Always greedily pick the smallest two probabilities to form the leaf node, then repeat. This is guaranteed to give us the optimal solution.\n\nIt is interesting to note its similarity to Shannon-Fano encoding, sometimes, all you need is the slightest shift in perspective to solve some of the world's unsolved problems :)\n\nHuffman was able to **prove** that his encoding gives us the most optimal solution for encoding any set of $(data, probability)$ pairs as given. But... _can we do even better?_ Theoretically no, but there are algorithms that can reduce the size of our data even more. The primary idea used by these algorithms is to chunk data into multiple byte chunks and then applying Huffman encoding. Note that while we mostly referred to ASCII text, Huffman encoding can be used to losslessly compress any form of binary data.\n\nThe following video was referenced while making this diary and is the source of some of the illustrations above, highly recommend watching [this video](https://www.youtube.com/watch?v=B3y0RsVCyrw).\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [Huffman Codes: An Information Theory Perspective - Reducible](https://youtu.be/B3y0RsVCyrw?si=5kFqRPa_XsGxpqBr)",
    "lastmodified": "2024-05-30T09:58:44.208691838+05:30",
    "tags": []
  },
  "/blog/algorithms-for-approximate-string-matching-alignment": {
    "title": "Algorithms for Approximate String Matching - Alignment",
    "content": "# Preface \u0026 References\nI document topics I've discovered and my exploration of these topics while following the course, [Algorithms for DNA Sequencing, by John Hopkins University](https://www.coursera.org/learn/dna-sequencing) on [Coursera](https://www.coursera.org/). The course is taken by two instructors [Ben Langmead](https://scholar.google.com/citations?user=2JMaTKsAAAAJ\u0026hl=en) and [Jacob Pritt](https://www.coursera.org/instructor/jacobpritt).\n\nWe will study the fundamental ideas, techniques, and data structures needed to analyze DNA sequencing data. In order to put these and related concepts into practice, we will combine what we learn with our programming expertise. Real genome sequences and real sequencing data will be used in our study. We will use Boyer-Moore to enhance naïve precise matching. We then learn indexing, preprocessing, grouping and ordering in indexing, K-mers, k-mer indices and to solve the approximate matching problem. Finally, we will discuss solving the alignment problem and explore interesting topics such as De Brujin Graphs, Eulerian walks and the Shortest common super-string problem. \n\nAlong the way, I document content I've read about while exploring related topics such as suffix string structures and relations to my research work on the STAR aligner.\n# Algorithms for Approximate Matching\n\nAs we saw previously, due to sequencing errors and the fact that while another genome of the same species might have a $99\\%+$ but not perfect match with the genome we're reconstructing, the reads we are trying to sequence together might suffer severely if we simply attempt exact matching. Here we rely on techniques of approximate matching to tell us where these short reads might fit together in the final puzzle. \n\n[Levenshtein Edit Distance](/blog/levenshtein-edit-distance) is a string metric which is used to quantify how different two strings (such as words) are from one another. It is calculated by calculating the smallest number of operations needed to change one string into the other. These operations are very similar to operations which might happen in real DNA which causes these changes. Substitution could be the errors in sequencing, insertions and deletions along with substitutions could model gene-splicing and related operations.\n\n## Global Alignment\n\nThe [Levenshtein Edit Distance](/blog/levenshtein-edit-distance) is what we use to solve the **global alignment** problem in DNA sequencing. Global alignment is pretty much equivalent to the edit distance problem, except for a minor change in the scoring system which we'll discuss at the end of this section. If we define a function $F$ to be the edit distance between two strings.\n\n## Local Alignment\n\nLocal alignment is similar, but instead of searching for the match score between two sub-sequences, it is more suited to working with short reads in a bigger sequence. That is, it is good at finding positions in a bigger text where a smaller pattern could've occurred using approximate matching. This is pretty similar to our exact pattern finding algorithms except that it is more versatile in how it detects its matches and assigns them scores instead of binary exact matching. The recurrence here is pretty simple, we use the same global alignment recurrence except we change one of the base cases to:\n\n$$F(0, j) = 0$$\n\n![local-alignment](/images/local-alignment.png)\n\n\nThis lets us solve the local alignment problem in the same time complexity as global alignment.\n\n## The Scoring Matrix\n\nFor edit distance, the scoring is pretty much just $\\pm1$ for all operations. For DNA sequences however, take the example of the human genome:\n\n![human-genome-scoring](/images/human-genome-scoring.png)\n\n\nSimply listing the possibilities reveals that there are twice as many different types of transversion as there are different types of transitions. We may thus assume that transversions will occur twice as frequently as transitions. However, it turns out that transversions are only slightly more common than transitions when we look at the replacements that separate the genomes of two unrelated individuals. So, contrary to what we may think, it is the opposite way around. Therefore, we should penalize transgressions more severely than transitions in our penalty system. Further, indels are less frequent than substitutions. So we might want to penalize indels more than substitutions. So we modify our scoring matrix to reflect these real world statistics in practice.\n\n![penalty-matrix](/images/penalty-matrix.png)\n\n\n# Combining Both Approximate and Exact Matching\n\nIt seems like approximate matching is the solution we've been waiting for and a useful tool that will give us good matches for placing our short reads and thus help us reconstruct the sequence. This is true, but the problem herein lies in the fact that the approximate matching algorithms, while versatile, are much slower than their exact matching counterparts. While most of the exact matching algorithms run in linear or with an extra logarithmic factor, the approximate matching algorithms run in quadratic time and are usually also hard to vectorize or speed-up due to the dependency between their states. \n\nIf we simply ran local alignment between each of the short reads (which we usually have a billion or so off) and the human genome (which is also a billion characters in length), the computational task is infeasible for even today's most powerful compute nodes to solve quickly. Therefore we have to come up with a match of both approximate and exact matching algorithms to solve the overall problem quicker. Exact matching ([Booyer-Moore \u0026 Knuth-Morris-Pratt for Exact Matching](/blog/booyer-moore-knuth-morris-pratt-for-exact-matching)) is useful in pinpointing a few specific locations where we can then go and run approximate matching algorithms on. Consider the following figure:\n\n![exact_approximate_matching](/images/exact_approximate_matching.png)\n\n\nWe begin by querying the k-mer index table for a query which allows us to **rapidly** home in on small set of candidate needles which are the only places in the entire sequence we really need to run our powerful but slower approximate matching algorithms on. \n\nThus, both concepts work well **together**, kind of making up for each other's shortcomings while still accomplishing their goals. On the one side, the index is highly quick and effective at reducing the number of locations to check, but it completely lacks a natural way to manage mismatches and gaps. However, dynamic programming does handle inconsistencies and gaps rather nicely. But it would be incredibly slow if we simply used dynamic programming.\n\n",
    "lastmodified": "2024-05-30T09:58:44.215358525+05:30",
    "tags": []
  },
  "/blog/analyzing-fibonacci-karatsuba-multiplication": {
    "title": "Analyzing Fibonacci \u0026 Karatsuba Multiplication",
    "content": "# Fibonacci \nLet's try analyzing a few different algorithms for computing $F_n$, the $n^{th}$ Fibonacci number\n\n**Note:** The number doubles rapidly and hence grows at an exponential rate. In fact, $F_n \\approx 2^{0.694}$, which implies that we need around 0.694 bits to represent the $n^{th}$ Fibonacci number. This number can grow very large, very quickly. Not it is no longer accurate to consider the addition of 2 numbers as a constant time operation. For large values of n, the number of bits required to represent $F_n$ grows larger than any machine's word size and addition becomes a *linear-*time operation.\n\n\u003e 👾 **How do we know that $F_n \\approx 2^{0.694}$?** We can solve the recurrence relation to get the following formula:\n\u003e$$ F_n = \\frac{1}{\\sqrt5}((\\frac{1+\\sqrt5}{2})^n-(\\frac{1-\\sqrt5}{2})^n) $$\n\u003e\n\u003e For large n, the second term is negative and hence tends to 0. So for large n, the equation simplifies to $F_n = \\frac{\\Phi^n}{\\sqrt5}$, taking log here, we get $log_2(F_n) = n\\times0.694 - \\sqrt5$. Again, for larger n the change produced by $\\sqrt5$ reduces and the dominant term is simply $0.694n$.\n\u003e\n\u003e From this, we can infer that the $n^{th}$ Fibonacci number, especially for larger and larger n will require about $0.694n$ bits to represent in binary\n\u003e \n\u003e Bonus, running the below C++ program allows us to verify that the above relation even holds for smaller values of n. And with increasing n, the equation only grows more accurate and hence is a very good approximation of $F_n$\n\u003e```cpp\n\u003eint main(void){\n\u003e\tlong long n;\n\u003e\tcin\u003e\u003en;\n\u003e\n\u003e\tlong long p_2 = 0;\n\u003e\tlong long p_1 = 1;\n\u003e\tfor(int i=0; i\u003cn; i++){\n\u003e\t\tlong long fib = p_2 + p_1;\n\u003e\t\tcout\u003c\u003cfib\u003c\u003c\" \";\n\u003e\t\tswap(fib, p_2);\n\u003e\t\tswap(p_1, fib);\n\u003e\t}\n\u003e\tcout\u003c\u003cendl;\n\u003e\n\u003e\tlong double phi = 1.61803398875;\n\u003e\tfor(int i=0; i\u003cn; i++){\n\u003e\t\tlong long fib = round(pow(phi, i+1) / sqrt(5));\n\u003e\t\tcout\u003c\u003cfib\u003c\u003c\" \";\n\u003e\t}\n\u003e\tcout\u003c\u003cendl;\n\u003e}\n\u003e```\n\n## Algorithm 1 for computing $F_n$\n\n```bash\nif n = 0: return 0\nif n = 1: return 1\nreturn fib1(n-1) + fib1(n-2)\n```\n\nProving **correctness** for this particular algorithm is relatively straightforward as this algorithm is pretty much the exact definition of the Fibonacci function. This is one of the most powerful features that recursion is able to offer.\n\nIt is however important to keep track of space usage analysis as recursion stacks may grow very large and potentially overflow the stack.\n\n\u003e🛠 This is one of the reasons functional programming is a powerful idea. Functional languages have the inherent property that all code is expressed in a functional manner. This allows the code to pretty much express its own correctness proof.\n\n### Recurrence relation\n\n$T(n) = T(n - 1) + T(n - 2) + A\\ for \\ n \u003e 2$ where $A$ is the complexity for addition of two numbers Therefore, the time complexity for adding two numbers via this algorithm is $O(2^nA)$. We can visualize the branching like a tree and every node branching into two child nodes at every step of the recursion. And at every node, we perform $A$ operations for addition. For large $F_n$, since addition is linear in the number of bits and since $F_n \\approx 2^{0.694n}$, our final time complexity evaluates to $O(n2^n)$.\n\n## Algorithm 2 for computing $F_n$\n\nThe key idea used here is converting the recursion to iteration. Just keeping track of $f_{i-1}$ and $f_{i-2}$ for computing $f_i$ is enough. This idea is a very basic application of the concept of **dynamic programming.**\n\nBelow is an algorithm that keeps track of the computed Fibonacci numbers for all $i \\leq n$\n\n```bash\nif n = 0: return 0\ncreate an array f[0..n]\nf[0] = 0, f[1] = 1\nfor i = 2...n:\n\t\tf[i] = f[i-1] + f[i-2]\nreturn f[n]\n```\n\n**Note** that this is however not a linear time algorithm\n\nWhile the loop itself is linear, $F_n$ is about $0.694n$ bits long, and each addition is an $O(n)$ operation when we are dealing with arbitrarily large numbers. Therefore the overall complexity is $O(n^2)$ in the size of the input\n\n\u003e📈 We can also observe that the space complexity for the above algorithm is also evaluated to about $O(n^2)$. Arbitrarily large numbers can occupy $0.694n$ bits in memory, and we are storing all values of $F_n$ from $i \\dots n$.\n\u003e\n\u003eHowever, a simple optimization will help us reduce the space complexity to simply $O(n)$. We only ever need the previous two values of $F_i$ to compute it. That is, we only need to keep track of $F_{i-1}$ and $F_{i-2}$ to compute $F_i$. The rest of the values of $F_{j \\lt i-2}$ are not required. Keeping track of just the 3 values allows us to reduce space complexity by simply storing the number of bits in $F_n, F_{n-1}$ and $F_{n-2}$, which is linear in the input size. Space complexity: $O(n)$\n\u003e\n\u003eThe **key** realization here is just observing that our algorithm calculates all values of $F_i$ for **all $0\\leq i\\leq n$.** This is a redundancy. We only require to calculate the $n^{th}$ Fibonacci number. This realization will help us reduce the time complexity even further, as we will see below.\n\n## Algorithm 3 for computing $F_n$\n\nMotivated by our realization to eliminate the redundancy, we can attempt to make our computation even faster.\n\nLet us assume that we know $F_{i-1}$ and $F_{i-2}$ and we are attempting to compute $F_i$. Notice that to compute $F_i$, our equation looks like $F_i=1\\times F_{i-1} + 1\\times F_{i-2}$. This gives us $F_i$. Now from $F_i$, to get $F_{i+1}$, we need the term $F_{i-1}$ as well.\n\nWe get the following equations\n\n$$ F_i = F_{i-1}+F_{i-2} \\\\ F_{i-1} = 0\\times F_{i-2} + 1\\times F_{i-1} $$\n\nNotice that these set of equations can be represented nicely in a matrix form which lets us write\n\n$$ \\begin{pmatrix} F_{i-1} \\\\ F_{i} \\\\ \\end{pmatrix} =\\begin{pmatrix} 0 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix} \\begin{pmatrix} F_{i-2} \\\\ F_{i-1} \\end{pmatrix} $$\n\nNotice that by simply left-multiplying the RHS with our constant matrix, we calculate any $F_n$ that we desire. This allows us to come up with the following beautiful equation.\n\n$$ \\begin{pmatrix} F_n \\\\ F_{n+1} \\\\ \\end{pmatrix} =\\begin{pmatrix} 0 \u0026 1 \\\\ 1 \u0026 1 \\end{pmatrix}^n \\begin{pmatrix} F_0 \\\\ F_1 \\end{pmatrix} $$\n\nTo calculate any $F_n$, we only need to know the values of $F_0, F_1$ , and a constant matrix exponentiated to some $n$. Exponentiation of a constant to some power $n$, can be solved via _binary exponentiation._ Therefore the time complexity of this Algorithm comes out to be $O(M(n)log(n))$ where $M(n)$ is the time complexity for multiplying two n-bit integers\n\n## Algorithm 4 for computing $F_n$ (Direct formula)\n\n$F_n = \\frac{1}{\\sqrt{5}}(\\frac{1 + \\sqrt{5}}{2})^{n} - \\frac{1}{\\sqrt{5}}(\\frac{1 - \\sqrt{5}}{2})^{n}$t\n\nWe can also attempt to compute $F_n$ using the direct formula we obtain by solving the recurrence relation. However, notice that there are irrational and divisions involved. This might give us accuracy issues depending on machine type and whatnot. This makes it very difficult to prove accuracy of the algorithm on machines.\n\nFurther, we can note that we still need to compute some value to the power n. This requires $log_2(n)$ operations for the exponentiation and $M(n)$ operations for multiplication. This algorithm is essentially equivalent to our previous algorithm in terms of time complexity.\n\nWe also see that the eigenvalues of the matrix we use in Algorithm #3 appear in the direct formula. Therefore, it's better if we just use Algorithm #3 as we don't have to deal with irrational numbers, hence no accuracy issues arise. They are essentially 2 forms of the same algorithm.\n\n\u003eIn fact, if we calculate the eigenvalues of the matrix obtained in our 3rd algorithm, we get $\\lambda_1 = \\frac{1+\\sqrt5}{2} \\\\ \\lambda_2 = \\frac{1-\\sqrt5}{2}$\n\u003e\n\u003eThis further solidifies our suspicion that both algorithms 4 and 3 are essentially two different ways of expressing the same idea. One is a more mathematical method to compute $F_n$ and the other, a matrix represented technique that will be easier to implement on computers. They both have equal time complexity. But the matrix method is preferred as we do not have to deal with accuracy issues.\n\nNote that in all the above algorithms, the derived complexity involved the function $M(n)$. This is the number of operations required for multiplying 2 n-bit numbers.\n\nNotice that the naive algorithm for implementing n-bit multiplication is of the order of $n^2$. This makes our algorithms 3 and 4 worse than 1 and 2 as they become $n^2logn$ in the order of input size. However, if we are able to reduce the complexity of the multiplication operation, we will be able to do better than algorithms 1 and 2.\n\n# Karatsuba Multiplication\n\n_Can we do better than the order of $n^2$ operations per multiplication?_\n\nThis problem is an **open** question. We know an algorithm that can do better than $n^2$, but we have not been able to prove the optimality of this algorithm.\n\n**Intuition**:\n\nMultiplying two complex numbers.\n\nTo compute $(a+ib) \\times (c+id)$, we require 4 steps naively. $(ac-bd)+i(ad+bc)$\n\nIt is, however, possible to compute this in just 3 steps.\n\n- Compute $a \\times c$\n- Compute $b\\times c$\n- Compute $(a+b)\\times(c+d)$\n\nNotice that $(ad+bc) = (a+b)\\times(c+d)-ac-bd$\n\nLet us try to realize this same concept while multiplying 2 n-bit integers.\n\nSay we have some n-bit integer $X$. This implies that there are n-bits in its binary representation. This also means that we can divide every n-bit integer into 2 sets of $\\frac{n}{2}$ bits each (+-1).\n\n$X = 01001011 \\implies x_1 = 0100, x_0 = 1011$\n\nThat is, we can write $X = 2^{\\frac{n}{2}}x_1+x_0$. Notice that multiplying by $2^x$ is the same as shifting the binary by $x$ steps to the left. Hence shifting can be considered a constant operation.\n\n**Note** that this is true for any base. Multiplying by $k$ for any number in base $k$ is equivalent to shifting.\n\nThis is essentially all we need to know for coming up with the Karatsuba algorithm ourselves.\n\n## The algorithm\n\nTo multiply any two n-bit integers,\n\n1. Add two $\\frac{1}{2}n$ bit integers\n2. Multiply three $\\frac{1}{2}n$ bit integers\n3. Add, subtract, and shift $\\frac{1}{2}n$ bit integers to obtain the answer\n\n$$ X = 2^{\\frac{n}{2}}.x_1 + x_0 \\\\ Y = 2^{\\frac{n}{2}}.y_1 + y_0 \\\\ X.Y = (2^{\\frac{n}{2}}.x_1 + x_0)\\times(2^{\\frac{n}{2}}.y_1 + y_0) \\\\ = 2^n.x_1.y_1 + 2^{\\frac{n}{2}}((x_0+x_1)(y_0+y_1)-x_1.y_1-x_0.y_0) + x_0.y_0 $$\n\nNotice that the last step of the expansion is essentially the same as the constructive change we put forward in the multiplication of complex numbers idea to reduce multiplications required from 4 to **3**. This allows us to multiply 2 n-bit integers with an algorithm that recursively divides its input into $\\frac{n}{2}$ bit chunks and requires only 3 multiplications per $\\frac{n}{2}$ bit chunk.\n\nLet's suppose that our algorithm takes $T(n)$ steps to compute. At every step, we need to calculate the following terms.\n\n1. $x_1.y_1$ which can be done in $T(\\frac{n}{2})$\n2. $x_0.y_0$ which can be done in $T(\\frac{n}{2})$\n3. $(x_0+x_1)(y_0+y_1)$. Notice that the addition of two $\\frac{n}{2}$ bit numbers _can_ be a $1+\\frac{n}{2}$ bit number. Hence this will take us $T(\\frac{n}{2}+1)$ steps.\n4. Finally, once the shifts are done, we have a few $O(n)$ additions to be done.\n\nThis gives us the following result,\n\n**[Karatsuba-Ofman, 1962]** Can multiply two n-digit integers in $O(n^{1.585})$ bit operations.\n\n$$ T(n) \\leq T(\\frac{n}{2})+T(\\frac{n}{2})+T(\\frac{n}{2}+1) +\\Theta(n) \\\\ T(n) = O(n^{log_2(3)})=O(n^{1.585}) $$\n\n## Can we do _better_?\n\nWe shall cover the algorithm in detail later, but there does indeed exist an algorithm that can do it better.\n\n- The [The Fast Fourier Transform (FFT)](/blog/the-fast-fourier-transform-fft) based algorithms are able to compute this operation in $O(n\\ log(n)\\ log(log(n)))$\n- In 2007, we discovered a new method that computes it in $O(n\\ logn\\ 2^{log*n})$\n- The **best** (with proof of optimality) algorithm is still... an **open** problem\n\nThe fastest known algorithm till now is of the order of $O(nlogn)$ [by Harvey and van der Hoeven, 2019]. This is the [relevant paper](https://hal.archives-ouvertes.fr/hal-02070778v2/document). It begins by introducing the previously known algorithms and then deep dives into the math behind proving its upper bound. We were able to prove a lower bound on sorting as seen in [How to analyze algorithms? Proving a lower bound for comparison based sorting](/blog/how-to-analyze-algorithms-proving-a-lower-bound-for-comparison-based-sorting), can we do the same for this problem?\n\nNo, we do **not know** if this algorithm is the best at the time of writing this note. The theoretical lower bound we know of is $\\Omega(n)$ as the very least we require to do is process every bit of the input. There may or may not exist an algorithm better than $nlog(n)$, but we do not know of any such algorithms.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.218691869+05:30",
    "tags": []
  },
  "/blog/are-there-computational-problems-that-computers-cannot-solve": {
    "title": "Are There Computational Problems That Computers Cannot Solve?",
    "content": "Are there computational problems that computers cannot solve? How do we find the answer to this question? Turns out there's a very simple way to answer this question, even without defining what an \"algorithm\" is ([Church-Turing Hypothesis](/blog/church-turing-hypothesis)).\n\nNotice that if we are able to prove that there are **uncountable many** computational problems and only **countably many** computer programs. Then this would imply that there must exist uncountable many problems for which, **no computational program solution exists**.\n\n\u003e 🧮 **Countable sets** An infinite set is countable if there is a bijection $f:N\\to S$ from natural numbers to S **Uncountable sets** An infinite set is countable if it is not possible to construct a bijection $f:N\\to S$ from natural numbers to S. A common proof method is cantor's diagonalization which first assumes that it is possible to construct such a bijection and then proves that for every such bijection, we can always create a new element in the set that was not mapped before. Thus disproving that any such bijection can be created.\n\u003e \n# Proving that the set of all programs is countable\n\nNow, notice that every single program that we write, must be encoded to some subset in the set of all finite-length bit strings, i.e., some subset of $\\{ 0, 1 \\}^*$. We can draw an analogy here to how every compiled C program, for example, has its own unique binary file which can be used to represent it as a finite-length bit string.\n\nTheoretically, it is true that every possible program that we can write can be uniquely encoded as some finite-length binary string. We also know that the subset of an infinite countable set must be countable. Therefore, it suffices to prove that the set $\\{ 0, 1 \\}^*$ is countably infinite for the first part of our proof.\n\nEvery finite length binary string is just a natural number encoded in binary. This allows us to uniquely map such a bijection from the natural numbers to the set $\\{0,1\\}^*$.\n\n$0\\to0, 1\\to1, 10\\to2, 11\\to3, 100\\to4 \\ \\dots$\n\nThis implies that the set of all finite-length binary strings $\\{ 0, 1 \\}^*$ must be countably infinite. Therefore, since the set of all finite-length programs is a subset of this set, it must also be countable.\n\n# Proving that the set of all computational problems is uncountably infinite\n\nLet us prove that $P( \\{0, 1\\}^*)$, i.e., the power set of all finite-length bit strings is uncountable. Notice that every problem is modeled as a decision problem. And every decision problem is characterized by a set. Or it's \"language.\" Therefore, every possible subset of the set of all finite-length binary strings, actually represents a problem. Each subset is a unique language and each of them characterizes unique problems.\n\nTherefore, counting the total number of computational problems essentially reduces to calculating the cardinality of the power set $P(\\{0, 1\\}^*)$\n\nConsider the following function $f:\\{0, 1\\}^*\\to\\{0, 1\\}$ which maps the set of all finite-length binary strings to a subset. Let us pick some subset $S \\subset \\{0, 1\\}^*$. Then the function is defined as follows:\n\n$$ f(x)= \\begin{cases} 1 \\ \\forall \\ x \\in S \\\\ 0 \\ \\forall \\ x \\notin S \\\\\n\n\\end{cases} $$\n\nNow let us calculate $f(x)$ for every such language and write it in the form of a table\n\n![cantor-diagonalization-table](/images/cantor-diagonalization-table.png)\n\n\nLet us assume that we have enumerated an infinite number of such languages. Now we will use diagonalization to prove that there will always exist some language $L_x$ that does not belong to our set.\n\nWe construct $L_x$ as follows. We move along the diagonal and flip the value of $L_i$ for each element $i$ of the set.\n\n$L_x(\\epsilon) = 0, L_x(0) = 1, L_x(1) = 1, L_x(00) = 0, L_x(01) = 0 \\ \\dots$\n\nWe notice that such a language $L_x$ does not belong to the set as it differs from each $L_i$ belonging to our bijection at the $i^{th}$ element. This means we have successfully proved the existence of a language that does not belong to our bijection. No matter how many times we repeat the process of finding such a language and adding it to the bijection, we will always be able to prove the existence of such a new language that does not belong to the bijection. Hence we have proved that the power set $P(\\{0, 1\\}^*)$ is indeed, uncountably infinite.\n\n**This implies that the cardinality of the set of all computational problems is greater than the set of all possible computer programs. This in turn implies that there are uncountably many computational problems that we cannot find computational solutions for.**\n\nThat is sad. But we might still hope that most of these computational problems that we **cannot** solve are also problems that we are **not interested** in solving. This is, however, **not** true. Consider the following problem,\n## Program equivalence problem\n**Definition:** _Write a program that takes two programs as input and checks whether both the programs solve the same problem._\n\nWe will prove this in further lectures, but for the sake of intuition, notice that there are many many different ways to program an algorithm to solve a particular computational problem. It is not intuitively possible for us to write a program that can take two finite-length bit strings and deterministically say whether they both solve the same problem.\n\nThis is a useful program as it allows us to check the accuracy of programs easily. However, since this is not a problem we can solve, we have resorted to probabilistic solutions which test two programs by running them on a large collection of sample test cases and checking if their outputs are the same. However, note that this is a **probabilistic** solution and not a **deterministic** solution.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.222025213+05:30",
    "tags": []
  },
  "/blog/booyer-moore-knuth-morris-pratt-for-exact-matching": {
    "title": "Booyer-Moore \u0026 Knuth-Morris-Pratt for Exact Matching",
    "content": "# Preface \u0026 References\nI document topics I've discovered and my exploration of these topics while following the course, [Algorithms for DNA Sequencing, by John Hopkins University](https://www.coursera.org/learn/dna-sequencing) on [Coursera](https://www.coursera.org/). The course is taken by two instructors [Ben Langmead](https://scholar.google.com/citations?user=2JMaTKsAAAAJ\u0026hl=en) and [Jacob Pritt](https://www.coursera.org/instructor/jacobpritt).\n\nWe will study the fundamental ideas, techniques, and data structures needed to analyze DNA sequencing data. In order to put these and related concepts into practice, we will combine what we learn with our programming expertise. Real genome sequences and real sequencing data will be used in our study. We will use Boyer-Moore to enhance naïve precise matching. We then learn indexing, preprocessing, grouping and ordering in indexing, K-mers, k-mer indices and to solve the approximate matching problem. Finally, we will discuss solving the alignment problem and explore interesting topics such as De Brujin Graphs, Eulerian walks and the Shortest common super-string problem. \n\nAlong the way, I document content I've read about while exploring related topics such as suffix string structures and relations to my research work on the STAR aligner.\n# Algorithms for Exact Matching\n\n## The Naive Algorithm\n\nThe naive algorithm is trivial and simply scans the main text $S$ for the pattern $T$ in a quadratic manner by just iterating over each character of the main text for the starting position and comparing it with the pattern text $T$ character by character. This is clearly extremely inefficient and has a worst case running time complexity of $O(nm)$. A simple example where such a run-time is possible is the strings.\n\n$$T = aaaaa$$ $$S = aaaaaaaaaaaaaaa \\dots$$\n\n![Pasted image 20240530083916](/images/pasted-image-20240530083916.png)\n\n\n## Boyer-Moore Pattern Matching\n\nBoyer Moore algorithm starts matching from the last character of the pattern. It uses two main heuristics to solve the problem. \n\n-   **Bad Character Rule:** If we do some character comparisons, and we find a mismatch, we will skip all alignment checks until one of two things happens. Either the mismatch becomes a match, or the pattern $T$ moves all the way past the mismatched text character.\n\n    ![bad-character-rule](/images/bad-character-rule.png)\n\n\n    ***Explanation:*** *Our mismatching character is \\\"C\\\". We then search $T$ for the last occurrence of \\\"C\\\". Then we will shift $T$ by $3$ such that \\\"C\\\" is aligned between $S$ and $T$*\n\n-   **Good Suffix Rule:** Let $t$ represent the longest common suffix matched by our pattern $T$ with the portion of $S$ we are checking for a match with. We can now skip all comparisons until either there are no mismatches between $S$ and $t$ or $S$ moves past $t$. This can be done relatively fast with some pre-processing.\n\n    ![good-suffix-rule](/images/good-suffix-rule.png)\n\n\t\n    ***Explanation:*** *We have a sub-string $t$ of $T$ matched with pattern $S$ (in green) before a mismatch. We then find an occurrence of $t$ in $S$. After finding this, we jump checks to align $t$ in $S$ with $t$ in $T$.*\n\nThe algorithm simply tries both heuristics and picks the maximum skip distance returned by both.\n\n### Booyer-Moore Performance:\n\n-   **Worst-case performance:** $\\Theta(m)$ pre-processing $+ O(mn)$ matching.\n\n-   **Best-case performance:** $\\Theta(m)$ pre-processing $+ \\Omega(\\frac{n}{m})$ matching.\n\n-   **Worst-case space complexity:** $\\Theta(k)$\n\nThe traditional Boyer-Moore technique has the drawback of not performing as well on short alphabets like DNA. Because sub-strings recur often, the skip distance tends to cease increasing as the pattern length increases. One can however acquire longer skips over the text at the cost of remembering more of what has already been matched. \n\n## Knuth-Morris-Pratt Pattern Matching (KMP)\n\nKMP is a string matching algorithm that reduces the worst case time complexity of the pattern finding problem in a given text to $O(n+m)$. The idea behind KMP is pretty simple. We discuss it in the following sections.\n\n### Prefix Function\n\nGiven a string $s$ such that $|s| = n$, we define the **prefix function** of $s$ as a function $\\pi$ where $\\pi(i)$ is the length of the longest proper prefix of the prefix sub-string $s[0:i]$. Here $s[0:i]$ refers to the sub-string starting at (zero-indexed) index $0$ and ending at index $i$, both inclusive. A prefix that is distinct from the string itself is a proper prefix. We define $\\pi(0) = 0$. We usually compute the prefix function as an array $\\pi$ where $\\pi[i]$ stores the value of $\\pi(i)$. \n\nMore formally, we define the prefix function as:\n$$\\pi[i] = \\max_{k = 0 \\rightarrow i} \\{k : s[0 \\rightarrow k-1] = s[i-(k-1) \\rightarrow i] \\}$$\n\nFor example, prefix function of string *\"abcabcd\"* is  $[0, 0, 0, 1, 2, 3, 0]$ , and prefix function of string *\"aabaaab\"* is  $[0, 1, 0, 1, 2, 2, 3]$ .\n\nThe naive way to compute this array is to simply iterate on each prefix starting position, the prefix length and then compare sub-strings. This gives us a worst case time complexity of $O(n^3)$ which is clearly pretty poor.\n\n**Optimizations**\n\n-   Prefix function values can only increase by a maximum of one between consequent indices.\n\n    *Proof by contradiction:* If $\\pi[i + 1] \\gt \\pi[i] + 1$, we may take the suffix ending in position $i + 1$ with the length $\\pi[i + 1]$ and delete the final character from it. We then get a suffix that ends in position $i$ and has the length $\\pi[i + 1] - 1$, which is preferable to $\\pi[i]$.\n\n    The prefix function's value can therefore either increase by one, remain unchanged, or drop by a certain amount when going to the next slot. The function can only increase by a total of $n$ steps and can only decrease by a total of $n$ steps. This means that we only really need to perform $O(n)$ string comparisons. This reduces our time complexity to $O(n^2)$.\n\n-   We use dynamic programming to store the information computed in previous steps. Let's say we have computed all values of $\\pi$ till $i$ and now want to compute $\\pi[i+i]$. Now we know that the suffix at position $i$ of length $\\pi[i]$ is the same as the prefix of length $\\pi[i]$. We get two cases:\n\n    1.  If $s[i+1] = s[\\pi[i]]$ , this implies that $\\pi[i+1] = \\pi[i] + 1$.\n\n    2.  If  $s[i+1] \\neq s[\\pi[i]]$ , we know we have to compare shorter strings. We want to move quickly to the longest length $j \\lt \\pi[i]$ , such that the prefix property at position $i$  holds ( $s[0 \\dots j-1] = s[i-j+1 \\dots i]$ ). This value ends up being  $\\pi[j-1]$ , which was already calculated.\n\nThe final algorithm looks something like this:\n\n![Pasted image 20240530091949](/images/pasted-image-20240530091949.png)\n\n\n### Efficient Pattern Matching\n\nTo do this task fast, we simply apply the prefix function we discussed above. Given the pattern $t$ and main text $s$, we generate the new string $t + \\# + s$ and compute the prefix function for this string. \n\nBy definition  $\\pi[i]$ is the largest length of a sub-string that coincides to the prefix and ends in position $i$. Here this is just the largest block that coincides with  $s$  and ends at position  $i$ , this is a direct implication of our separation character $\\#$. Now, if for some index $i$, $\\pi[i] = n$  is true, it implies that  $t$  appears completely at this position, i.e. it ends at position  $i$ . \n\nIf at some position  $i$  we have $\\pi[i] = n$ , then at position $i - (n+1) - n + 1 = i - 2n$ in the string  $s$  the string  $t$  appears. Therefore we just need to compute the prefix function for our generated string in linear time using the above mentioned algorithm to solve the string matching problem in linear time.\n\nTime complexity: $O(|s|+|t|)$\n\n## Comparison of Both\n\nSource: [*What are the main differences between the Knuth-Morris-Pratt and Boyer-Moore search algorithms?* - **StackOverflow**](https://stackoverflow.com/questions/12656160/what-are-the-main-differences-between-the-knuth-morris-pratt-and-boyer-moore-sea)\n\n-   **Boyer-Moore's** approach is to try to match the last character of the pattern instead of the first one with the assumption that if there's not match at the end no need to try to match at the beginning. This allows for \\\"big jumps\\\" therefore BM works better when the pattern and the text you are searching resemble \\\"natural text\\\" (i.e. English) \n\t[*Boyer-Moore example run through*](https://www.cs.utexas.edu/~moore/best-ideas/string-searching/fstrpos-example.html)\n\n-   **Knuth-Morris-Pratt** searches for occurrences of a \\\"word\\\" W within a main \\\"text string\\\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.\n    [*KMP example run through*](https://www.cs.utexas.edu/~moore/best-ideas/string-searching/kpm-example.html)\n\nThis means KMP is better suited for small sets like DNA (ACTG)\n\n## Offline Algorithms: Indexing and k-mers\n\nA few more ideas we can use when working with pattern matching problems is grouping and indexing. In essence, we can enumerate all sub-strings of some constant length (say 5) from the main text and store this in an vector for example. We call these sub-strings of some constant length $k$ a \\textbf{k-mer}. The stored index of the k-mer contains the indices at which it is found in the main string. \n\nNow as for how we quickly search for a k-mer in this pre-processes data, we have two options.\n\n-   **Binary search.** We store the pre-processed data in sorted order. Can be done online during the pre-process stage or later be sorted after collection. Once done, finding a query k-mer simply involved binary searching on this sorted array. Note that strings can always be sorted by lexicographic order, this means that they will form a monotonous sequence for our query k-mer which means that our binary search is guaranteed to succeed in $O(log(n))$ comparisons. Each comparison can at worst case be $O(k)$. Hence we have a total time complexity of $O(klog(n))$. Here $n$ is the number of k-mer's we have pre-processed. \n\n-   **Hashing.** Here, we hash each k-mer using some well known hashing function such as djb2, murmur, Fibonacci hashing, etc. to quickly query their existence into a hash-table. Various types of hash-tables can be used, Cuckoo, Robin-Hood, etc. This is pretty much just using the already existing plethora of literature on the subject of hashing to help speed up the algorithm. Worst case time complexity is $O(k*c)$ where $c$ is the constant involved with hashing. Note that this constant can have varying degrees of performance based on factors such as the number of k-mers stored, etc. \n\n-   **Tries.** Another data structure that I believe would be useful here is a Trie. Tries are a type of tree data structure that are used for efficient data insertion, searching, and deletion. Tries are also known as prefix trees, because they store data in a way that allows for fast search operations based on prefixes of keys. A k-mer trie is a specific type of trie data structure that is used to store and search for k-mers, which are sub-strings of a fixed length $k$ in a given string. The k-mer trie data structure allows for efficient searching of k-mers within a string, and can be used in applications such as sequence alignment and gene prediction. Time complexity is $O(k)$ for each query operation.\n\n## Further k-mer optimizations\n\nNotice that one of the main bottlenecks here is the number of k-mers we have to store in our pre-processed data structure. Since the k-mers have a lot of overlap, one idea might be to reduce the number of k-mers we have to store in half by storing only those k-mers which start at odd indices. A consequence of this however, is that now our match success rate is only $50\\%$. However, we can get back to a $100\\%$ success rate by realizing that all we have to do is query indices which cover the entire field $\\mod \\ 2$ around the query index $q_i$. \n\n![kmer-index-variants](/images/kmer-index-variants.png)\n\n\nFor example, if we store only $\\frac{1}{3}^{rd}$ the number of k-mers, one way to do it would be to store every kmer which starts at a position $0 \\ \\mod \\ 3$. Now we just query indices around $q_i$ which give us the entire field $Z_3$ when taking indices $\\mod \\ 3$. Now we just check for existence of prefixes and suffixes as required for the required types of kmers and verify existence of the actual query string. \n\nHere we are paying extra penalty during the query phase for reducing the pre-process time. Further, pre-processing larger number of kmers usually requires more memory and this consequently leads to major penalty during the query phase due to terrible caching of the pre-processed data. So often it is worth the extra penalty paid during query operations to reduce the size of the pre-processed data simply to benefit from cache optimizations.\n",
    "lastmodified": "2024-05-30T09:58:44.225358556+05:30",
    "tags": []
  },
  "/blog/chain-matrix-multiplication": {
    "title": "Chain Matrix Multiplication",
    "content": "Previously, we discussed a dynamic programming solution to solve [Levenshtein Edit Distance](/blog/levenshtein-edit-distance). Today, we'll look at another interesting problem.\n# Chain Matrix Multiplication / Parenthesization\n\n## The problem\n\nThe problem of chain matrix multiplication is quite interesting. We previously saw that the best-known algorithms for multiplying two matrices are around the order $O(n^{2.81})$, This is not very ideal, especially for multiplying a chain of matrices. However, there is something we can do to severely save computing power!\n\nConsider the following problem.\n\n$$ A = 20 \\times 1 \\\\ B = 1 \\times 20 \\\\ C = 20 \\times 1 \\\\ Compute \\ ABC $$\n\nBecause the multiplication is associative, we can **choose** what multiplication we wish to perform. That means, we can do both of the following.\n\n$(A \\times B) \\times C$ and $A \\times (B\\times C)$. Notice that if we did the former, our first computation would give us a $20 \\times 20$ matrix which must be multiplied with a $20 \\times 1$ matrix. This will give Strassen's input of the order $O(n^2)$.\n\n**However**, if we picked the alternate route, after the first multiplication, we would have a $1\\times1$ matrix to be multiplied with a $20 \\times 1$ matrix. This is far more superior and will help reduce the input sizes of the matrices we perform multiplication on as this gives Strassen's input only of the order $O(n)$.\n\n**This** is the core idea of chain matrix multiplication. A more general term for this problem can be _\"Parenthesization.\"_ It simply asks the question, _\"For some associative computation where each computation takes some cost $c$ to compute, what is the minimum cost I can incur in total for my total computation by just reordering the computation by rules of associativity?\"_\n\n## How do we approach this?\n\nWe realize pretty quickly that greedy approaches will not work here. There is no notion of the locally optimal solution. Even if we pick the first pairing to be the one that gives the least cost it says nothing about how this pick affects the later picks. Hence we must try them all out.\n\n## What about DP?\n\nHow can we effectively exploit some substructure of this problem to write a recursive solution?\n\nLet's say we're given a sequence of $n$ matrices to multiply $a_0 \\times a_1 \\times \\dots \\times a_{n-1}$.\n\nNotice that at any given point, we can use the following idea to divide the problem into sub-problems. For any given **continuous** sub-segment, I must divide it into a multiplication of exactly **two** segments. For the above sequence, let the optimal pairing be $[a_0 \\dots a_i]\\times[a_{i+1} \\dots a_{n-1}]$. Then this is the split that I must perform at this state.\n\nHow do I know what the optimal split is? I must simply try all possible positions for the split all the way from between $a_0$ , $a_1$ to $a_{n-2}$ , $a_{n-1}$. To \"try\" each of these possible positions, I must know beforehand the cost of calculating each subpart.\n\nSo far we've seen examples of prefix and suffix dp. In the LIS problem, we calculated the LIS for every prefix. For edit distance, we could've done it either using a prefix or suffix dp. However, we quickly realize that this problem does not have that kind of structure. It is a lot more difficult to draw the DAG structure for this problem as this problem does not have a very \"linear\" way of solving it. Notice that our solution essentially requires us to compute the minimum cost for each and every \"sub-segment\" in our array of matrices.\n\n### Arriving at the DP solution\n\nLet's try to answer the following questions as we try to arrive at our DP solution.\n\n1. **What is the number of subproblems?**\n    \n    As stated previously, we need to compute the optimal cost of multiplying every \"subarray\" of matrices. For some given array of length $N$ we can have $\\frac{N \\times (N+1)}{2}$ such sub-segments. (We will have 1 segment of length $N$, 2 of length $N-1$, etc. Which gives us a total of $\\sum_{i=1}^{n}i$)\n    \n    Hence our sub-problems are of the order of $O(n^2)$. Our DP will likely be at least of $n^2$ complexity.\n    \n2. **Find the solution at some state and count the number of possibilities we have to brute force over**\n    \n    At some given state, notice that we are trying to compute the minimum cost required to multiply an ordered list of matrices from $[a_i\\dots a_j]$. To do so, we must brute force over all possible splits of this sub-array. The following pseudo-code will paint a better picture.\n    \n    ```cpp\n    for k in [i, j-1]:\n    \t\tDP[i][j] = min(DP[i][j], DP[i][k] + DP[k+1][j] + cost(M[i][k], M[k+1][j])\n    ```\n    \n    Here, $DP[i][j]$ stores the minimum cost incurred in optimally multiplying the segment from $i \\to j$ and `cost` simply calculates the cost of multiplying the resultant two matrices $[a_i \\dots a_k]\\times[a_{k+1}\\dots a_j]$.\n    \n    Notice that for any given $i, j$ there are a linear number of problems we must brute force over. Hence this step of our algorithm will have $O(n)$ time complexity.\n    \n3. **Finding the recurrence**\n    \n    We already derived the recurrence to explain the previous point better. The recurrence is the same as the one given in the pseudo-code. Each of the $DP[i][k]$ and $DP[k+1][j]$ states there represents the solution to one of its sub-problems.\n    \n4. **Figuring out the DAG structure and making sure we don't have any cycles**\n    \n    This turns to be a lot messier and harder to work with for substring/subarray dp as compared to prefix/suffix dp. This is intuitively understood from the fact that we lose linear structure. Hence we will visit this topic at a later point in time.\n    \n5. **Completing the solution**\n    \n    Notice that we have $O(n^2)$ sub-problems and each sub-problem requires $O(n)$ time to compute. This gives our algorithm an overall running time of $O(n^3)$ time complexity. And since we have $O(n^2)$ sub-problems we would require that much space to store the solutions to all our sub-problems.\n    \n    \u003eNote that this is fairly high complexity for an algorithm that simply just determines the best and most optimal order in which to multiply an ordered list of matrices. It does not make sense to spend time planning, coding, and integrating such an algorithm in the workflow pipeline if the matrix computations we are doing are fairly small.\n    \u003e\n    \u003eHowever, if we are working with matrices of huge sizes and the number of matrices is relatively smaller than the size of the matrices, precomputing the best order of multiplication before multiplying the matrices themselves could provide us with a **huge** boost in performance. Think about the example given at the beginning but several orders of magnitudes higher!\n\nAnother nice thing to notice is that this solution is not only applicable to chain matrix multiplication. We could've really changed the `cost` function in our algorithm to any cost function of our choice. In fact, the problem we have solved can be generalized to picking the optimal order of performing some operation on an ordered list of elements where the operation follows the **associativity** property alone.\n\n## Realizing the DAG structure\n\nAs mentioned before, it is not quite simple to understand the DAG structure for this problem. To get a good idea of what's going on, lets begin by simply drawing the recursion diagram for a small case. Let's say $[1, 4]$.\n\n![chain-matrix-mult-1](/images/chain-matrix-mult-1.png)\n\n\nNotice that the leaves of our tree are all the sub-segments of length 1. Imagine visually pruning all the leaves from our tree. We will now have a new set of leaves.\n\nThese are the new states/sub-problems to calculate. Notice that after performing such an operation, we have a mix of segments of different lengths. But which ones can be computed completely after having just computed the previous leaf states?\n\nNotice that these are just the segments of length 2. $[1, 2], [2, 3], [3, 4]$. We can perform this operation again, and again, and so on till we reach $[1, 4]$. In general, this construction can be extended to any general $[1, n]$.\n\nFrom this, it is easy to realize that we are computing DP states in order of increasing the length of sub-segments. Our DAG would look as follows.\n\n![chain-matrix-mult-2](/images/chain-matrix-mult-2.png)\n\n\nHere, I've attempted to paint the arrows showing the transition from a state of length just 1 below in green, 2 below in yellow, and 3 below in blue.\n\nThere are no cycles and we have $O(n^2)$ nodes.\n\n### DP ≠ Shortest Path on DAG\n\nWhile the shortest / longest path in a DAG example was quite useful to visualize DP previously, we must realize that this is not always the case. Why?\n\nThis is because the state at some node $[i, j]$ is not **just** dependent on the previous state. Remember that there is a cost associated with every multiplication that is dependent on the state it is being compared with.\n\nFor example, when computing the solution at node $[1, 3]$, it is not enough to just consider the cost from $[1, 1]$. The cost at $[1, 3]$ only has meaning when we sum up the total effect from both $[1, 1]$ **AND** $[2, 3]$.\n\nIn this DP solution, we cannot simply construct a DAG structure and find the longest/shortest path as the solution for that node is reliant on the values of **multiple** nodes. It was a great way to visualize and be introduced to DP, but it is **not** always the case :)\n\n### Can we do better?\n\nLast time, we were able to reduce the space complexity of our DP by realizing that the DP only relied on the states of the DP solution exactly **one** level below the current level. However, here we realize that this is sadly not the case. The solution at some node $[i, j]$ is very much reliant on every level below it. 1D row optimization etc does not seem to be of much use here. There is also no monotonicity that can be exploited to make the linear computation at some node logarithmic similar to how we did with LIS. Hence I do not think there is a better way to solve this problem.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.2286919+05:30",
    "tags": []
  },
  "/blog/church-turing-hypothesis": {
    "title": "Church-Turing Hypothesis",
    "content": "# Church-Turing Hypothesis\n\nThe church turning hypothesis is essentially a hypothesis that answers the question, _\"What are algorithms?\"_ The hypothesis states that algorithms are nothing but Turing machines. Any mechanical computation can be performed by a Turing machine. If there is no Turing machine that can decide a problem P, then there is no algorithm that can solve P.\n\nNote that while the Church-Turing Hypothesis is just a hypothesis and **not** proof. However, it is widely accepted as it is founded on very logical arguments that are based on the \"axioms\" of computing we stated while defining what constitutes a computational solution.\n\n## The Turing Machine\n\nWe have an infinite length tape that has been split into an infinite number of finite length cells. We have a \"read/write\" head that can move along the tape (to the left or right cells). This read-write head operates on the current cell it is on top of. Note that this is in line with the fact that it is not possible to read/write to an infinite amount of memory. Hence it must operate on finite-sized cells.\n\nThe Turing machine is defined as a 7-tuple $(Q, \\Sigma, \\Gamma, \\delta, q_0, q_{accept}, q_{reject})$.\n\n1. $Q$ - **The finite set of states**\n    \n    The TM must at all times be in one of the finitely many _control_ states defined by the set $Q$. This is similar to the state diagram of circuits. When in a particular state the TM decides and responds to some input based on the control state it is in.\n    \n2. $\\Sigma$ - **The finite alphabet set (not containing blank)**\n    \n    $\\Sigma$ is the finite input alphabet. This is the _alphabet_ we encode our input in. Since input can be of any size, we also require a _blank_ alphabet to represent the end of some particular input. This is analogous to the C implementation of strings. Strings can be thought of as the input with ASCII constituting its input alphabet. The blank character here would be the `\\\\0` null terminator which signifies the end of the string. Note that the blank alphabet is **not** a part of this set.\n    \n3. $\\Gamma$ - **The finite tape alphabet (includes blank)**\n    \n    $\\Gamma$ is the finite _working_ alphabet + the _input_ alphabet. It is hence, a superset of the finite alphabet set $\\Sigma$. The tape alphabet but contain at least _one_ more symbol than the input alphabet. Namely, the blank alphabet. However, apart from the blank alphabet, we can have many more _work_ alphabets which signify something of meaning to the TM read-write head. This is analogous to the \"instruction encoding\" of the ISAs used by modern computers.\n    \n4. $\\delta:(Q\\times \\Gamma) \\to(Q\\times\\Gamma\\times\\{L, R\\})$ - **The transition function**\n    \n    When we are in some control state $Q$ and we read some tape symbol $\\Gamma$, then we can move to some other state $Q'$, overwrite the contents of the current cell with some tape symbol $\\Gamma'$ and move either to the left or to the right. Note that $\\Gamma'$ and $Q'$ may be the same states. The transition function essentially just tells the Turing machine what to do when it reads some tape alphabet when it is in some control state $Q$.\n    \n    Note that it is also possible to remain in the same state. We can simply encode a move right and the right cell can be a move right. This would take 2 steps but the result is the same. The goal here is to define an abstract construct in a **simple** manner that allows us to represent any algorithm. _NOT_ defining the most efficient such construct.\n    \n5. $q_0$ - **The start state**\n    \n    Whenever the machine begins to work on a decision problem it must begin in some pre-defined control state. This is the _start_ state of the machine.\n    \n6. $q_{accept}$ - **The accept state**\n    \n    If the machine ever reaches this state, then the machine can decide that the input does indeed belong to the language and output `1` and stop.\n    \n7. $q_{reject}$ - **The reject state**\n    \n    If the machine ever reaches this state, then it can decide that the input does not belong to the language and output `0` and stop.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.2286919+05:30",
    "tags": []
  },
  "/blog/complexity-theory-reductions": {
    "title": "Complexity Theory - Reductions",
    "content": "# Reductions\n\n\u003eIn computability theory and computational complexity theory, a reduction is an algorithm for transforming one problem into another problem. A sufficiently efficient reduction from one problem to another may be used to show that the second problem is at least as difficult as the first.\n\nIntuitively, what does this mean?\n\nLet's say we have two problems $f$ and $g$. Let's suppose that problem $g$ has a known solution. Then the following is a reduction from $f \\to g$.\n\n![reductions-1](/images/reductions-1.png)\n\n\nThe **\"Reduction\"** is basically finding those two blue boxes, which convert the input and output from that of problem $f$ to equivalent input for problem $g$. Now we can simply compute the solution for problem $g$ and then use the reverse of our reduction algorithm to transform the output to that required by $f$.\n\nIf we can find two such blue triangles which can transform the input \u0026 output in such a way then we can effectively say that problem $f$ has been reduced to solving problem $g$. This is because solving $g$ implies being able to solve $f$.\n\nWhat's more interesting is if these blue triangles are **polynomial-time** algorithms. If we can find poly-time algorithms which can perform this transformation of the input and output, then we have an _efficient_ reduction.\n\nWe have effectively managed to solve $f$ using the solution of $g$, along with some (hopefully efficient) pre and post-processing.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. A discussion with [Anurudh Peduri](https://anurudhp.github.io/) on the Theory Group Discord. ",
    "lastmodified": "2024-05-30T09:58:44.232025244+05:30",
    "tags": []
  },
  "/blog/covered-calls-strategy": {
    "title": "Covered Calls Strategy",
    "content": "# Introduction\nThe covered call strategy is an options trading strategy ([Derivatives - Options](/blog/derivatives-options)) in which an investor holds a long position in an underlying asset and simultaneously writes (sells) call options on that same asset. This strategy is employed by investors seeking to generate income from the premium received by selling the call options, while also benefiting from potential stock price appreciation up to the strike price of the call option.\n\n# How It Works\n![Pasted image 20240507114807](/images/pasted-image-20240507114807.png)\n\nA call option is a contract that gives the buyer the right, but not the obligation, to purchase a specified quantity of an underlying asset at a predetermined strike price within a specific period. In the covered call strategy, the investor owns the underlying asset (e.g., shares of a stock) and writes (sells) call options on those shares. \n\n**Example:** For instance, consider an investor holding 100 shares of a stock currently trading at ₹400 per share (market value). The investor can sell a call option with a strike price of ₹420 and an option premium of ₹4 per share (₹400 total premium). The call option expires in 30 days.\n\nIf the stock price remains below ₹420 at expiration, the call option will not be exercised, and the investor retains the ₹400 premium as income. However, if the stock price rises above ₹420, the call option holder may exercise the option, requiring the investor to sell the shares at the strike price of ₹420, regardless of the higher market price. The payoff profile of the covered call strategy is characterized by a capped upside potential but limited downside risk. The maximum profit is equal to the call option premium received, plus the difference between the strike price and the purchase price of the underlying asset (if the option is exercised). The maximum loss is limited to the initial cost of purchasing the underlying asset, minus the premium received. The covered call strategy generates income from the call option premium, providing a cushion against potential downside risk in the underlying asset. However, the strategy also caps the upside potential, as the investor is obligated to sell the underlying asset at the strike price if the option is exercised. Additionally, the investor faces the risk of having the underlying asset called away, potentially missing out on further appreciation.\n\n# Advantages\n## Dividend Income\nIf the underlying asset is a dividend-paying stock, the investor continues to receive dividends while the covered call position is held.\n## Stock Price Appreciation\nBy selling out-of-the-money call options, the investor can benefit from potential stock price appreciation up to the strike price of the call option.\n## Option Premium Income\nThe premium received from selling the call option generates additional income for the investor. If the option expires unexercised, the investor can sell a new call option on the same underlying position, generating recurring premium income.\n## Downside Hedge\nThe covered call strategy is sometimes regarded as a downside hedge for an investor's portfolio. If the portfolio remains flat or declines in value, the income generated from selling call options can partially offset the losses, providing a cushion against downside risk compared to simply holding the underlying assets.\n# Disadvantages\n## Capped Upside Potential\nWhile the covered call strategy provides three potential sources of return (dividend income, stock price appreciation up to the strike price, and option premium income), the third source of return is achieved by significantly restricting the second source – stock price appreciation. As illustrated by the payoff graph, the covered call strategy caps the upside potential, as any appreciation beyond the strike price is transferred to the call option buyer. Consequently, the strategy effectively hedges potential upside gains.\n## Higher Tax\nThe premiums received from selling call options are generally considered short-term capital gains, which are taxed at a higher rate than long-term capital gains. Additionally, if the options are exercised, requiring the sale and repurchase of the underlying asset, the investor may need to report and pay taxes on the capital gains, potentially reducing the tax-deferral benefits of holding the position.\n## Share Holding Power Reduction\nIf the stock price fluctuates significantly, even if it remains flat over the long term, the covered call strategy may result in a gradual reduction in the number of shares held due to the exercise of call options. This can lead to realizing losses, despite the overall position being flat.\n## Risk-Adjusted Return Considerations\nWhile some people argue that covered call strategies may offer superior risk-adjusted returns, meaning higher returns per unit of risk taken, this claim is subject to debate and may depend on the specific market conditions and underlying assets involved.",
    "lastmodified": "2024-05-30T09:58:44.235358587+05:30",
    "tags": []
  },
  "/blog/de-novo-assembly-overlap-graphs": {
    "title": "De-Novo Assembly \u0026 Overlap Graphs",
    "content": "# Preface \u0026 References\nI document topics I've discovered and my exploration of these topics while following the course, [Algorithms for DNA Sequencing, by John Hopkins University](https://www.coursera.org/learn/dna-sequencing) on [Coursera](https://www.coursera.org/). The course is taken by two instructors [Ben Langmead](https://scholar.google.com/citations?user=2JMaTKsAAAAJ\u0026hl=en) and [Jacob Pritt](https://www.coursera.org/instructor/jacobpritt).\n\nWe will study the fundamental ideas, techniques, and data structures needed to analyze DNA sequencing data. In order to put these and related concepts into practice, we will combine what we learn with our programming expertise. Real genome sequences and real sequencing data will be used in our study. We will use Boyer-Moore to enhance naïve precise matching. We then learn indexing, preprocessing, grouping and ordering in indexing, K-mers, k-mer indices and to solve the approximate matching problem. Finally, we will discuss solving the alignment problem and explore interesting topics such as De Brujin Graphs, Eulerian walks and the Shortest common super-string problem. \n\nAlong the way, I document content I've read about while exploring related topics such as suffix string structures and relations to my research work on the STAR aligner.\n# De-Novo Assembly\n\nNow that we've covered the section where we worked on the genome reconstruction problem ([DNA Sequencing](/blog/dna-sequencing)) assuming the existence of another genome from the same species, what do we do when there exists no such previously reconstructed genome? Such a situation can occur when we're studying the genome of a new exotic species or simply lack access to said genome. This was the problem that the original scientists who worked on the Human Genome Project had to deal with and the problem is indeed far more computationally intensive than when we already have a snapshot to work with. \n\n## Core Ideas\n\nTo slowly build up to the solution, let us first understand the key ideas involved in the problem. We essentially have many, many short reads of DNA sequences from the main genome and need to somehow piece them back together to reconstruct the original genome. To reiterate, we are given these short reads in no particular order and have no picture of where to match these short reads inorder to reconstruct the main sequence.\n\nTo solve this problem, let us begin by working back from the final solution. \n\n!![coverage](/images/coverage.png)\n\n\nLet's suppose we did know the positions of the short reads in the original sequence. We then define the term **coverage** as the number of overlapping reads for each character $c$ of the main genome. We can then simply define a term **average coverage** as the coverage we can expect for each character of the sequence given the length of the sequence, length of each read and the total number of short reads we have of the sequence. \n\n$$\\text{Avg. Coverage} = \\frac{\\text{Length of read } * \\text{ Number of reads}}{\\text{Length of genome}}$$\n\nFor the above example, the value comes out to be around $5$ (simply round to the nearest integer). We hence call this a **5-fold** coverage of the genome. Now notice that if we have two overlapping reads, the *suffix* of one read is **very similar** to the *prefix* of the next read. This follows from the fact that they are overlapping consequence reads. From this, we get the two laws of assembly.\n\n1.  If a suffix of read $A$ is similar to a prefix of read $B$, then $A$ and $B$ *might* overlap.\n\n2.  More coverage leads to more and longer overlaps.\n\n3.  **Repeats are bad.** (Will be discussed later.)\n\nNote that in the first law we again use the term *similar*, because there can be errors. These mainly stem from DNA sequencing errors and from *polyploidy.* That is, species can have two copies of each chromosome, and these copies can differ slightly.\n\n## Overlap Graphs\n\nWe define overlap graphs for a particular set of reads as follows.\n\nLet the nodes of the graphs represent the reads we have obtained of the genome. Now, there exists an edge $e$, between an **ordered** pair of nodes $(u, v)$ when a suffix of $u$, overlaps with the prefix of $v$.\n\nNow, not all overlaps are equally important. For example, an overlap of size $1$ can be very frequently occurring and doesn't provide much evidence of it occurring due to it being consequent overlapping reads in the genome. Hence, we can build overlap graphs where an edge $e$ exists between an ordered pair of nodes $(u, v)$, only when the overlap between them exceeds some constant value. Consider the following overlap graph for overlaps of size $\\geq 4$\n\n![overlap-graph](/images/overlap-graph.png)\n\n\n",
    "lastmodified": "2024-05-30T09:58:44.242025274+05:30",
    "tags": []
  },
  "/blog/defining-computational-problems": {
    "title": "Defining Computational Problems",
    "content": "# Introduction\nTo study \u0026 analyze algorithms, we must have a solid foundational theory for what algorithms are. We often define algorithms as a series of steps that must be followed to solve a problem. This raises a few basic questions that we must answer concretely.\n\n- What are **computational** problems?\n- Once _a_ solution is obtained, how do we know that it is indeed correct?\n- Say we have 2 solutions `a` \u0026 `b`. How can we compare these 2 solutions? On what basis can we say one solution is better than the other?\n- Once we can do the above, how can we find the lower bound, i.e., the most optimal solution to a problem? and more importantly, can we prove that it is indeed optimal?\n\n# What are computational problems?\n\nThere are many kinds of problems in the world. Math problems, world peace \u0026 hunger problems, problems arising from eating a bat, etc. to name a few. We wish to concern ourselves with problems of a particular class, **computational** problems. However, there are many difficulties associated with defining such a class. Consider the following challenges:\n\n- **There may be infinite ways of posing the same problem**\n    \n    Consider the following problems.\n    \n    1. _What is the smallest prime number?_\n    2. _What is the smallest even positive integer?_\n    3. _What is the GCD of the set of even positive integers?_\n    \n    Notice that the output (solution) for all the above problems is two. From a computational standpoint, the above problems are all the same. But there are infinite ways to pose the same problem.\n      \n- **How do we pose a question without solving it?**\n    \n    Consider the age-old problem of sorting. The problem is usually phrased as follows,\n    \n    \u003e _Given a sequence of `n` integers, among all possible permutations of this sequence, output such a sequence that it is in ascending order. That is, $a_i \u003c a_{i+1} \\ \\ \\forall \\ \\ 1 \\leq i \\lt n$_\n    \n    Notice that in the above statement, we provide the solution to the question itself. Phrased differently, the problem is essentially telling us to iterate over all possible $n!$ permutations and pick the permutation such that the sequence is in ascending order. While granted, this isn't a _good_ solution, it is a solution. We must come up with a way to pose problems that we do not have a solution to yet. Or maybe even problems for which there does _not_ exist any solution.\n    \n- **What kind of tools can we allow to be used to solve the problem?**\n    \n    Notice that depending on the solutions that are allowed to a problem, the meaning of \"solving\" the problem changes. Consider the following _mythological_ sort algorithm.\n    \n    1. Meditate until god appears\n    2. Provide input to god\n    3. Obtain sorted output from god\n    \n    This is not a _computational_ solution to the sorting problem. Hence it is necessary to enforce a _computational_ constraint on problems.\n    \n    ## Defining a computational problem\n    \n    When defining a computational problem, we make the following assumption about the world we live in.\n    \n    \u003e🧭 The input is digitized. We live in a noisy environment. Whenever there is noise, if 2 symbols are closer than some threshold, we say they are in the same equivalence class and are one and the same. This ensures that the total number of symbols we must deal with becomes finite in a finite space. This ensures that we're able to digitize the input and output.\n    \n    Further, assume that the output has multiple but a finite number of bits. We can then model each bit as a separate problem. This enables us to model all problems with finite output as decision problems. A decision problem is simply a problem with **1-bit** output. \"_Is this true or false?\"_\n    \n    **This allows us to pose problems as membership queries in _\"languages.\"_**\n    \n    ### Defining a \"Language\"\n    \n    We can reduce the question _\"X is my input and I am looking for the output Y\"_ to \"_Does my input X belong to the set of all inputs which give output one?\"_\n    \n    \u003e💬 **Languages** Each decision problem is characterized by a subset of the set of all possible inputs. (,i.e., subset of say, {0, 1}*)\n    \u003e\n    \u003e $L = \\{x \\ | \\ s\\in \\{0,1 \\}^* \\}$\n    \u003e\n    \u003e For example: Consider the problem of checking if a sequence is sorted or not.\n    \u003e \n    \u003e Let us encode our strings as numbers in binary separated by some terminator which splits each number in the sequence. Our question now reduces to, _\"Given a string encoded this form, does it belong to the language $L_{sorted}$?\"_ The string that encodes the sequence {1, 2, 3} would belong to this set. But the string which encodes the sequence {3, 2, 5} would not. Our encoding must be able to represent all different inputs in a unique manner. Notice that this has allowed us to reduce the problem to a simple decision problem of querying membership in our language $L_{sorted}$\n    \n    This is essentially how all problems in complexity theory are formalized.\n    \n    This formalization allows us to deal with most of the challenges aforementioned. Multiple ways to pose the same question no longer matter as the problems are characterized by the language. If the set of all possible inputs for which the output is 1 is the same for 2 different problems, then they are one and the same. Further, we can now pose problems without providing a solution as it is possible for us to define sets without having to enumerate them.\n    \n    We will discuss the problem of what tools can the solver be allowed to use when discussing how we define a _solution_ to a computational problem.\n    \n    # What are solutions in the world of computing?\n    \n    While there is no such thing called \"axioms of computation\" in standard literature, Prof. Kannan calls the following the base \"axioms\" or assumptions we make when defining what counts as a solution.\n    \n    - **It takes non-zero time to retrieve data from far-off locations**\n        \n        This essentially implies that the flow of information is not instantaneous. Consider memory so large that it spans the distance from Earth to Mars. If information transfer from the 2 ends of this memory was instantaneous then it would imply information traveling faster than light speed. This shouldn't be possible. Hence it is not feasible to allow our computational solutions to be able to have **random** access to huge chunks of memory.\n        \n    - **Only finite information can be stored/retrieved from finite volume**\n        \n        We cannot store infinite information in finite memory. Note that this assumption/axiom essentially invalidates solutions that allow time travel. If time travel were possible, we could go back in time to change the contents of memory/access infinite different states of some finite volume, and hence, allow infinite information access from finite memory. This is now ruled out.\n        \n    - **A finite-length code only exerts a finite amount of control**\n        \n        Any finite-length program cannot be omnipotent. That is, because the number of instructions is finite, there can only be a finite number of states the machine can exist in. Both the symbols making the instruction set and the instructions are finite, hence limiting the states it can be into a finite amount.\n        \n    \n    \u003e ⚠️ Note that these assumptions are made because we are limited by the technology of our time. If we are able to construct technology that can, indeed, violate any of the above \"axioms\", then we will in fact be able to come up with a model of computation where we will be able to solve problems much harder than the ones we are able to do today.\n# How to compare computational solutions?\n\nNow that we have defined computational problems and solutions, we need a way to compare two different solutions to a problem and say deterministically which solution is _\"better\"._\n\nHowever, we again run into multiple challenges. It is difficult to come up with a deterministic answer to a somewhat subject question, _\"Which solution is better?\"_\n\nIn the field of complexity theory, we usually focus on worst-case analysis/asymptotic analysis. We measure the performance of a solution in terms of its input size. However, note that this is not necessarily the best method to compare two solutions. Let's say some solution 'a' takes (a heavy) constant amount of time to run and another solution 'b' runs in logarithmic time. For larger inputs, we should see algorithm `a` perform better than `b`. But it may be true that our machine is never provided large inputs. In this case, it might be better to compare the best case.\n\nTo judge which algorithm is \"better\", we can say that the solution which uses lesser resources to compute is better. However, we run into another challenge. Computing a solution does not usually just depend on _one_ resource. One very precious resource is time. But there are other resources that matter too. Space \u0026 power are two other important resources.\n\nIn general, we put **time** at a pedestal compared to all other resources. In general, every other resource can be reused or generated in some manner. We can reuse memory \u0026 generate power, but time once lost can never be gained back. Hence unless specified explicitly, when we compare 2 solutions, we often implicitly assume that we are comparing 2 solutions based on the most precious resource **time.**\n\n\u003e ☄️ Note that, _technically,_ according to relativity: space and time are the same constructs and we can effectively interchange them. It is possible to say that I can start a program that would take 50 million years to compute, load it into a spaceship, let it go on a trip across the universe close to the speed of light and then when it returns to earth after a month, collect the output. However, as we lack the resources to be able to do anything even remotely close to this in the near (or distant) future, we ignore these technicalities when deciding on the quantity we'd primarily wish to compare algorithms with.\n\nNow that we've defined how to define a problem, let's try to construct useful arguments using this definition. [Are there computational problems that computers cannot solve?](/blog/are-there-computational-problems-that-computers-cannot-solve). How do we define a \"solution\" to a computation problem in an \"algorithmic\" sense. Note that this theory dates to before when computers were invented. How do we formalize a notion of a machine that can carry out these tasks? This is what the [Church-Turing Hypothesis](/blog/church-turing-hypothesis) aims to answer. \n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.238691931+05:30",
    "tags": []
  },
  "/blog/derivatives-options": {
    "title": "Derivatives - Options",
    "content": "In [What is the Stock Market?](/blog/what-is-the-stock-market), we learnt about what a stock market is, what stocks (or shares) of companies are and why people trade for them on the stock market. We use the blanket term **Equities** to refer to the company stocks traded on the stock market. \n\n\u003e Equity, typically referred to as shareholders' equity (or owners' equity for privately held companies), represents the amount of money that would be returned to a company's shareholders if all of the assets were liquidated and all of the company's debt was paid off in the case of liquidation. - [Equity Definition: What it is, How It Works and How to Calculate It - Investopedia](https://www.investopedia.com/terms/e/equity.asp)\n\nWhat we will discuss in this chapter, is a specific **derivative** of an financial instrument (here, a stock), called an **option**. \n\n\u003eA derivative is a security whose underlying asset dictates its pricing, risk, and basic term structure. Each derivative has an underlying asset that dictates its pricing, risk, and basic term structure. The perceived risk of the underlying asset influences the perceived risk of the derivative. - [Derivatives 101 - Investopedia](https://www.investopedia.com/articles/optioninvestor/10/derivatives-101.asp)\n\n# History \u0026 Origin\n\n\u003eThe earliest known options were bought around 600 BC by the Greek Philosopher Thales of Miletus. He believed that the coming summer would yield a bumper crop of olives. To make money of this idea, he could have purchased olive presses, which if you were right, would be in great demand, but he didn't have enough money to buy the machines. So instead he went to all the existing olive press owners and paid them a little bit of money to **secure the option to rent their presses in the summer for a specified price**. When the harvest came, Thales was right, there were so many olives that the price of renting a press skyrocketed. Thales paid the press owners their pre-agreed price, and then he rented out the machines at a higher rate and pocketed the difference. Thales had executed the first known call option.\n\u003e\n\u003e**CALL OPTION**\n\u003eA call option gives you the right, but not the obligation to buy something at a later date for a set price known as the strike price.  Call options are useful if you expect the price to go up.\n\u003e\n\u003e**PUT OPTION**\n\u003eYou can also buy a put option, which gives you the right, but not the obligation to sell something at a later date for the strike price. Put options are useful if you expect the price to go down. \n\u003e\n\u003e- [The Trillion Dollar Equation - Veritasium](https://www.youtube.com/watch?v=A5w-dEgIU1M\u0026t=148s)\n\n# A Toy Example \nImagine you're bullish on Reliance Industries (RIL) and think its share price will rise. The current price of RIL is ₹1000, but you can buy a **call option** that gives you the **right, but not the obligation**, to buy RIL shares i**n one year** for **₹1000** (the **strike price**) by paying a **premium**, say ₹100.\n\n\u003e**Quick side note:** There are two main *styles* of options: American and European. American options allow you to exercise the option at any point before the expiry date. European options allow you to exercise the option on the expiry date. We'll focus on European options for now. In certain places, if the trader doesn’t specify exercising instructions, it goes for compulsory exercising by the regulatory authority and that day is termed as the exercise date for that option.\n\nSo, if after a year the price of RIL shoots up to ₹1300, you can use your option to buy shares at ₹1000 and immediately sell them at ₹1300. Here, after factoring in the ₹100 premium you paid, you've pocketed a profit of ₹200 (₹1300 selling price - ₹1000 strike price - ₹100 premium).\n\nHowever, if the share price tanks to ₹700 in a year, you simply let the option expire, losing only the ₹100 you paid for it.\n## PnL ANALYSIS\n![Pasted image 20240310192917](/images/pasted-image-20240310192917.png)\n\n- **If the stock price falls below the strike price, you lose the option premium.** (In this case, you lose ₹100)\n- **But if the price climbs higher than the strike price, you earn the difference minus the option cost.** (Here, you make a profit of ₹200)\n\n|                |                    | **PRICE INCREASES** |            | **PRICE DECREASES** |            |\n| -------------- | ------------------ | ------------------- | ---------- | ------------------- | ---------- |\n| **Instrument** | **Money Invested** | **Profit/Loss**     | **Return** | **Profit/Loss**     | **Return** |\n| Stock          | ₹1000              | ₹300                | **30%**    | -₹300               | **-30%**   |\n| Option         | ₹100               | ₹200                | **200%**   | -₹100               | **-100%**  |\nThe key thing to note here is the percentage difference in returns between the profit and loss scenarios. Options provide **massive leverage**. With the same ₹1000, I can instead choose to buy 10 options and possibly make ₹2000 in profit or stand to lose the entire amount invested (₹1000). \n### Strike Price\nThe predetermined price at which the holder of a stock option has the right (call option) or obligation (put option) to buy or sell the underlying stock / financial instrument.\n### In-the-Money (ITM) Option\nAn option is considered \"in the money\" if the current market price of the stock is already **favorable** for the option holder to exercise the option.\n- For a **call option**, the stock price should be **higher** than the strike price.\n- For a **put option**, the stock price should be **lower** than the strike price.\n### Out-(Of)-the-Money (OTM) Option\nAn option is considered \"out of the money\" if the current market price of the stock is **not favorable** for the option holder to exercise the option.\n- For a **call option**, the stock price should be **lower** than the strike price.\n- For a **put option**, the stock price should be **higher** than the strike price.\n# Advantages of Using Options\n## Limited Downside Risk\nCompared to buying the stock directly, options limit your potential losses. If you had bought RIL shares instead of the option and the price went down to ₹10, you'd lose ₹990. The downside risk with stocks is possibly infinite. With options, you only lose the premium, no matter how low the stock price goes. That said, most traders usually always place a stop-loss on the stocks they have in holding to artificially limit their downside. However, if the stock crashes in a single day, it might not be possible to trade at the stop loss and you might still stand to lose a lot more. With an option, you have a **fixed** downside. \n## Leverage\nOptions offer leverage, which means you can amplify your returns. If you had directly bought RIL at ₹1000 and the price went up to ₹1300, your investment would've grown by 30%. But with the option, you only paid ₹100 upfront. So your profit of ₹200 is actually a 200% return on your investment (₹200 profit / ₹100 option cost). However, remember that if the price falls, you lose your entire ₹100 premium, whereas owning the stock would only mean a loss equivalent to the fall in price. This is both useful and extremely risky if used as a gambling option. In practice, downside with stable stocks is not much compared to the 100% downside with options. \n## Hedging\nOptions can be a hedging tool to manage risk in your portfolio. They were originally created to mitigate risk, and can act like insurance for your stock holdings. To understand this better, let's walk through another toy example. \n### Toy Example\nImagine you're a big believer in HDFC Bank's long-term prospects, but you're worried about a potential market crash and want to hedge yourself against this risk. You currently hold 100 shares of HDFC Bank, currently priced at ₹2500 each (total investment: ₹2,50,000). To hedge against this risk, you **buy put options**. Think of a put option as an **insurance policy** for your stock. You can buy a put option that gives you the right, but not the obligation, to sell your HDFC Bank shares at a predetermined price (strike price) by a specific expiry date. For example, let's say you buy a put option with a strike price of ₹2500 and an expiry date of 3 months for a premium of ₹50 per share (total premium cost: ₹5000 for 100 shares). Now, let's do some PnL analysis. \n#### PnL ANALYSIS\n- **SCENARIO 1: Market Crash**\n\tThe worst happens. The market crashes, and HDFC Bank's share price drops to ₹2000. Without the options hedge, you would lose ₹$(2500 - 2000) \\times 100$ = ₹50,000. But, because you hedged yourself by buying put options, you can exercise your put option and sell your 100 HDFC Bank shares at the predetermined strike price of ₹2500 each (total sell value: ₹2,50,000). Here's the PnL breakdown:\n\t- Loss from stock price drop =\u003e ₹50,000\n\t- Profit from put option: ₹2500 (strike price) $\\times$ 100 shares - ₹2000 (cost of buying HDFC share now) $\\times$ 100 shares - ₹5000 (premium) = ₹45.000\n\tBy using the put option, you limited your loss to the cost of the premium (₹5000) instead of the entire ₹50,000 drop in stock price. \n\t\n- **SCENARIO 2: HDFC Stock Booms!**\n\tThankfully, the market remains stable, and HDFC Bank's share price even goes up to ₹2800. In this case, you wouldn't exercise the put option since you can sell your shares at a higher price in the open market. The put option would simply expire, and you would lose the initial premium of ₹5000. But that's a small price to pay for the security the put option provided during those nervous market moments.\n#### Key Takeaway\nOptions offer a flexible way to hedge your stock portfolio. While they won't completely eliminate risk, they can act as a safety net to minimize your losses in case the stock price takes a tumble. Think of it as setting a stop loss on your stock investments that you know you're guaranteed to bottom out at and you pay the insurance cost upfront. \n# Going Long vs Short on Options\n## Call Options\n### Long Call\nBuying a call option grants you the **right, but not the obligation**, to purchase a stock at a specific price (strike price) by a certain date (expiry). You're essentially betting the stock price will rise above the strike price by expiry. It's a **bullish** strategy.\n### Short Call\nSelling a call option obligates you to sell the underlying stock at the strike price by expiry if the buyer exercises the option. You collect a premium upfront for taking on this obligation. This strategy is used when you're **neutral** or **slightly bullish** on the stock price, believing it won't significantly rise above the strike price by expiry. It carries **unlimited potential loss** if the stock price soars.\n## Put Options\n### Long Put\nBuying a put option grants you the **right, but not the obligation**, to sell a stock at a specific price (strike price) by a certain date (expiry). You're essentially betting the stock price will fall below the strike price by expiry. It's a **bearish** strategy.\n### Short Put\nSelling a put option obligates you to buy the underlying stock at the strike price by expiry if the buyer exercises the option. You collect a premium upfront for taking on this obligation. This strategy is used when you're **bullish** on the stock's long-term prospects but believe it might dip in the short term. It offers **limited profit** but protects against a significant price decline (capped at the difference between the strike price and the purchase price).\n# Settlement Methods\nThis is an exchange specific problem, but different exchanges tackle the method of future / option contract settlement in different ways. The two ways of dealing with this implementation detail are **cash settlement** and **physical delivery**.\n## Cash Settlement\nCash settlement simplifies stock option contracts in India by eliminating the physical delivery of shares. \n\n\u003eA **cash settlement** is a settlement method used in certain futures and options contracts where, upon expiration or exercise, the seller of the financial instrument does not deliver the actual (physical) underlying asset but instead transfers the associated cash position. - [Cash Settlement - Investopedia](https://www.investopedia.com/terms/c/cashsettlement.asp)\n\nImagine you believe RIL's share price will fall and decide to go short on a call option contract for 100 shares. Traditionally, exercising this option would require you to purchase those 100 shares on contract expiry. With cash settlement, you only deal with the price difference at expiry.\n\n- **Profit Scenario:** Let's say you entered the contract when RIL was trading at ₹2,500 per share, with the contract quoting a predetermined purchase price of ₹2,800 per share. If the share price plummets to ₹2,000 at expiry, **the seller wouldn't have to arrange funds for the unwanted shares**. Instead, the long position (who bet on the price going up) would simply credit you with the difference – (₹2,800 - ₹2,000) $\\times$ 100 shares = ₹30,000.\n- **Loss Scenario:** Conversely, if RIL's share price skyrockets to ₹3,500, you wouldn't have to buy 100 shares at ₹3,500 either. You can just pay the difference: ₹70,000. \n\nIt eliminates the hassle of physical share delivery, focusing solely on the price differential at expiry. This translates to a more efficient and safer market for stock exchanges as the risk they have to take up is less. Since the one taking the loss side of the trade doesn't need to have assets to buy the entire underlying stock and just needs to pay the difference, which is often much cheaper in comparison.\n\n## Physical Delivery\nPhysical delivery in stock options is the traditional method where the underlying shares are physically exchanged upon expiry. While cash settlement simplifies things, physical delivery offers a different experience. Physical delivery adds an extra layer of complexity compared to cash settlement. It requires managing the logistics of share certificates and potential delivery costs.\n\nThe Indian National Stock Exchange (since July 2018 expiry contracts), uses physical delivery as the mode of settlement of futures contracts. \n\n\u003eAs stated in this [SEBI circular](https://www.sebi.gov.in/legal/circulars/apr-2018/review-of-framework-for-stocks-in-derivatives-segment_38629.html), starting from July 2018 expiry, F\u0026O positions are being settled moved from cash settlement mode to compulsory physical delivery settlement in a phased manner. Starting from October 2019 expiry, all stock F\u0026O contracts will be compulsorily physically settled. If you hold a position in any Stock F\u0026O contract, at expiry, you will be required to give/take delivery of stocks. \n\u003e\n\u003e- **In the money contracts (ITM)**\n\u003e\tAll ITM contracts which aren’t CTM will be mandatorily exercised by the exchange. This means that anyone holding an ITM option contract will receive/give delivery of stocks depending on whether one is holding call/put options. All the costs arising out of this delivery obligation will be applied to the client’s account.\n\u003e\n\u003e- **Out of the money contracts (OTM)**\n\u003e\tAll OTM options will expire worthless. There will be no delivery obligations arising out of this.\n\u003e\n\u003e**Spread and covered contracts**\n\u003e\tSpread contracts that result in both – take and give delivery obligation will be netted off for the client. For example, you have a bull call spread of Reliance of the same expiry, a lot of long call options of strike 1300 and a lot of short call options of strike 1320 and the spot expires at 1330, this will result in a net-off and there won’t be any delivery obligation.\n\u003e\t\n\u003e- [Policy on settlement of compulsory delivery derivative contracts — Update Oct 2019 - Zerodha](https://zerodha.com/z-connect/general/policy-on-settlement-of-compulsory-delivery-derivative-contracts-update-oct-2019)\n\u003e\n\u003ePhysical delivery of stock options can potentially lead to systemic risk in the capital markets and pose a risk to traders. \n\u003e\n\u003e**The physical delivery risk**\n\u003e\tLike I mentioned earlier, if you hold stock futures or any in the money stock option at the close of expiry, you are assigned to give or take delivery of the entire contract value worth of stocks. Since the risk goes up with respect to the client not having enough cash to take delivery or stock to give delivery, the margins required to hold a future or short option position goes up as we get closer to expiry. Margins required are a minimum of 40% of the contract value for futures on the last day of expiry. For in the money long or buy option positions, a delivery margin is assigned from 4 days before expiry. The margins for in the money long options [go up from 10% to 50% of contract value](https://support.zerodha.com/category/trading-and-markets/margin-leverage-and-product-and-order-types/articles/policy-on-physical-settlement)—50% on the last two days of expiry. If the customer doesn’t have sufficient funds or stocks to give or take delivery, the broker squares off the contract. If the customer shows an intent to hold after the higher margin is blocked, it shows an intent to give or take delivery. \n\u003e\t\n\u003e\tThe risk though comes from out of the money options that suddenly turn in the money on the last day of expiry. No additional margins are blocked for OTM options in the expiry week, and when it suddenly turns in the money, a customer with small amounts of premium and no margin can get assigned to give or take large delivery positions, causing significant risk to the trader and the brokerage firm.\n\u003e\t\n\u003e- [Physical delivery of stock F\u0026O \u0026 their risks - Zerodha](https://zerodha.com/z-connect/general/physical-delivery-of-stock-fo-their-risks)\n\n### A Case Study on the Risk Involved in Physical Delivery Settlement\n\u003eThis happened on Dec expiry, Thursday 30th Dec 2021. Shares of Hindalco closed at Rs 449.65 at expiry. This meant that the Hindalco 450 PE expired just in the money by 35 paise. This meant that everyone who had bought this 450 PE and held it at the expiry was required to deliver Hindalco stock—1075 shares for every 1 lot of Hindalco held. \n\u003e\n\u003eThis is what happened to Hindalco shares on 30th Dec:\n\u003e\n\u003e![Pasted image 20240312051304](/images/pasted-image-20240312051304.png)\n\n\u003e\n\u003eThe stock was above Rs 450 for most of the expiry day and even a few days prior to it. Since it was out of money, no additional physical margins would have been charged, and everyone holding this strike would have assumed that it would expire out of the money. In all likelihood, everyone who held this put option would have written off the trade as a loss and assumed that the maximum loss would be limited to the premium paid. \n\u003e\n\u003eSo at 3 pm, when the Hindalco stock price went below 450, this was how the marketdepth looked like. Those who realized that this option would expire in the money trying to exit, but with no buyers to be able to do so even at Rs 0.05 when the intrinsic value of the strike was Rs 0.35.\n\u003e\n\u003eEveryone holding long puts would have been forced assigned to deliver Hindalco shares. 1 lot of Hindalco = 1075 shares = ~Rs 5lks contract value. Customers who had bought put options with a few thousand rupees were potentially required to deliver tens of lakhs of Hindalco stock. Failing to deliver would have meant short delivery. The [consequences of short delivery](https://support.zerodha.com/category/trading-and-markets/trading-faqs/general/articles/what-is-short-delivery-and-what-are-its-consequences) are losses in terms of auction penalty, apart from the market risk of Hindalco stock price going up from the close of expiry to the auction date. Hindalco stock was up 5% already on Friday, and the auction happens on T+3 days or on Tuesday, and assuming the stock price doesn’t go up further, that is still a whopping loss of Rs 25 (5% of Hindalco) for Rs 0.35 worth of premium at market close. \n\u003e\n\u003eIf this wasn’t puts but calls, there wouldn’t be a short delivery risk, but there would still be a market risk that the customer would be exposed to from the close of expiry to when the customer can sell the stock. But in case of buy delivery (Buy futures, buy calls, short puts), the stock can be sold the next day itself and hence there is no marked to market risk of 3 days. The risk is exponentially more in the case of F\u0026O positions that can lead to short delivery (Short futures, sell calls, buy puts). \n\u003e\n\u003eThe risk exists with futures, short options, and buy ITM options as well. But since there are sufficient margins that also go up closer to expiry, a customer who provides additional margin is willingly holding the position, or else the position is squared off. Because there are no additional physical delivery margins for OTM options and because most option buyers think that when they buy options the maximum they can lose is equal to the premium paid and take no action, the risks go up for the entire ecosystem.\n\u003e\n\u003eApart from the risk to the trader, this can be a systemic issue because if a customer account goes into debit, the liability falls on the broker. A large individual trader or group of customers of a broker could potentially go into a large enough debit to bankrupt the brokerage firm and, in turn, put the risk on other customers as well. Stocks can move drastically on expiry day, and out of the money, option contracts can suddenly move just in the money with no liquidity to exit, making it impossible for brokerage risk management teams to do anything. All option contracts are settled based on the last 30 min average price of the underlying stock and not the last traded price, making this even trickier without knowing if a CTM option strike will actually close in the money or not until post the market closing. And like I explained earlier, the risk is not just in terms of the auction and short delivery, but also marked to market risk for 3 days.\n\u003e\n\u003eForcing traders to give or take large delivery positions can potentially be misused by large traders or operators wanting to manipulate the price movement of stocks.\n\u003e- [Physical delivery of stock F\u0026O \u0026 their risks](https://zerodha.com/z-connect/general/physical-delivery-of-stock-fo-their-risks)\n\n# References\n1. [The Trillion Dollar Equation](https://www.youtube.com/@veritasium)\n2. [What is Zerodha's policy on the physical settlement of equity derivatives on expiry?](https://support.zerodha.com/category/trading-and-markets/margins/margin-leverage-and-product-and-order-types/articles/policy-on-physical-settlement)\n3. [Cash Settlement - Investopedia](https://www.investopedia.com/terms/c/cashsettlement.asp)\n4. [Physical Delivery - Investopedia](https://www.investopedia.com/terms/p/physicaldelivery.asp)\n5. [Policy on settlement of compulsory delivery derivative contracts — Update Oct 2019 - Zerodha](https://zerodha.com/z-connect/general/policy-on-settlement-of-compulsory-delivery-derivative-contracts-update-oct-2019)\n6. [Physical delivery of stock F\u0026O \u0026 their risks - Zerodha](https://zerodha.com/z-connect/general/physical-delivery-of-stock-fo-their-risks)\n",
    "lastmodified": "2024-05-30T09:58:44.245358618+05:30",
    "tags": []
  },
  "/blog/dna-sequencing": {
    "title": "DNA Sequencing",
    "content": "# Preface \u0026 References\nI document topics I've discovered and my exploration of these topics while following the course, [Algorithms for DNA Sequencing, by John Hopkins University](https://www.coursera.org/learn/dna-sequencing) on [Coursera](https://www.coursera.org/). The course is taken by two instructors [Ben Langmead](https://scholar.google.com/citations?user=2JMaTKsAAAAJ\u0026hl=en) and [Jacob Pritt](https://www.coursera.org/instructor/jacobpritt).\n\nWe will study the fundamental ideas, techniques, and data structures needed to analyze DNA sequencing data. In order to put these and related concepts into practice, we will combine what we learn with our programming expertise. Real genome sequences and real sequencing data will be used in our study. We will use Boyer-Moore to enhance naïve precise matching. We then learn indexing, preprocessing, grouping and ordering in indexing, K-mers, k-mer indices and to solve the approximate matching problem. Finally, we will discuss solving the alignment problem and explore interesting topics such as De Brujin Graphs, Eulerian walks and the Shortest common super-string problem. \n\nAlong the way, I document content I've read about while exploring related topics such as suffix string structures and relations to my research work on the STAR aligner.\n\n# DNA Sequencing\nDNA sequencing is a powerful tool used by scientists to study topics such as rare genetic diseases in children, tumors, microbes that live in us, etc. all of which have profound implications on our lives. Sequencing is used pretty much everywhere in live sciences and medicines today. The technology used for sequencing has come down in cost and that has caused for a big 'boom' in the development of this field, similar to how transistor prices going down kick-started the computing industry. \n\nAlgorithms play a key role in this field. Take for example, the effort to sequence the human genome back in the late 90s. There were two popular school's of thought, one who believed that an algorithm crux to the sequencing of the human genome (called de novo assembly) was computationally infeasible in practice, while the others believed that with a large enough compute node it was indeed possible. Finally, it was the second set of people who succeeded by tackling the computational challenge head to head, which allowed them to progress much quicker. It is important for us to know what's possible and what's practical to actually compute. Further, knowing about what work has already been done is the first step to figuring out where the next contribution's should be and how. \n\n## DNA sequencing: Past and present\n\nFirst generation DNA sequencing was a method invented by Fred Sagner and was also known as \"Chain termination\" sequencing. It was quite labour intensive but over the years improved and many tasks were automated. The HGP (Human Genome Project) used 100s of first generation DNA sequencers to sequence the human genome. However, what we're more interested in is what happened to the cost-per-genome ratio right after the end of the Human Genome Project towards the beginning of the 2000s. \n\n![cpg-1](/images/cpg-1.png)\n\nSource: [Sequencing Human Genome Cost - NIH](https://genome.gov/sequencingcosts)\n\nAs we can see, something important happened around the year 2007. This is the year when a new kind of sequencing technology started to be used in life science labs around the world. This technology was called 'next' generation sequencing or 'second' generation sequencing. But the name that probably describes it best is 'massively-parallel' sequencing. Add to this improvements in technology, speed, etc. and there was massive technological and algorithmic improvements in this field since then. \n\n## How DNA Gets Copied\n\n### DNA as Strings\n\nWe are pretty familiar with the double-helix structure of DNA. If we un-ravel this helix and just pick one of the two 'rails', then this strand of the original DNA sequence is split into four sub-sequences made up of the bases A, C, G, or T so that four point sets may be created based on the position of each nucleotide in the original DNA sequence in order to fully use the global information of the DNA sequence. This means that we can represent DNA sequences in the form of a long string containing just the characters 'A', 'C', 'G' and 'T'. \n\n![dna-as-a-string](/images/dna-as-a-string.png)\n\n\nThis has further implications that any read of the DNA sequence simply translates to sub-strings in the original DNA string. This essentially allows us to use the massive literature and work that we have done in the field of string algorithms in the field of DNA sequencing. \n\n### The copying process\n\nDNA exists in base pairs A-T and C-G. Your genome is almost present in every cell in your body, therefore when one of these cells splits, it must transmit a copy of your genome to each of the progeny cells. Consequently, DNA is first double stranded before being divided into two single stranded molecules. It seems as though we split this ladder straight down the middle. We now have two distinct strands as a result of the separation of the complimentary base pairs. The genome sequence is still recorded on each strand, and the two strands are complementary to one another despite their separation. It acts as a sort of template and provides the instructions necessary for re-creating the original DNA sequence. The name of the molecule, the enzyme that puts the complementary bases in it's place, is called DNA polymerase. Given one of these single-stranded templates and a base, DNA polymerase is a tiny biological device that can synthesize DNA (this base might be floating around somewhere just waiting to be incorporated). With these two elements, the polymerase will piecemeal construct the complementary strand to produce a double-stranded replica of the template strand.\n\n## Massively parallel DNA sequencers\n\nReads refer to random sub-strings picked from a DNA sequence. One human chromosome is on the order 1 million bases long. Massively parallel sequencers produce reads that are around 100-150 bases long, but produce a huge amount of them. A sequencer 'eavesdrops' on the DNA copying process to sequence many templates simultaneously. This is how the process works in a nutshell. \n\n1.  Convert input DNA into short single-stranded templates.\n\n2.  Deposit on slide (scattering the multiple strands randomly across surface)\n\n3.  Add DNA polymerase to this slide\n\n4.  Add bases (raw material) to this slide, which are 'terminated' by a special chemical piece which doesn't allow the polymerase to construct anything **on top** of the base it adds to the template.\n\n5.  Take a 'top-down' snapshot of the entire slide. (Terminators are engineered to glow a certain color which allows easy identification of the base)\n\n6.  Remove the terminators\n\n7.  Repeat until all the templates are built fully\n\nThe following is a visual depiction of the same.\n\n![mpds-1](/images/mpds-1.png)\n\n![mpds-2](/images/mpds-2.png)\n\n![mpds-3](/images/mpds-3.png)\n\n![mpds-4](/images/mpds-4.png)\n\n\n## Sequencing Errors and Base Quality\n\nThe process described above is largely accurate, but a minor detail we skimped out on is that before the sequencing begins, we amplify each template strand with multiple copies in a cluster. This allows the camera to more easily spot the glowing color of each cluster as just one strand is not enough to accurately distinguish the color. However, there is a hidden problem here. Say during one of the build cycles one of the bases in the solution is unterminated. This would cause the polymerase to go ahead and place the next base as well on top of what should've been this cycle's base. Now because this is a cluster, the majority color would still likely dominate. However, notice that once a base is out of cycle, it will always remain out of cycle. This means that with more and more cycles, the rate of error gets higher and higher. \n\n![mpds-e](/images/mpds-e.png)\n\n\nTo counter this, we developed software called the 'base caller' which analyzer the images and tries to attach a confidence score to how confident it is about the base for each cluster in each cycle. The value reported is called the 'base quality.'\n\n$$Base \\ Quality \\ (Q) = -10 \\cdot \\log_{10}p$$\n\n$p$ is the probability that the base call is incorrect. This scale provides an easier interpretation of the probability value. For example, $Q = 10 \\to 1$ in $10$ chance that the call is incorrect. $Q = 20 \\to 1$ in $100$, and so on. The probability computation probably involves ML model nowadays but a reasonable measure one would imagine is simply computing \n\n$$p(not \\ orange) = \\frac{non \\ orange \\ light}{total \\ light}$$\n\nif it predicts orange as the base.\n\n## Re-Constructing Genome Using the Sequencing Reads\n\nOnce we have the billions of tiny sequenced reads, they are analogous to having a lot of tiny paper cutouts of a newspaper. Good for a collage, but not useful for reading the news. To make sense of these reads, we need to be able to stitch this back into one complete picture (back to the genome). To do this, we consider the following two cases.\n\n-   When there already exists a genome reconstruction of the same species\n\n-   When there exists no such reconstruction. (We are sequencing a new exotic species.)\n\n### Case I\n\nWe rely on the fact that the genomes of two different animals of the same species have $\\gt 99 \\%$ similarity in their genomes. That is, if we already have a sequenced genome from the same species, we can be guaranteed that the new genome will be **extremely** similar to the already reconstructed genome. If we imagine genome reconstruction similar to putting together a jigsaw puzzle, the already existing genome reconstruction is something like a photograph of the completed puzzle. We can then rely on this existing construction to guide us in putting together the jigsaw. \n\nIn our context, what we can do is match these short reads to the original sequence and see which places in the original sequence are very good matches for our read. We then use these markings as a guide to where the short read actually fits in the puzzle of the complete genome reconstruction. However, as we see in one of the practice labs, simply doing *exact string matching* is not sufficient. In practice, we find that trying to find exact matches of the short read in the original sequence gives **very low** matches in the original sequence. In the context of reconstructing our puzzle, this means we have very few clues to go off of for reconstruction. This happens primarily due to two main reasons:\n\n1.  The DNA sequencing process can have errors as mentioned above. Perfect reads are not very likely.\n\n2.  The 'snapshot' we are following will not be an exact match and will have some (albeit few) differences.\n\nBut the primary reason exact matching fails is due to the error(s) inherent in the DNA sequencing process. This essentially gives us a fair idea why exact string matching won't be sufficient for solving our problem. We later explore approximate matching and alignment problems which are primarily what we use to tackle this issue. ([Algorithms for Approximate String Matching - Alignment](/blog/algorithms-for-approximate-string-matching-alignment), [Booyer-Moore \u0026 Knuth-Morris-Pratt for Exact Matching](/blog/booyer-moore-knuth-morris-pratt-for-exact-matching)). \n\n### Case II\n\nIn the case where there exists no already existing snapshot to follow, we will have to tackle the same problem faced by the people working on the original Human Genome Project (HGP). We rely on techniques of de novo assembly to reconstruct the genome string. We will discuss this in more detail towards the end of the course. \n\n",
    "lastmodified": "2024-05-30T09:58:44.248691962+05:30",
    "tags": []
  },
  "/blog/dp-as-dags-shortest-path-on-dags-lis-in-o-nlogn": {
    "title": "DP as DAGs,  Shortest Path on DAGs \u0026 LIS in O(nlogn)",
    "content": "Over the past few notes, we learned about developing efficient strategies to solving computational problems by using the greedy idea ([Set Cover \u0026 Approximation Algorithms](/blog/set-cover-approximation-algorithms), [More Greedy Algorithms! Kruskal's \u0026 Disjoint Set Union](/blog/more-greedy-algorithms-kruskal-s-disjoint-set-union), [Activity Selection \u0026 Huffman Encoding](/blog/activity-selection-huffman-encoding)). The greedy idea focuses on choosing the most optimum solution at a local stage and reducing what's left to a subproblem with the same structure. This is great when problems have a locally optimum solution and have optimal substructure properties. But what do we do when this is not the case? What to do when greedy does not work?\n# Dynamic Programming\n\nDynamic programming is a technique used to efficiently solve problems that check the **optimal substructure** criteria. If we are able to reduce the given problem to smaller subproblems with the same structure, then we can employ a technique similar to divide and conquer. The idea here is that we can model a problem as a **transition** of sorts from the solution to its subproblem. If this is true, then it is possible that we might have a **LOT** of overlapping subproblems. Notice that instead of repeatedly recomputing the solutions to these subproblems, we can store them in memory somewhere and simply look up the solution for a specific subproblem in $O(1)$ instead of recomputing it.\n\n## Visualizing DP as DAGs\n\nA **very** interesting view of visualizing DP was discussed. DP is usually presented as some form of DP table, transition states, and magic for loops that compute the answer to some problem. I often find this **extremely** unintuitive and difficult to follow along with. DP by nature is nothing but the idea of recursively solving a problem by splitting it into smaller problems and applying memoization whenever possible to overlapping subproblems.\n\nA very cool way to visualize this is by modeling the recursion tree for solving a problem in terms of DAGs (directed acyclic graphs).\n\nWe mentioned that DP relies on a problem having a recursive solution. That is, it must be possible to model it as a transition from the solution to its subproblems.\n\nNote that if we attempted to visualize this recursive method of solving as a graph, with some solutions dependent on the solution of its subproblems, we can **never** have a cycle. The presence of a cycle would imply that a problem depends on its subproblem and the subproblem depends on its parent. Computing this would lead to an infinite cycle.\n\n![dp-1](/images/dp-1.png)\n\n\nSay we wish to compute $a_1$. For the problem structure depicted on the left, it is **impossible** to compute it recursively as we would be in an infinite cycle. The problem on the right however can be solved by independently computing the solution for $a_2, a_3$ and then computing $a_1$.\n\nThis also means that we can think of every recursive problem in some kind of DAG-like structure.\n\n### Visualizing Fibonacci\n\nConsider the famous Fibonacci problem. We can recursively state $F_n = F_{n-1}+F_{n-2}, \\ _{n \\gt1 }$\n\nLet's try to visualize the recursion tree for $F_4$ (which is also a DAG)\n\n![dp-2](/images/dp-2.png)\n\n\nNotice that we are computing $F_2$ multiple times. (Assume $F_0$ and $F_1$ are known constants).\n\nWe can eliminate this overlap by computing it **just** once. This allows us to model the DAG as follows,\n\n![dp-3](/images/dp-3.png)\n\n\nBy using the once computed $F_2$ to compute $F_3$, notice that we managed to eliminate an entire subtree of recursion. **This** is the core idea behind DP. By saving the states of previous computations, we are effectively able to eliminate recomputation for **all** overlapping subproblems, thus considerably reducing the complexity of our solution.\n\nNote that DP is essentially a brute force. It can recursively try a greedy/brute force over all possible solutions for a smaller subproblem, then use this to again use the same strategy and solve a bigger problem. DP allows us to apply brute force to the problem by reducing it into smaller subproblems which we can attempt to solve using brute force / other techniques.\n\n## The shortest path on a DAG\n\nConsider the problem of the shortest path on a DAG. The problem simply asks, _\"Given a DAG with V vertices and E weighted edges, compute the shortest path from Vertex $v_i$ to every other vertex on the graph.\"_\n\nOn normal graphs without negative edge weights, the Dijkstra algorithm can compute the solution in $O(V+ElogV)$ time. But given that our graph is directed, and has **no cycles**, _can we do better?_\n\nIn fact, yes we can. A very simple solution exists to this problem which is capable of computing the answer in just $O(V+E)$ time.\n\n### Toposort\n\nNotice that for **every** DAG, there exists at least one topological sort of its vertices which is valid. This is trivially inferred from the fact that by definition, a DAG does not contain any cycles. This implies that there must be at least one arrangement where we can list vertices in a topological ordering.\n\nA topological ordering essentially guarantees that when we reach vertex $v_i$ in the ordering, there is **no** path from $v_i$ to **ANY** vertex $v_j$ where $j \\lt i$. Further, there is **no** path from any vertex $v_k$ to $v_i$ where $k \\gt i$.\n\nThis means that the shortest path to $v_i$ will be a result of some transition from the shortest paths to all vertices $v_j$ such that $\\exists \\ (v_j, v_i) \\in E$. And since $j \\lt i$ must be true, we can simply process the vertices in **topological** order.\n\n### The algorithm\n\n$$ \\text{Toposort V in O(V+E)} \\\\ \\text{Initialize all dist[ \\ ] values to } \\infty \\\\ \\text{for each } v \\in V-\\{s\\} \\text{ in topological order:} \\\\ dist(v) = min_{(u, v)\\in E}\\{dist(u)+d(u,v)\\} $$\n\nNotice that the _recursive_ step in this algorithm is that to compute $dist(v)$ we require the value of $dist(u)$. Now, $dist(u)$ can always be computed recursively, but notice that **because** we're going in topological order, it **MUST** be true that any such $u$ where $\\exists (u,v)\\in E$ **must** have already been processed. This implies that we must have already computed the value of $dist(u)$.\n\nSo instead of recursively recomputing it, we can just store the value of $dist(u)$ and access the computed value in $O(1)$.\n\nAnd that's it. We've managed to use Dynamic Programming to solve our problem in $O(V+E)$.\n\n## Longest path in a DAG?\n\nWhat about the problem of finding the longest path from some vertex $s$ to every other vertex on a DAG? How can we efficiently compute this? Unlike with shortest path problems, computing the longest path in a general graph is **NP-Complete**. That is, there exists **NO** polynomial-time algorithm that is capable of computing the solution.\n\nWhy? A very common way to understand the longest path problem is as follows.\n\n\u003eThe longest path between two given vertices s and t in a weighted graph G is the same thing as the shortest path in a graph −G derived from G by changing every weight to its negation. Therefore, if shortest paths can be found in −G, then longest paths can also be found in G.\n\nThat is, by simply negating the weights, the longest path problem can be reduced to the shortest path problem. So... what's the issue? Why is one NP-Complete? Note that the Dijkstra algorithm for finding shortest paths relies on the fact that all edge weights are positive. This is to ensure that there exist no negative cycles in the graph. If a negative cycle exists, the shortest path is simply $-\\infty$. By negating the weights on our graph $G$, we might end up with a negative weight cycle.\n\n**However**, note that this does not affect DAGs. In DAGs, the longest path problem is the **same** as the shortest path problem. Just, with negative edge weights. Or another way to think of it is as the exact same recursion but instead of defining $dist(v)$ as the minimum of $dist(u) + d(u,v)$ we simply define it as the maximum of the recursion. This simple change effectively changes the algorithm to the longest path solution.\n\n```cpp\nfor t in toposort:\n    for each node from t:\n      dp[node] = min(dp[t] + distance(t, node), dp[node]); // max for longest path\n```\n\n# The LIS problem\n\nThe LIS (Longest increasing subsequence) problem asks the following question, _\"Given an ordered list of n integers, what is the length of the longest increasing subsequence that belongs to this list?\"_\n\nLet's take an example.\n\nLet the list $arr$ be $[10, 22, 9, 33, 21, 50, 41, 60, 80]$. One possible solution to this list is as given below.\n\n![dp-4](/images/dp-4.png)\n\n\nSo how do we solve this problem?\n\nBefore we attempt to solve this problem, let us take a short detour to learn about the idea of _reductions_ in the field of computational complexity theory. Linked here: [Complexity Theory - Reductions](/blog/complexity-theory-reductions).\n## Back to finding the LIS\n\nConsider the following idea, let's transform the given array $arr = [10, 22, 9, 33, 21, 50, 41, 60, 80]$ to a DAG by applying the following rules.\n\nThere exists a directed edge from the element at position $i$ to another element at position $j$ **if and only if**\n\n1. $i \\lt j$, and\n2. $arr[i] \\lt arr[j]$.\n\nLet's consider the implications of such a construction. What does it **mean** to find the LIS of some given array? Especially after this transformation.\n\n![dp-5](/images/dp-5.png)\n\n\nNotice that there is **no difference** between the longest path on a DAG problem and finding the LIS of an array after we have performed this transformation to the array. In such a DAG, every \"path\" is a sequence of increasing numbers. We wish to find the longest such sequence. This, in turn, translates to simply finding the longest such path on the graph.\n\nOur graph enumerates all such increasing subsequences. The longest path is, therefore, also the longest increasing subsequence.\n\nWe have hence, **successfully** found a reduction to the problem. We have shown that by applying the transformation of the array to a DAG which was constructed by following the above two rules, we have managed to reduce the problem of finding the LIS of an array to the problem of finding the longest path on a DAG.\n\nSadly, our reduction is not as efficient as the solution to $g$ itself. Notice that constructing the graph is of order $O(V^2)$. Let us define the construction of our graph (the reduction) as a function $R(x)$ which takes in input $x$ for problem $f$ (LIS) and converts it to input for problem $g$ (Longest path on a DAG.)\n\nOur overall complexity will be $O(R(x)) + O(g(x))$. Since the reduction step is $O(V^2)$, our final solution will be $O(n^2)$. We may have up to $n^2$ edges.\n\nHence we have a solution to the LIS problem which computes the answer in $O(n^2)$.\n\nSimply transform it to the increasing subsequence DAG and compute its longest path.\n\nHowever, the natural question to ask again is, _can we do better?_\n\n## Computing LIS in $O(nlogn)$\n\nThe reduction to convert the LIS problem to the longest path on DAGs was great and gave us an $O(n^2)$ solution. But how can we do better? Is there any redundancy in our computation? Is there some extra information unique to this problem that we haven't exploited yet?\n\nTurns out, there is.\n\nLet's define our DP state as follows.\n\n$\\text{Let } dp[i] \\text{ be the smallest element at which a subsequence of length } i \\text{ terminates.}$\n\nIf we can compute $dp[i]$ for all $i$ from $1 \\ to \\ n$, the largest $i$ for which $dp[i]$ contains a valid value will be our answer. How do we compute this? Consider the following naïve algorithm, here $a$ is our input array and $dp$ is our dp table.\n\n```cpp\ndp[0 to n] = ∞\nd[0] = -INF\nfor i from 0 to n-1\n    for j from 1 to n\n        if (dp[j-1] \u003c a[i] and a[i] \u003c dp[j])\n            dp[j] = a[i]\n```\n\nWhy is the above algorithm correct? Notice that the outer-loop is essentially trying to decide where to include the value $a[i]$. Further, notice that when we are iterating over $i$, the inner loop will never assign a value to any $dp[j]$ where $j \\gt i+1$.\n\nIntuitively this makes sense because at this point in time we are only considering the first $[0, i]$ segment/subarray. Such a subarray only has $i+1$ elements and can hence not be part of any $dp[j]$ where $j \\gt i+1$. If we look at what the algorithm is doing, $a[i]$ can **only** replace $dp[0 \\ to \\ i+1]$. Notice that after $i+1$, $dp[j] \\geq a[i]$. This means the replacement can never happen.\n\nNotice that according to our algorithm, the condition $dp[j-1] \\lt a[i] \\text{ and } a[i] \\lt dp[j]$ implies that the LIS of length $j-1$ must be lesser than $a[i]$ and $a[i]$ must be lesser than whatever the current computed smallest element is which terminates a LIS of length $j.$ The first part of the condition makes sure the LIS is increasing and the second part makes sure it is the smallest such element that fits the condition.\n\n**Key observation:** Note that we will at most, update **one** value and the DP array will always be **sorted.**\n\nWhy? Note that $dp[i]$ is the **smallest** element at which an increasing subsequence of length $i$ terminates. The keyword here is **smallest**.\n\nThis implies that, if in the future, $dp[i]$ is replaced by some $a[j]$, then $a[j]$ is the smallest element which terminates an increasing sequence of length $i$. What is the implication of this sentence?\n\nIf $a[j]$ is the **smallest** element that terminates an increasing sequence of length $i$, then it **can never** be the **smallest** element in the array that terminates an increasing sequence of any length $\\gt i$. The fact that it is used at position $i$ means that **any** such terminating value for **any position** $\\gt i$ **must** be $\\gt a[i]$.\n\nIf this is understood, then we have inferred that the array is both sorted and we require at most one replacement in each iteration of the outer loop. We have managed to transform the inner loops job into a simpler problem. The inner loop is actually trying to solve the following question, _\"Given a sorted array, what is the first number that is strictly greater than $a[i]$?\"_\n\nNote that the above question can be trivially solved using binary search. This means that our inner loop can be replaced with a simple binary search to achieve $O(nlogn)$ overall time complexity.\n\n### Code\n```cpp\n// Sample psuedo code\nint lis(int arr[], int n) {\n    int dp[n+1] = INF;\n    d[0] = -INF;\n\n    for (int i = 0; i \u003c n; i++) {\n        int j = upper_bound(dp, dp+n+1, a[i]); // Computed in log(n) by binary search\n        if (dp[j-1] \u003c a[i] \u0026\u0026 a[i] \u003c dp[j])\n            dp[j] = a[i];\n    }\n\n    for (int i = n; i \u003e= 0; i--)\n        if (dp[i] \u003c INF) return i;\n}\n```\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [Huffman Codes: An Information Theory Perspective - Reducible](https://youtu.be/B3y0RsVCyrw?si=5kFqRPa_XsGxpqBr)",
    "lastmodified": "2024-05-30T09:58:44.252025305+05:30",
    "tags": []
  },
  "/blog/extended-euclidean-algorithm": {
    "title": "Extended Euclidean Algorithm",
    "content": "# An efficient algorithm to find GCD\n\nThe problem we're attempting to solve is pretty simple. Given two numbers $a \\text{ and } b$, find their GCD.\n\nA naïve way to solve this problem is as follows, find all prime factors of the two numbers, and multiply all the common ones. However, even with fast prime finding algorithms like Eratosthenes sieve, it will work only for small numbers. It is not feasible to precompute the sieve for large numbers.\n\nHowever, by combining the idea behind iterating over all roots of a number and the sieve logic we can actually come up with an algorithm that obtains the prime factors of a number in $O(\\sqrt n . logn)$. How? Consider this approach, let us iterate from $i = 2 \\to \\sqrt n$ and **if $i$** divides $n$, then keep dividing $n$ by $i$. Each of these divisions implies that $i$ is each time, a prime factor of $n$.\n\nThe code looks something like this\n\n```jsx\nfor(int i=2; i*i \u003c= n; i++){\n\t\twhile(n % i == 0){\n\t\t\t\tprimefactors.insert(i);\n\t\t\t\tn /= i;\n\t\t}\n}\nif(n\u003e2) primefactors.insert(n);\n```\n\nConsider let us try to **prove** why this works.\n\n**Our claim:** _Every composite number has at least one prime factor less than or equal to the square root of itself._\n\n**Proof:** Since our number is composite, it must have at least one-factor $a$. This implies that there exists some number $b$ such that $a \\times b = n$. Now, we want to prove that either\n\n$a \\leq \\sqrt n \\text{ or } b \\leq \\sqrt n$\n\nWe prove this by contradiction. Assume that both $a \\gt \\sqrt n$ and $b \\gt \\sqrt n$.\n\nThis implies that $a\\times b \\gt \\sqrt n \\times \\sqrt n \\implies ab \\gt n$. This is a contradiction. Hence either $a \\leq \\sqrt n$ or $b \\leq \\sqrt n$.\n\nNow, W.L.O.G. assumes that $a \\leq \\sqrt n$. Either $a$ can be prime or by the fundamental theorem or arithmetic, $a$ must have a prime divisor $\\lt a$. In both cases, our claim is true.\n\nThe inner while loop which removes every instance of the prime factor is pretty similar to the marking composite step in the sieve algorithm. Once they're removed we can move\n\nHowever, notice that in the worst case, if $n$ itself happens to be prime our algorithm would have a worst-case runtime of $O(\\sqrt n)$. Since the prime factors are obtained in sorted order, we can always run 2 pointers in linear time to get the common factors. But the worst case is still $O(\\sqrt n)$,\n\nThe question is, _can we do better?_\n\n## Euclid's Algorithm\n\nThe algorithm Euclid proposed to solve the GCD problem was extremely simple. According to him,\n\n$$ gcd(a, b) = \\begin{cases} a, \u0026 \\text{ if } b =0 \\\\ gcd(b, a \\ mod \\ b) \u0026 \\text{otherwise.} \\end{cases} $$\n\n### Proof\n\nFirst, note that the second argument strictly decreases with each iteration of the Euclidean algorithm, implying that the method will always halt (because the arguments are never negative).\n\nNow, to prove correctness, we must prove $gcd(a, b)= gcd(b, a \\ mod \\ b) \\forall a \\geq 0, b \\gt 0$\n\nFirst, notice that $a \\ mod \\ b = a-b.\\lfloor \\frac{a}{b} \\rfloor$\n\nWith this reduction, let us try to prove a simpler identity. $gcd(a, b) = gcd(a-b, b)$.\n\nLet $gcd(a, b) = d.$ This implies that $d|a \\text{ and } d|b$. This also means that $d | (a-b)$ and $d|b$.\n\nThis is true for all common factors of $a$ and $b$. Therefore, $(a, b)$ and $(a-b, b)$ share the same set of common factors. Hence $gcd(a,b) = gcd(a-b, b)$.\n\nNow notice that $a \\ mod \\ b$ is simply performing this operation $\\lfloor \\frac{a}{b} \\rfloor$ times. Hence $gcd(a, b) = gcd(a \\mod b, b)$. Hence proved.\n\nNow that we have managed to prove correctness, let us try to put an upper bound on the running time of this algorithm.\n\n### Time complexity\n\nNotice that in every step of the recursion, one of the arguments get cut in at least half. Consider the operation $a \\ mod \\ b$.\n\n**If $b \\leq \\frac{a}{2}$ :** Then by property of the mod, $a \\ mod \\ b \\lt \\frac{a}{2}$\n\n**If $b \\gt \\frac{a}{2}$:** Then the operation $a \\ mod \\ b = a-b \\lt \\frac{a}{2}$\n\nTherefore the number of recursive steps will be at max $log(min(a, b))$. And this is indeed the complexity of our algorithm.\n\nFurther note that for a $n$-bit number, since the operands get halved at every other step, we are removing one bit of the numbers per every 2 recursions. Hence the number of calls is linear in the number of bits of the number. The modulo operation is quadratic in the number of bits of the number. Hence final complexity $O(n^3)$.\n\n\u003e Lamé's theorem is used to estimate the method's running time, and it establishes an unexpected link between the Euclidean algorithm and the Fibonacci sequence: The Euclidean algorithm executes at most $n-2$ recursive calls if $a \\gt b \\geq 1$ and $b \\lt F_n$ for some $n$. Furthermore, the upper bound of this theorem can be shown to be optimal. $gcd(a,b)$ will do exactly $n-2$ recursive calls when $a = F_n$ and $b = F_{n-1}$. In other words, the worst-case input for Euclid's algorithm is a series of Fibonacci numbers.\n\n### Code\n\nBelow is the C++ implementation of the algorithm. Notice the conciseness that writing the algorithm recursively gives us.\n\n```cpp\nint gcd (int a, int b) {\n    if (b == 0) return a;\n    else return gcd (b, a % b);\n}\n```\n\nHowever, we can also write it iteratively for more efficiency as follows\n\n```cpp\nint gcd (int a, int b) {\n    while (b) {\n        a %= b;\n        swap(a, b);\n    }\n    return a;\n}\n```\n\n# The Extended Euclidean Algorithm\n\nWhile the Euclidean algorithm calculates only the greatest common divisor (GCD) of two integers $a$ and $b$, the extended version also finds a way to represent GCD in terms of $a$ and $b$, i.e. coefficients $x$ and $y$ for which:\n\n$a \\cdot x + b \\cdot y = \\gcd(a, b)$\n\n## The algorithm\n\nLet the GCD of $a$ and $b$ be $g$.\n\nWe can find this representation by simply extending the previously explained algorithm. Notice that the previous algorithm terminates when $b=0$ and $a = g$. At this step, we can easily find the coefficients $g = g \\cdot 1 + 0 \\cdot 0$ .\n\nFrom here, the main idea is to **backtrack** through our recursive calls. The only transition we need to describe is the transition of $\u003cx, y\u003e$ from $(b, a \\ mod \\ b) \\to (a,b)$ ,\n\nLet's suppose that we have the coefficients $\u003cx_1, y_1\u003e$ for $(b, a\\ mod \\ b)$, This implies that the following equation holds true always.\n\n$$ b \\cdot x_1 + (a \\bmod b) \\cdot y_1 = g $$\n\nNow, we want to find the transition of $\u003cx_1, y_1\u003e \\to \u003cx_2, y_2\u003e$ for the pair $(a, b)$. That is,\n\n$$ a \\cdot x + b \\cdot y = g $$\n\nRecall that we can write $a \\bmod b = a - \\left\\lfloor \\frac{a}{b} \\right\\rfloor \\cdot b$. Now, substituting this in the previous equation, we get,\n\n$$ g = b \\cdot x_1 + (a \\bmod b) \\cdot y_1 = b \\cdot x_1 + \\left(a - \\left\\lfloor \\frac{a}{b} \\right\\rfloor \\cdot b \\right) \\cdot y_1 $$\n\nWe can now solve this equation to get,\n\n$$ g = a \\cdot y_1 + b \\cdot \\left( x_1 - y_1 \\cdot \\left\\lfloor \\frac{a}{b} \\right\\rfloor \\right) $$\n\nAnd that's it! We have found our transition.\n\n$$ \\begin{cases} x_2 = y_1 \\\\ y_2 = x_1 - y_1 \\cdot \\left\\lfloor \\frac{a}{b} \\right\\rfloor \\end{cases} $$\n\n## Proof\n\n**Lemma:** If $d$ divides both $a$ and $b$, and $d = ax + by$ for some integers $x$ and $y$, then necessarily $d = gcd(a, b)$.\n\n**Proof:**\n\n1. Since it is given that $d|a$ and $d|b$, then $d$ is a common divisor of $a$ and $b$. This implies that $d \\leq gcd(a, b)$ by definition of $gcd$.\n2. Since $gcd(a, b)|a$ and $gcd(a, b)|b$, it implies that $gcd(a,b)|(ax+by) \\implies gcd(a,b)|d$. Therefore $gcd(a,b) \\leq d$.\n\nWe have $gcd(a,b) \\leq d$ and $d \\leq gcd(a,b)$. Therefore it must be true that $d = gcd(a,b)$\n\n## Code\n\nAs before, we can implement this both recursively and iteratively. The recursive version is quite concise. Below is the C++ implementation of the recursive code.\n\n```cpp\nint gcd(int a, int b, int \u0026x, int \u0026y) {\n    if (b == 0) {\n        x = 1;\n        y = 0;\n        return a;\n    }\n    int x1, y1;\n    int d = gcd(b, a % b, x1, y1);\n    x = y1;\n    y = x1 - y1 * (a / b);\n    return d;\n}\n```\n\n# Modular Division (Multiplicative Inverse)\n\nWhen doing operations in the modular field, we perform operations between two numbers $a, b$ belonging to the field like $(a+b)mod \\ k$. Here $+$ is the binary operation and $Z_k$ is the modular field.\n\nNotice that while this is fine for integer addition, subtraction, and multiplication, it is not so easy to define division.\n\nIn the world of modular arithmetic, we define the modular multiplicative inverse of an integer $a$ as an integer $x$ such that $a \\cdot x \\equiv 1 \\mod k$. Further, in terms of defining notation, we write such an integer $x = a^{-1}$.\n\nFurther, notice that the modular inverse of an element belonging to the modular field of $Z_k$ does not always exist. For example, consider $3 \\in Z_6$. $Z_6 = \\{0, 1, 2, 3, 4, 5\\}$. By trying all possible elements we can confirm that there exists no integer $\\in Z_6$ such that $3 \\cdot x \\equiv 1 \\ mod \\ 6$. It can be proven that the modular inverse for an integer $a$ exists in the modular field $Z_k$ **if and only if $gcd(a, k) = 1$.** That is, $a$ and $k$ are relatively prime.\n\n## Finding the modular inverse using the Extended Euclidean Algorithm\n\nLet's take the following equation,\n\n$$ a \\cdot x + k \\cdot y = 1 $$\n\nRemember that the modular inverse of $a$ exists, if and only if $gcd(a, k) = 1$. Further, notice that the above equation can be solved by the extended euclidean algorithm.\n\nOnce the **EED** algorithm gives us the values of $x$ and $y$, we can mod the entire expression with $k$ to get\n\n$$ a \\cdot x + 0 \\equiv 1 \\ mod \\ k $$\n\nThen $x = a^{-1}$\n\n### Code\n\nThe code for it is pretty simple. Below is C++ implementation of the same\n\n```cpp\nint a, k, x, y;\nint g = EED(a, k, x, y);\nif(g!=1) // No solution\nelse // (x % m + m) % m is our solution \n```\n\nNote that we do the addition %m + m step to make sure $x$ is positive.\n\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [Extended Euclidean Algorithm - cpalgorithms](https://cp-algorithms.com/algebra/extended-euclid-algorithm.html)\n",
    "lastmodified": "2024-05-30T09:58:44.255358649+05:30",
    "tags": []
  },
  "/blog/how-the-economic-machine-works-by-ray-dalio": {
    "title": "How the Economic Machine Works by Ray Dalio",
    "content": "This is going to be my written understanding of Ray Dalio's absolutely beautiful 30 minute animated video to answer the question, \"Hoe does the economy really work?\" To any reader, watch the original video first. It's one of the best presented educational videos on the internet in my opinion. \n\n\u003ciframe title=\"How The Economic Machine Works by Ray Dalio\" src=\"https://www.youtube.com/embed/PHe0bXAIuk0?si=FUQfilqcRfebsYf7\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n# The Machine Template\nRay describes the economy as a simple machine, a machine that many people don't understand or agree upon. He claims he as a **simple but practical** economical template that paints an accurate picture of the economy and markets in a country, which allowed him to sidestep the global financial crisis and has worked well for him for the last 30 years.\n\n\u003chr\u003e\n\n## Forces That Drive the Economy\nAccording to Ray, there are three primary forces that drive the economy. **Productivity growth**, the **short-term debt cycle** and the **long-term debt cycle** .\n\n![Pasted image 20240307000515](/images/pasted-image-20240307000515.png)\n\n\nHis template, focuses on only layering these three forces together, and observing the effect it has on **transactions** and how that in turn affects the economy. \n\n![Pasted image 20240307000718](/images/pasted-image-20240307000718.png)\n\n\n### Transactions\nTo start modelling the economy, we should first understand what we're talking about when we say the word 'economy.' \n\n\u003eAn economy is simply the sum of the transactions that make it up, and a transaction is a very simple thing. Everyone makes transactions all the time. Every time you \"buy\" something, you create a transaction. Each transaction consists of a buyer exchanging **money** or **credit**, for **goods**, **services**, or **financial assets**.\n\nSpending **credit**, is the same as spending money. So simply summing up the amount of credit spent and money spent, we can calculate the total spending of a nation. And the **total amount of spending drives the economy**. Let the *total spending* in a country be $T$. If $T_m$ is the total money spent, and $T_c$ is the total credit spent, we have $T = T_m + T_c$. Now, let the total weighted-quantity sold be $Q$. Now the price of $Q$ is just $P = \\frac{T}{Q}$. Keep this in mind for later, it'll help reason about why prices increase during an **inflation** and decrease during a **deflation**. So if a buyer, pays price $P$ to buy some item / service $Q$, we have a transaction. \n\n**Transactions are the basic building block of the economic machine** described by Ray. All cycles and forces in an economy are driven by transactions. Understanding transactions will help us understand the entire structure and working of the economic machine. \n## Market \nA market consists of all the buyers and sellers making transactions for the **same** thing. For example, we have a wheat market, a stock market, a car market, etc. \n## Economy\nAn economy consists of all of the transactions in **all** of its markets. If you add up the total spending and the total quantity sold in all of the markets, we have everything we need to know to understand the economy. \n## The Participants \nThe participants involved in transactions are **people**, **businesses**, **banks** \u0026 **governments**. \n### The Government\nThe most important buyer and seller that we want to understand is the government. It is primarily the actions of the government that control inflation, deflation, and all the other large scale economic events that occur. The government consists of two parts:\n1. **CENTRAL GOVERNMENT**\n\tThe central government is the body responsible for collecting taxes and interacting with the people. This is important because they collect taxes from the people, usually proportional to wealth and redistribute it back to the have-not in difficult times via stimulus checks and other similar means of support. \n2. **CENTRAL BANK**\n\tIt is different from other buyers and sellers because it **controls the amount of money and credit in the economy**. The central bank is both capable of **influencing interest rates** and **printing new money**. The central bank is an important player, perhaps the most important player in the flow of credit in the economy. \n### People \u0026 Businesses\nBoth work roughly the same way. Both people and businesses use the capital and income available to them to buy assets and other items by partaking in transactions. In particular, they act as propagators in the economic machine. People and businesses often take up debt, giving them access to credit to increase their spending. This will be covered in detail soon.\n### Banks\nSimilar to the described central bank above. They cannot print money, but they can adjust the interest rate and thereby help control the flow of credit in the economy.\n## Credit\nIs is **THE** most important part of the economy and often the least understood. It is important because it's extremely volatile and because a huge proportion of transactions that happen are done with credit. In 2021, the national debt of India amounted to around $2.36$ trillion USD while the money in circulation in 2021 is only 0.341 trillion USD. \n- [Statista - National Debt of India](https://www.statista.com/statistics/531619/national-debt-of-india/)\n- [Statista - Value of currency in circulation in India](https://www.statista.com/statistics/1028099/india-currency-in-circulation-value/)\n### How Is Credit Created?\nOut of thin air actually. Just like buyers and sellers make transactions in a market, so do **lenders** and **borrowers**. (Slight misconception involved here, will be cleared in the \"MISCONCEPTIONS\" section).\n#### Lender and Borrower Dynamic\n##### Lender Incentive\nLender's want to make their money work for them and generate more profits. So they often give **loans** to borrowers who agree to pay back the loan with some **interest** over a period of time. Essentially, they give money to a borrower on an agreement that the borrower will return them the principal amount along with some extra interest within some period of time. This assures them profits as long as the borrower doesn't go bankrupt and **default** on the loan. \n##### Borrower Incentive\nThis depends. It is possible the borrower might want to buy something that they can't outright afford, like a new car or house. Or maybe they want it as an investment into funding a new business they might want to start. The credit (loan) allows them to stretch a big investment over a period of time (EMI (Equated Monthly Installment) for example) or in funding their risky business idea that might generate them profits, using which they can pay back the loan and still have made more profit than incurred losses in interest payback. You can also think of it this way, for an individual that used his credit to take his loan, he uses credit to smoothen his buying power over time. He knows he'll have more buying power in the future as his income will increase, so he wants to increase his buying power now itself, and not have it be a direct function of his exact current income.\n\nEssentially, this lending and borrowing of money agreement allows both borrowers and lenders to get what they want. **When such an agreement is made, credit is created**. However, there are some important factors that control credit creation.\n#### Factors That Influence This Dynamic\n##### Interest Rate\nAs mentioned previously, borrowers pay back the principal borrowed along with some extra interest. This is typically simple or compound interest. In either case, the extra amount repaid depends on a number called the **interest rate**. When **interest rates are high**, there is **less borrowing** because it's expensive. But when **interest rates are low**, **borrowing increases** because it's cheaper.\n##### Creditworthy Score\nThe risk associated with making money for the lender comes from whether or not they believe the borrower to be able to pay back the principal in worst-case situations. For example, a lender would probably not reach this agreement with someone who wants to gamble all the borrowed money in a casino. Usually, borrowers have some private financial assets they can put down as **collateral** for the loan. In case they go bankrupt, the lender can rely on the borrower to sell the asset and repay the borrower with that money. However, keep in mind that this is still **not** completely safe, as the price of financial assets can go up or down depending on the demand and supply for that asset. However, income and collateral are still widely used to judge the creditworthy-ness of individuals to decide how much credit can be trusted to them. \n\nWhen any two people engage in this agreement, with a borrower promising to repay and a lender who believes in the borrowers and gives him the money, **credit is created**. Yes, out of thin air. \n## Debt\nCredit goes by many names. Debt is like the twin-brother of credit. As soon as credit is created, so is debt. Let's say a lender $A$ loans Rs. $x$ to a borrower $B$. Immediately, credit and debt are created. The lender receives a credit **asset** of value Rs. $x$ + $I$ , where $I$ is the interest to be paid, and the borrower receives a debt **liability** of value Rs. $x+I$.\n\n![Pasted image 20240307042628](/images/pasted-image-20240307042628.png)\n\n### How Is Credit Destroyed?\nIn the future, when the borrower repays the loan, plus the interest amount, he completes the transaction and both the asset (credit) and liability (debt) associated with that loan disappear. \n## Common Misconceptions\nThe word credit is often used to describe many different things. In the context of this template, we will use the term credit to describe two things:\n1. **AN ASSET** - Here, we mean credit as essentially a bookkeeping entry for the lender that he expected to make some $x + I$ amount by a future date. \n2. **EXTRA CAPITAL** - Credit can also refer to the 'extra' borrowed money that the borrower now has access to. This is mostly what we'll be referring to when we use the word credit in the future. This is important, because this is a major driving force of economic growth. Essentially, think of it this way, no new money is actually created, but the amount of money involved in transactions is now greater. Why? The money given by the lender is money that the lender would've **otherwise just let lie dormant as cash**. However, because he sees a net positive ROI on lending the money to the borrower, he gives away this money and then this money is **injected into the economy via the borrower who uses this extra capital to make transactions**. \n   This extra capital is just money that the borrower borrows from his future self. However, by borrowing money and making transactions with it, he essentially allows this 'extra' money which would've not partaken in transactions to now help increase the expenditure in the economy at the present. \n# The Operation of the Machine \nYou can probably figure out everything from scratch as long as you keep in mind this single mantra, that pretty much dictates all the why's in the operation of this machine.\n\n\u003e **One person's spending is another person's income.** Every dollar you spend, someone else earns. And every dollar you earn, someone else has spent. So when you spend more, someone else earns more. When someone's income rises, it makes lenders more willing to lend him money because now he's more worthy of credit. ...\n\u003e So increased income allows increased borrowing, which allows increased spending. And since one person's spending is another person's income, this leads to more increased borrowing and so on.  (Remember, the opposite also holds true). \n\n![Pasted image 20240307134703](/images/pasted-image-20240307134703.png)\n\n## Productivity Growth\nRemember, in a transaction, we get income (money) for whatever goods / services / financial assets we offer in return. The price we can put on the sold quantity depends on the quality and usefulness of the quantity. Over a period of time, humans learn, humans innovate and humans evolve. In essence, over a period of time, an experienced professional is able to do quality work in a shorter period of time. Or a startup founder who has studied the market and worked on a solution for years is able to use that experience to develop new goods that solve difficult problems. We see further by standing on the giants of yesterday. Those that are hardworking and inventive increase their productivity and living standards faster over a long period of time. Productivity growth is stable, strong, predictable and grows slowly. Because it doesn't fluctuate much, it is **not a big driver of economic swings**.\n\n![Pasted image 20240307235115](/images/pasted-image-20240307235115.png)\n\n## Short-Term Debt Cycle\nDebt on the other hand, is far more short-term focused and occurs in cycles. When you take up debt, it allows you to **consume more than you produce**. And when you are paying back debt, you are forced to **consume less than you produce**. Debt swings, according to Dalio, occur in two big cycles. Short-term debt cycles occur $\\approx 5-8$ years. These swings are primarily caused based on how much credit there is active in the economy. \n\n![Pasted image 20240308043010](/images/pasted-image-20240308043010.png)\n\n\nLet's see how it works. These 'cycles', as described by Ray, occur because of our **human nature** to borrow money. Borrowing money is essentially just a way to manage buying power over a period of time.  To spend more than you make today, you borrow from your future self. Essentially you create a time in the future when you need to spend less than you make in order to pay it back. One reason why you would do this is to smooth-en your buying power over time. Remember that in the future, your income increases because you're more productive and you are likely to make more money. So it makes sense to you to borrow more from your future self as you know your future self will be likely making a lot more money. And getting access to more money early is a lot more profitable because if you're able to invest it well, you can generate a lot of money from it due to the simple [POWER OF COMPOUND INTEREST](/blog/power-of-compound-interest). \n\n![Pasted image 20240308043345](/images/pasted-image-20240308043345.png)\n\n\nThe very act of borrowing, creates a very **mechanical** and **predictable** series of events that will happen in the future. If you borrow now, you **must** pay it back later. This is just as true as it for an individual, as it is for the economy as a whole. This series of events will always occur in a cycle. \n### Is Credit Always Good?\nNot really. If you borrowed credit just to spend more on liabilities and you do not increase your productivity or income growth in any significant way, it it bad. You now create a time in the future where you need to pay back this debt with the same income level you have today. It wasn't used to create any meaningful growth. \n\n\u003e\"_If you can't buy it twice_, _don't buy it_\"\n\n### Super-Charging the Economy\nWhen interest rates are low, a lot of people are encouraged to borrow, and this has the effect of supercharging incomes in an economy. This is how it works, let's say an individual $A$ earns $100,000\\$$ a year and has no debt. He is credit worthy enough to borrow $10,000\\$$. Now, let's say he spends all his money. His spending is now $110,000\\$$ instead of just $100,000\\$$. And applying the single most important principle used to describe this whole machine, _since one person's spending is another person's income_, somebody else makes $110,000\\$$. This makes them credit worthy to borrow $11,000\\$$. Then they spend $121, 000\\$$ (instead of just $100,000\\$$), and again since their spending is somebody else's income... this process self-reinforces itself and causes living standards and income to rise for everyone!\n\n![Pasted image 20240308170223](/images/pasted-image-20240308170223.png)\n\n\nSince a lot of the money flowing in the economy is actually _credit_ (money that would've otherwise **not** been involved in transactions), it encourages people to borrow since they are credit worthy to borrow more money due to their increased income (which was a result of others spending borrowed money) and become a self-reinforcing pattern. This pattern, like discussed before, will be a cycle. And if the cycle goes up, it eventually must come down. \n### Phase 1 - Economic Expansion\nAs economic activity increases, there's an **economic expansion**. This is the **first phase** of the short-term debt cycle. People's income rises, and spending continue to increase. However, this rise in spending is not matched with an increase in the quantity and quality of goods produced. This is because the increase in quantity / quality of goods is associated with productivity growth, and that is a much slower and stabler force than the fast-paced credit cycle. Now because we have increased spending (demand) and the supply remains the same, prices of goods go up. This is called **inflation**. However, the central bank doesn't want too much inflation, because it causes problems. Seeing prices rise, it raises interest rates, launching us into the second phase of the short-term debt cycle. \n### Phase 2 - Economic Recession\nWith higher interest rates, borrowing becomes less appealing. Fewer people borrow money, and the cost of existing debts increases. This naturally causes humans to prioritize debt repayments more and borrow less. Put in this situation, the amount of money individuals have left, after paying debt repayments and **not** borrowing credit, is lesser than what it would've been in the previous phase. And *since one person's spending is another person's income*, people's income drops. This pattern is exactly the same as we had previously during the first phase, but in the opposite direction. Incomes drop, borrowing slows and the economy falls as the money spent in transactions drops sharply. And in contrast to before, the supply remains constant but the demand drop since people spend less. This causes the price of goods to go down, we call this **deflation**. Economic activity decreases and we have a **recession**.\n### Flipping the Switch\nRecession is clearly not good for the economy. Remember that the central bank increased interest rated on credit borrowing to help beat inflation. If they caused this to happen, they have the power to pull the economy out of a recession to. If the recession becomes too severe and inflation is no longer a problem, they can simply lower interest rates again and plunge the economy back into the first phase of the cycle. \n\nThe short-term debt cycle is primarily controlled by the central bank. When there is surplus credit flowing through the economy, there is expansion and people's incomes and living standards rises. When there is too little credit in the economy and spending slows, there's a recession. And it is the central bank which carefully varies interest rates on credit borrowing to keep this cycle going in a mechanical fashion. According to Ray, this cycle typically lasts 5-8 years and repeats itself over and over again for decades. \n## Long-Term Debt Cycle\nHowever, in practice, we will often see that the bottom and top of each cycle finish with more growth than the previous cycle **and more debt**. \n\n![Pasted image 20240308172045](/images/pasted-image-20240308172045.png)\n\n\nThis occurs primarily due to **human nature / greed**. We as humans have an inclination to borrow and spend more instead of paying back debt. Humans push it too far. Paying back debt is boring and painful, spending more than you earn gives you more gratifying experiences. But this mountain of debt can't keep growing forever, we eventually reach a point where debts grow faster than incomes and this creates the **long-term** debt cycle. \n\n### The Expansion Phase\nThe crazy thing is, despite borrowers debt-burdens growing, lenders even more freely extend credit. Because when living in a particular point along this curve, people focus only on what's been happening lately. Looking at a curve from too up-close just shows you a straight line. When we are in the expansion phase of the long-term debt cycle, incomes are rising, financial asset prices are sky-rocketing, the stock market hits new peaks. Why? Because of all the credit flowing through the economy, a lot of credit is being used in transactions. Causing all the aforementioned events to occur. It's an economic boom! When people do a lot of this, and most of it is actually using credit, we call this a **bubble**.\n\nEven though debts are growing, incomes have been growing nearly as fast to offset them. And more importantly, remember the factors that make someone creditworthy? Income levels and collateral. Because financial asset prices are sky-rocketing, people have more collateral to lay down and take debts on. Note that the value of these financial assets are not fixed, and depends on the supply-and-demand for said asset in the market. And even though its increased price is mostly caused due to the artificial-demand induced due to credit in the market, most people don't realize this as they're looking at just what's been happening lately. \n#### Debt Burden\nWe call the **ratio** of **debt** to **income** the **debt-burden**. $DB = \\frac{debt}{income}$. As long as incomes rise, the debt-burden remains manageable. \n\nBecause people own financial assets as investments that make their prices rise even higher. People feel wealthy and remain creditworthy despite being under major debt. \n\n### The Recession Phase - The Deleveraging\nHowever, over decades, the debt-burden eventually begins to tip towards the unmanageable side. Incomes can't rise fast enough to match the debt-repayment amounts. Just like in the recession phase of the short-term debt cycle, people cut spending. This again becomes a self-enforcing pattern like in the short-term debt cycle case. But with one **crucial difference**. \n\nIn a short-term debt cycle, there was a savior installed in place to flip the switch and get the economy up and growing again. The central bank could just flip the switch, lower interest-prices and allow people to borrow credit again. Increasing spending, and creating a self-reinforcing pattern that boosts the economy. However, in a deleveraging, **interest prices are already at $0\\%$**. The central-bank cannot easily 'flip a switch' and fix the economy.\n\nDebt-burdens have simply become too big. In a deleveraging, people cut spending, the stock market crashes, prices of financial assets drop as demand for them subsides, social tensions rise and the whole imaginary bubble pop, leaving behind a total shit-show. Because people's income falls and debt repayments rise, borrowers are squeezed. They are no longer creditworthy, and borrowers **cannot borrow more to payoff debts**. This causes borrowers to go to their backup plan, the same backup plan the lenders trusted to give borrowers their loans. Collateral. Borrowers are forced to sell off their financial assets in order to complete their debt repayments. But this happens at a massive scale. Because the supply for financial assets suddenly soars, at the same time people cut their spending, the prices of these financial assets drop. To much lower than what it was previously, making borrowers even less credit worthy and still unable to payoff debts. \n\nPeople feel poor. Less spending -\u003e Less Income -\u003e Less Wealth -\u003e Less Credit -\u003e Less Borrowing -\u003e Less spending -\u003e ...\n\nLenders don't lend more money because they are still waiting for debt repayments that they're no longer sure if the borrower can payback and they need all their money with them to survive the deleveraging. Borrowers can't borrow any more money and can't pay off their existing debts. Their entire economy becomes not credit worthy. \n\n#### How to Get Out of a Deleveraging?\nThere are 4 ways to reduce the debt burden. Some inflationary and some deflationary. They usually tend to happen in the following order:\n##### Cut Spending (↓)\nThis is usually the first and instinctive reaction to most people during a deleveraging. Because the news is filled with bad news about the economy and because they realize they have a lot of debts to pay and are unable to borrow, people cut spending in order to pay off their debts. However, *because one person's spending is another person's income*, this process is deflationary in nature. Demand drops, making asset prices and incomes drop. This has the opposite effect of helping because although debts might reduce, incomes also reduce. And often incomes fall faster than debts, causing the debt-burden to grow even worse.\n##### Debt Restructuring  (↓)\nThe next call to action is usually debt restructuring. In this economic-crisis, many borrowers find themselves **unable** to pay off their debts. And remember, a **borrower's debts are a lender's assets**. When borrowers are unable to repay the bank (lender) and default, people get nervous that the bank won't be able to repay them and rush to withdraw money from the bank. Banks get squeezed and banks default on their debts, often starting a chain of bank collapses because there is often a *lot* of money shared between banks. \n\n\u003eMuch of an economic-depression is people discovering much of what they thought was their wealth isn't really there.  \n\nIn such situations, lenders often agree to **debt restructuring**. This means lenders agree to restructure the terms of the original agreement on which the loan was taken. This could include changes where the interest is lowered, the amount to be paid back could be reduced, or the time-frame over which the debt has to be payed back could be increased. Lenders are forced to agree in these situations because the choice for them is essentially between \"all of nothing\" or \"half of something\". This is however, still deflationary in nature because income and asset values fall as a result of debt restructuring. Money that could be injected into the system via transactions doesn't enter because the agreement no longer requires it to.  \n##### Wealth Redistribution (↓)\nThe next option to help get out of a deleveraging is to redistribute wealth between the rich and the have-not. Because people's income falls, the amount they pay in taxes falls and this directly impacts the income of the central government. At the same time the government finds itself in a situation where it needs to increase its spending because unemployment has risen due to companies laying off workers, etc. and left many people with inadequate savings to survive. This situation is called a **budget deficit**. A budget deficit occurs when expenditures surpass revenue and then up impacting the financial health of a country. In such a situation, especially since wealth is typically concentrated in the hands of a small percentage of the population, governments raise taxes on wealthy which allows to redistribute wealth in the economy from the rich to the have-not. This naturally increases resentment between the rich and the poor and could lead to social disorder. This sort of a situation lead to Hitler coming to power, war in Europe, and depression in the United States.\n##### Print Money (↑)\nThe issue in the first place is that a lot of what people thought was their money didn't really exist. It was credit which was borrowed from their future self. While the central bank cannot lower interest rates further, like in the case of short-term debt cycles, it still has one final card up it's sleeve. It can print money! Unlike the aforementioned methods, printing money is inflationary. This is because the act of printing money increases the amount of money in circulation (being used in transactions), thus increasing the income of people. And because _one person's spending is another person's income_, this causes incomes to rise and asset prices to increase. However, note that the central bank cannot do this alone. Because it can only buy financial assets (which still drives up asset prices) and bonds, it focuses on buying government bonds, essentially loaning money to the central government, which then uses this money and distributes it to the poor and helps combat the budget deficit. But note that this still **increases government debt**. \n\nFurther, note that printing money essentially lowers the worth / purchasing power of a single note. Policy makers need to carefully balance the inflationary ways with the deflationary ways to smooth-en the debt cycle while keeping panic and social disorder at minimums. Dalio terms a deleveraging where these forces are balanced perfectly a *beautiful* deleveraging.  In a beautiful deleveraging, debts decline relative to income, real economic growth is positive and inflation is kept under control. \n\nThis isn't easy to balance. Printing money is an easy solution but this could lead to unacceptably high inflation by thrashing the power of a rupee / dollar / *insert-currency-name-here* note. Because printing money is the only method that allows incomes to rise, it must be done. The only way to reduce the debt-burden is to increase incomes in the country relative to interest rates on debt repayments so that incomes are *just* high enough to outgrow debt payments. When this balance is achieved, it allows the country to grow out of the deleveraging phase of the long-term debt cycle in a less-dramatic and smooth fashion. \n\nEventually, incomes increase, people become credit worthy and we enter the expansion phase of the long-term debt cycle again. Dalio expects the deleveraging phase to take around a **decade**. \n\n# Rules for Handling Macro-Economics\n1. Don't have debts rise faster than income.\n\t-\u003e Fairly obvious, debt-burdens getting out of control is why we have depression phases of cycles. If we can handle it smoothly by increasing our future self's income (via increasing productivity), we would be dealing with a much smoother and less-dramatic entry back to the expansion phases. \n2. Don't have income rise faster than productivity.\n\t-\u003e This makes you unmotivated and non-competitive if you don't have a reason to keep pushing yourself. \n3. Do all that you can to raise your productivity. \n\t-\u003e Fairly obvious again, raising productivity is what let's you increase income in the long run and let you handle the debt repayments caused by your past-self borrowing money from you on the trust that his future self would have much higher income and buying power. \n\nResources referred to:\n1. [How The Economic Machine Works by Ray Dalio](https://www.youtube.com/@principlesbyraydalio)\n2. [Investopedia](https://www.investopedia.com/)",
    "lastmodified": "2024-05-30T09:58:44.262025336+05:30",
    "tags": []
  },
  "/blog/how-to-analyze-algorithms-proving-a-lower-bound-for-comparison-based-sorting": {
    "title": "How to Analyze Algorithms? Proving a Lower Bound for Comparison Based Sorting",
    "content": "# Analysis\nBefore we can attempt to solve problems, we must answer 3 important questions about the algorithm\n## Is it correct?\nThis is of the highest importance. An algorithm is simply _wrong_ if it does not produce the correct output for all input. How do we prove correctness? We cannot simply rely on test cases, this isn't \"proof\" enough as discussed in [Are there computational problems that computers cannot solve?](/blog/are-there-computational-problems-that-computers-cannot-solve). There may be many edge cases it can fail on that remain undiscovered. An inductive proof is far superior to \"proof by AC.\" Only in the case of highly complicated algorithms can we rely on test cases (as it gets more complicated to prove it mathematically.)\n\nThere are also probabilistic algorithms that give the correct output _most_ of the time. However, we will cover probabilistic algorithms at a later point in time and focus on deterministic algorithms for now.\n## How much time?\nHow much time? (or any other resource, does it require to compute the solution). Time complexity is usually expressed as an function in terms of input size and underlying model of computation used. Knowing this is important in knowing when and how to feasibly execute an algorithm. If we know an algorithm is $O(2^n)$ there's no point executing it for a very large $n$ as it would take more than our lifetime to compute.   \nNote that for computing time complexity, it is very important to fix the underlying model of computation. On a single tape Turing machine, each bit in the number corresponds to an unit of computation. On the other hand, if we think of a modern machine with 64-bit registers and we restrict our integer inputs to $\\leq 2^{64}$ then we can model arithmetic as constant time operations. But for a single tape Turing machine, addition would require $O(n)$ time. \n## Can we do better?\nWe have to see if we can do better, and if not, we must prove the optimality of our current solution. But how do you prove the optimality of a solution? The number of possible \"programs\" as discussed in [Are there computational problems that computers cannot solve?](/blog/are-there-computational-problems-that-computers-cannot-solve) is countable, but it is still infinite. We cannot attempt to go through all of them individually. To solve this issue, we need to instead build an image of the capabilities of a given model of computation and enforce constraints that allow us to come up with a proof for the same. Remember, time complexity is defined for a fixed model of computation. So we use that as the base and draw implications from there. For example, we know that the lower bound for comparison based sorting is $O(nlog_2(n))$ in the order of comparisons made. How? Let's discuss how we prove this.\n### Proving a lower bound of $\\Omega(nlog_2(n))$ for comparison based sorting\n#### Decision Tree Model\nTo prove this bound, let's come up with an useful model to visualize / enumerate the number of comparisons based in an arbitrary sorting algorithm. Any comparison based sorting algorithm must perform $x$ number of comparisons to end up with a result. Further, which comparison it makes on the $i^{th}$ iteration depends on the result of the $(i-1)^{th}$ iteration. In short, we can imagine the comparisons made to be a decision tree. Here's what the decision tree would look like for sorting an array of size three. \n\n```mermaid\ngraph TD\n    A --\u003e B{A \u003c B?}\n    B --\u003e|Yes| C{B \u003c C?}\n    C --\u003e|Yes| D[A, B, C]\n    C --\u003e|No| E{A \u003c C?}\n    E --\u003e|Yes| F[A, C, B]\n    E --\u003e|No| G[C, A, B]\n    B --\u003e|No| H{A \u003c C?}\n    H --\u003e|Yes| I{B \u003c C?}\n    I --\u003e|Yes| J[A, B, C]\n    I --\u003e|No| K[A, C, B]\n    H --\u003e|No| L{B \u003c A?}\n    L --\u003e|Yes| M[B, A, C]\n    L --\u003e|No| N[C, B, A]\n```\nNote that every non-leaf node in this diagram represents a comparison, and every leaf-node represents one of the resultant permutations generated by the sort. Every **path** from the root to a leaf node represents a sequence of execution of the sort. No matter in what order the sorting algorithm does the comparisons, it needs to take one of these paths to end up at a leaf node. And since we have this nice model, we can say that the worst case number of comparisons performed will be the height of the tree. If we can prove a lower bound for the height of the tree, we have proved the lower bound for comparison based sorting algorithms. \n\nNote that for any array of size $n$ (containing unique numbers), there exist $n!$ different ways to order these positions. Every inverse permutation of all possible permutations of $1 \\cdots n$ can be an input. This implies that there must be $n!$ leaf nodes, corresponding to every such input permutation. If there are less than $n!$ leaf nodes, we would not be able to solve each of the $n!$ inputs correctly since the solution for that input would not exist in the decision tree. Second, note that the decision tree is a **binary tree**, which implies that if the tree has height $h$, it can have at max $2^h$ leaf nodes. From this, we get\n$$\n\\begin{aligned}\n2^h \\geq n! \\\\\nh \\geq log_2(n!) = log_2(n\\cdot(n-1)\\cdot(n-2)\\cdots1) \\\\ \nh \\geq \\sum_{i=1}^nlog_2(i) = \\sum_{i=1}^{\\frac{n}{2}-1}log_2(i) + \\sum_{i=\\frac{n}{2}}^nlog_2(i) \\\\\nh \\geq 0 + \\sum_{i=\\frac{n}{2}}^nlog_2(i) \\geq \\frac{n}{2} \\cdot log_2(\\frac{n}{2}) \\\\\n\\implies \\Omega(nlog_2(n))\n\\end{aligned}\n$$\n### Alternate method\nFor any given sequence of $n$ elements, there exist $n!$ permutations in which they can be arranged. The sorting problem requires us to find one such permutation out of all $n!$ permutations such that $a_i \u003c a_{i+1} \\ \\forall a_{i\u003cn}\\in A$ where A is our sequence.\n\nLet's suppose that we can find the sorted array using just $k$ comparisons. This would imply that we can represent permutations $0$ to $(n! - 1)$ using just $k$ bits. To represent $(n! - 1)$ in bits, we need $log(n! - 1)$ bits, therefore $k \\geq log(n! - 1) \\implies k \\geq log(n!)$ bits is required.\n\n\u003e **Why is this true?** We assumed that our algorithm will be able to find the answer to the sorting problem using just $k$ comparisons. If we consider the result of a single comparison, it can have **exactly** 2 values. True or false. If we consider $k$ comparisons, we can have $2^k$ possible results. In essence, each comparison is able to cut **half** of the permutations we need to consider. Each of those $2^k$ results can be used to uniquely identify some permutation. Now, the answer to the sorting problem must be one of the $n!$ possible permutations. For our algorithm to successfully find the right permutation for every given input, it must be able to _at_ _least_ uniquely identify each of the $n!$ possible permutations. This leads us to the conclusion that $2^k \\geq n! \\implies k \\geq log_2(n!)$ bits/operations are required.\n\n$$ \nlog(n!) = log(n)+log(n-1)+\\dots+ log(\\frac{n}{2})+\\dots+log(2)+log(1)\n$$\n\nLet's discard the bottom half of this sum. The sum we get after discarding the bottom half must be **lesser** than the original sum as all the values are **positive.** This gives the following,\n\n$$ log(n!)\\geq log(\\frac{n}{2})+log(\\frac{n}{2}+1)+\\dots+log(n-1)+log(n) $$\n\nBy reducing all the terms inside $log$ to just $\\frac{n}{2}$, we will only get a sum **lesser** than the above as $log$ is an increasing function. This gives,\n\n$$ log(n!) \\geq log(\\frac{n}{2})+\\dots+log(\\frac{n}{2}) \\\\ log(n!) \\geq \\frac{n}{2}log(\\frac{n}{2}) \\\\ = \\Omega(nlogn) $$\n\nIt is also not very difficult to find an **upper bound** to $log(n!)$. Similar to the calculation above,\n\n$$ log(n!) = log(n)+log(n-1)+\\dots+log(2)+log(1) $$\n\nReplacing each of the above terms with $log(n)$ will only give us a sum greater than the above one as $log$ is an increasing function. This gives,\n\n$$ log(n!) \\leq log(n)+\\dots+log(n) \\\\ log(n!) \\leq nlog(n) \\\\ = \\Theta(nlogn) $$\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [CLRS, Introduction to Algorithms](https://en.wikipedia.org/wiki/Introduction_to_Algorithms)",
    "lastmodified": "2024-05-30T09:58:44.26535868+05:30",
    "tags": []
  },
  "/blog/introduction-to-complexity-theory": {
    "title": "Introduction to Complexity Theory",
    "content": "# Introduction to Complexity Theory\n\nIn most algorithms courses, students are taught a plethora of algorithms that are capable of solving many interesting problems. It often tends to internally suggest to the student that most problems have solutions. Solutions that are feasible to compute on their machines should they need to. On the contrary, most problems are unsolvable and even fewer are computable in any feasible amount of time.\n\nComputation complexity theory is a field of study where we attempt to classify \"computational\" problems according to their resource usage and relate these classes to one another. We begin by defining a few to classify algorithms based on running time.\n\n1. $P$\n2. $EXP$\n3. $NP$\n4. $R$\n\n## P or PTIME\n\nThis is a fundamental complexity class in the field of complexity theory. We define $P$ as the set of **all** decision problems that can be solved by a **deterministic** Turing machine in polynomial time.\n\n\u003eA more formal definition is given below: A language $L$ is said to be in $P$ $\\iff$there exists a **deterministic** Turing machine $M$ such that:\n\u003e\n\u003e1. $M$ runs for polynomial time on **all** inputs\n\u003e2. $\\forall l \\in L$, $M$ outputs 1\n\u003e3. $\\forall l \\in L$, $M$ outputs 0\n\nWhen we talk about computational problems, we like problems in $P$. These problems are feasible for computers to compute in a reasonable amount of time.\n\n## EXP or EXPTIME\n\nThis is the class of **all** decision problems that can be solved by a **deterministic** Turing machine in exponential time. Similar to how we gave a formal definition for $P$, it is easy to see that we can modify the same formal definition to fit $EXP$ as well.\n\n## R or RTIME\n\nThe $R$ here stands for \"recursive.\" Back when complexity theory was being developed, there was a different idea of what the word 'recursive' meant. But in essence, $R$ is simply the set of all decision problems that can be solved by a deterministic Turing machine in some finite amount of time.\n\nIt might seem as though all problems are solvable by a Turing machine in some finite time and hence unnecessary to have a class dedicated to it. But this is not true.\n\n## Undecidable problems\n\nAn undecidable problem is a decision problem for which it has been proven that it is impossible to develop an algorithm that always leads to a valid yes-or-no answer. To prove that there exist undecidable problems, it suffices to provide even just one example of an undecidable problem.\n\n## The Halting Problem\n\nOne of the most famous examples of undecidable problems is the halting problem, put forth by Alan Turing himself. Using this, Turing proved that there do indeed exist undecidable problems. But this isn't just the only reason why the halting problem is \"special.\"\n\nThe halting problem poses the following question: _\"Given the description of an arbitrary program and a finite input, decide whether the program finishes running or will run forever.\"_\n\nIn fact, if the halting problem were decidable, we would be able to know a LOT more than what we do today. Proving conjectures would be a LOT easier and we might have made a lot of progress in many fields.\n\n### Solve Goldbach's conjecture?\n\nConsider [Goldbach's conjecture](https://en.wikipedia.org/wiki/Goldbach%27s_conjecture). It states that _every even whole number greater than 2 is the sum of two prime numbers._\n\nUsing computers, we have tested the conjecture for a large range of numbers. This conjecture is \"probably\" true, but till today, we have **no** proof for this statement. Simply checking for large ranges is simply not enough. Finding even just one counter-example, even if this counterexample is 10s of digits long is enough to prove the conjecture **false**.\n\nLet's say we constructed a Turing machine $M$ that executes the below algorithm (given in pseudocode).\n\n```python\niterate from i : 0 -\u003e \\\\infty:\n\t\titerate from j : 0 -\u003e i:\n\t\t\t\tif j is prime and (i-j) is prime: \n\t\t\t\t\t\tmove to next even i\n\t\tif none of its summation were both prime:\n\t\t\t\toutput i\n\t\t\t\thalt  # We have disproved Goldbach's conjecture!\n```\n\nThis definition of our Turing machine is capable of disproving Goldbach's conjecture. But the question is, how long do we let it run for? If the number is not small, it might take years to find this number. Maybe millions of years. We do not know. And even worse, if the conjecture is indeed true, then this machine will **never** halt. It will keep running forever.\n\n**However, what if the halting problem was decidable?**\n\nWhat if, we could construct another such Turing machine $M_1$ this time which solves the halting problem? We can feed it $M$ as input, and let $M_1$ solve the halting problem.\n\nIf $M_1$ outputs \"halt\" then there **must** be some input for which Goldbach's conjecture fails. We have disproved it.\n\nIf $M_1$ outputs \"run forever\" then Goldbach's conjecture **must** be true. It is no longer a conjecture, we have managed to prove it!\n\nBeing able to solve the halting problem would help us solve so many such conjectures. Take the twin primes conjecture, for example, we would be able to solve it. We would be so much more powerful and armed in terms of the knowledge available to us. However, sadly, Alan Turing proved that the halting problem is undecidable. And the proof is quite fascinating to describe\n\n### The proof\n\nWe will prove that the halting problem is undecidable using contradiction. Therefore, we begin by assuming that there exists some Turing machine $M$ that is capable of solving the Halting problem.\n\nMore formally, there exists some deterministic Turing machine $M$which accepts some other Turing machine $A$ and $A$'s input $X$ as input and outputs 1 or \"Halt\" if $A$ will half on that input and 0 or \"Run forever\" if $A$ will not halt on that input.\n\nNow, let's construct another Turing machine \"Vader\" which does something quite interesting. Given some Turing machine $A$ and its input $X$, Vader first runs $M$on the input. If $M$ returns \"halt\", Vader will run forever. And if $M$ returns \"run forever\", Vader will halt.\n\nThis is still fine, but the masterstroke that Turing came up with was to give Vader, itself as input!\n\nIn the above explanation, we make $A = Vader$ and $X = Vader$. For Vader to work, it will first run $M$on this input. For simplicity, we will call the input program Vader as iVader. There can only be two possible outputs,\n\n1. **$M$ returns \"Halt\"**\n    \n    This means that $M$ thinks that iVader will halt when run on itself. _However_, when $M$ returns \"halt\", Vader will run forever. Remember that Vader is given itself as input. The input iVader and the program Vader are identical. $M$ predicts that iVader will halt but we know that Vader will run forever. We have a contradiction.\n    \n2. **$M$ returns \"Run forever\"**\n    \n    Again, we have ourselves a contradiction. Just like before, $M$ thinks that iVader will run forever, but we know that Vader will halt. The emphasis here is that iVader and Vader here are the same Turing machines that run on the same input.\n    \n\nTherefore, neither of the cases can be true. The fact that we have a contradiction here arises from the fact that our assumption is wrong. There can exist no such Turing machine $M$ which can solve the Halting problem.\n\n## NP or NP-TIME\n\nThere are a couple of different definitions used to define the class $NP$.\n\nOne of these definitions says, NP is the set of problems that can be solved in polynomial time by a **nondeterministic** Turing machine. Notice that the keyword here is **nondeterministic.** What this essentially means that at every \"step\" in the computation, the machine _always_ picks the right path. Let's say a Turing machine had states similar to the below picture. A non-deterministic machine would accept any input string that has **at least one accepting run** in its model. It is \"lucky\" in the sense that it is always capable of picking the right choice and moving to the right state which guarantees ending at a **YES** result as long as such a run exists in its model.\n\n![pnp-1](/images/pnp-1.png)\n\n\nThe second definition for $NP$ calls it the set of decision problems for which the problem instances, where the answer is \"yes\", have proofs verifiable in polynomial time by a deterministic Turing machine. To understand this, we must understand verification vs decision.\n\n## Verification vs Decision\n\nWe covered what it means to solve what a decision problem is, ([Defining Computational Problems](/blog/defining-computational-problems), [Church-Turing Hypothesis](/blog/church-turing-hypothesis)) verification is on the other hand is something you can send along with a solution. In most intuitive terms, let's say someone claims that they are very good at the game of Tetris and can win the game for some specified input. Here we consider a modified version of Tetris where all the next pieces are known in advance. How does this person **prove** to you that they can indeed win the game? By playing it out of course! It might be very difficult to figure out the strategy to win, but given the proof (the sequence of moves), implementing the rules of Tetris and playing it out to check if the person is correct can be done easily.\n\nEssentially, to be in $NP$, our machine can take an arbitrary amount of time to come up with proof for its solution for all possible inputs, but this proof must be _verifiable_ in polynomial time.\n\nWe'll attempt to explain further via means of an example. Consider the clique problem.\n\n$$ \\text{CLIQUE} = \\{\\langle G, k\\rangle : G \\text{ is an undirected graph with a k-clique} \\} $$\n\nHow would a _verifier_ verify this answer? Let's say the input to the verifier is given in the form $\\langle \\langle G, k\\rangle, c\\rangle$ where $c$ is the answer to our problem defined by $G$ and $k$.\n\n1. First, check if the answer $c$ contains exactly $k$ unique nodes $\\in G$ or not. If no, the answer can be trivially rejected. This can be done in $O(V)$ time.\n2. Next, check if there exists an edge between **every** pair of nodes in $c$. This is done in $O(V+E)$ time. If no, reject the answer.\n3. If both the above checks passed, accept the answer!\n\nHence we can say that the clique problem is in $NP$ because we've demonstrated that it is indeed possible to write a verifier that can check the \"correctness\" of an answer. In the field of complexity theory, we call such 'solution paths' or 'proofs' or 'witnesses' a **certificate** of computation.\n\n## NP-Complete\n\nFor a problem $p$ to be $\\in NP-Complete$ it must fit 2 criteria.\n\n1. $p$ must be $\\in NP$\n2. _Every_ problem $\\in NP$ must be _reducible_ to $p$\n\nWe cover reductions in depth later, but essentially, if we can come up with a polynomial-time algorithm(s) to 'reduce' the inputs and outputs $\\langle I, O\\rangle$ given to some machine $s$ to new inputs/outputs $\\langle I', O' \\rangle$ such that when applied to another machine $t$, $O' = O$. If this can be done, we say that we have reduced the problem solved by $s$ to $t$.\n\nNow, onto [P vs NP](/blog/p-vs-np)\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [P vs. NP - The Biggest Unsolved Problem in Computer Science - Up And Atom](https://youtu.be/EHp4FPyajKQ?si=YrgWuQpxfDbc0dmW) (Great Channel, recommend checking out)",
    "lastmodified": "2024-05-30T09:58:44.268692024+05:30",
    "tags": []
  },
  "/blog/knapsack-using-branch-and-bounding": {
    "title": "Knapsack Using Branch and Bounding",
    "content": "# Branch \u0026 Bound\nWhen trying to solve problems in general, (especially optimization problems) it's always a good idea to formulate a mathematical definition of the problem. To formulate it in mathematical terms, we assign each item in the knapsack a _decision variable $x_i \\in \\{0, 1\\}$._ Now, each item in the knapsack also has some weight $w_i$ and value $v_i$ associated with it. Let's say our knapsack capacity is denoted by $W$.\n\nThe decision variable $x_i$ simply indicates whether we include item $i$ in the knapsack or not. Using this, we can define the objective function that we wish to optimize (maximize) as:\n\n$$ V = \\sum_{i=1}^n v_ix_i $$\n\nUnder the constraints that\n\n$$ K = \\sum_{i=1}^n w_ix_i \\leq W $$\n\nNow we have a formal definition of the function we want to maximize under some constraints.\n\n## Branching\n\nNow, one way to solve the knapsack problem would be to perform an \"exhaustive\" search on the decision variables. This would mean checking every possible combination of values that our decision variables could take. Visualized as a tree, it would look like this:\n\n![bnb-1](/images/bnb-1.png)\n\n\nHere, we compute the answer by **branching** over all possible combinations. This is, of course, $O(2^n)$. Increasing input size by one would literally mean doubling the amount of computation done.\n\n## Relaxation\n\nThis is where the idea of relaxation comes in. Branching was basically us splitting the problem down into subproblems by locking or _constraining_ each variable and then deciding on the constraints for other variables. Exhaustive search isn't ideal, hence we try to reduce the search space by implementing some **\"bound.\"** Essentially, at each step of the recursion, we place an optimistic bet or estimate on just how good the solution to our subproblem can be. If at any point in the recursion, this estimate is lower than the best-found value so far, we can kill off the recursion.\n\nAs we've mentioned before, **relaxation** is the key to optimization. The original problem is **NP-Complete**. This means that until someone can prove $P=NP$, these problems have **NO** polynomial-time optimal solution. Relaxation is the art of how we deal with these problems.\n\n### Using relaxation on the exhaustive search\n\nSo, what constraints can we try to \"relax\" in the knapsack problem? The only constraint there is the weight of the Knapsack. So let's start by relaxing it to let us have an **infinite** knapsack. A picture is worth a thousand words, so let me just show you what the search would like with this relaxation.\n\n![bnb-2](/images/bnb-2.png)\n\n\nLet's try to see what we did here. First, we begin by letting root have $W = 10$ space and have $V = 0$ as it is completely unfilled. This is the $(0, 0, 0)$ state. The **estimate** here is our relaxation. Assuming infinite space, we see that the **most optimistic** value we can reach from here is $\\$128$ if we include all the items. Remember, the _relaxation_ is for calculating this _estimate_.\n\nOnce this is done, we're just performing an exhaustive search. But now, notice that the node on the left stops its recursion once the room in the knapsack has become negative. This is a simple base case on the original recursion.\n\nThe interesting part is the recursion that we've killed on the rightmost node. The leaves marked with crosses went till the end until they were discarded in favor of the left bottom leaf which is our optimal score of 80. Now, the rightmost node was killed even **before** it reached the leaves. This is because we had already achieved a better score (80) than the best possible estimate from this node. Hence we know for a fact that following the recursion can never give us a _better_ score. Recall that our relaxation was an infinite knapsack. If we cannot do better with an infinite knapsack from that point, there is no point in searching further down that track. This is the key idea behind relaxation and how using bounds can help us optimize exhaustive search. However, this particular relaxation was not very effective and did not help much in optimizing our search. But maybe with a _better_ heuristic, we can optimize the search further.\n\n## Coming up with a better heuristic\n\nLet's think about how we would normally solve the Knapsack problem **if we were allowed to take rational amounts of an item**. That is, it is no longer a 0-1 problem where we must either take or discard an item. We can take items in parts now. This problem has a fairly straightforward and greedy solution. We simply sort items by $\\frac{v_i}{w_i}$. This is essentially their \"value per 1 unit room.\" Simply pick the element that gives the best value per weight. So the strategy is now picking the element with the highest $\\frac{v_i}{w_i}$ ratio, and when we run out of space pick the last element in a fractional amount such that it fills up the entire knapsack.\n\nThis is the **optimal** solution for this version of the knapsack problem. But what about when we apply this relaxation to the original exhaustive search model instead of the infinite bag relaxation?\n\n![bnb-3](/images/bnb-3.png)\n\n\nNotice how much better we've managed to optimize the exhaustive search. The right child of the parent node is cut off at $\\$77$ and does not search further, because our \"estimated\" cost is lesser than the highest value we have found so far ($\\$80)$.\n\n### Optimality\n\nNotice that the fractional knapsack is the best-case version of the knapsack as we can optimally fill every unit of space according to a greedy strategy. Therefore if even the greedy estimation is below previously found maxima, then this quantity cannot be optimal. This means this branch and bound relaxation will still give us the **optimal** solution to the 0-1 knapsack problem.\n\n### Complexity\n\nAnalyzing the running complexity of branch and bound algorithms has proved notoriously difficult. The following blog gives some intuition as to why we find placing a bound on such techniques very difficult. [https://rjlipton.wpcomstaging.com/2012/12/19/branch-and-bound-why-does-it-work/](https://rjlipton.wpcomstaging.com/2012/12/19/branch-and-bound-why-does-it-work/)\n\nGeorge Nemhauser is one of the world's experts on all things having to do with large-scale optimization problems. He has received countless honors for his brilliant work, including membership in the National Academy of Engineering, the John Von Neumann Theory Prize, the Khachiyan Prize, and the Lanchester Prize. To quote him,\n\n\u003e _“I have always wanted to prove a lower bound about the behavior of branch and bound, but I never could.” -_ George Nemhauser\n\nPutting a good bound on branching and bounding is very difficult and is an open problem. One alternative measure that is used to better estimate the efficiency of branch and bound algorithms is its **effective branching factor (EBF).**\n\nWe define EBF as the number $b$ so that your search took the same time as searching a $b$-ary tree with no pruning. If you are searching a tree of depth {d}, this is well-defined as the $d$-th root of the total number of nodes you searched.\n\nThis is computed in practice and is used a lot in solving optimization problems as it is quite effective in practice, even if it is difficult to put bounds on theoretically. The fact that the runtime can be altered significantly by simply changing the relaxation criteria also makes it a great option to try out when coming up with relaxation ideas.\n\n## Why not just stick with Dynamic programming?\n\nThis is a natural question. ([A Deep Dive into the Knapsack Problem](/blog/a-deep-dive-into-the-knapsack-problem)) DP seems to give us an approach where we can be comfortable in sticking with an $N_W$ or $N_V$ complexity solution. However, what if we modify the question ever so slightly and allow items to have fractional or real weights? This seems like a problem that might surface in the real world a fair amount. The DP table approach is no longer feasible. In such a situation the branch and bound algorithm might come in clutch. As we explore such problems and minor variations of such problems, the need to expand our tool-belt and come up with more and more optimization algorithms becomes clear.\n\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. [Discrete Optimization - University of Melbourne - Coursera](https://www.coursera.org/learn/discrete-optimization)",
    "lastmodified": "2024-05-30T09:58:44.272025368+05:30",
    "tags": []
  },
  "/blog/levenshtein-edit-distance": {
    "title": "Levenshtein Edit Distance",
    "content": "Previously, we looked at a few famous dynamic programming problems ([DP as DAGs,  Shortest path on DAGs \u0026 LIS in O(nlogn)](/blog/dp-as-dags-shortest-path-on-dags-lis-in-o-nlogn)). Today we'll be looking at a pretty common problem that we have our computers solve for us, almost every single day. **Spellchecking**. Our computers are great at suggesting good auto-correct solutions for us whenever we misspell something. But to recommend one choice over the others, there must be some measure of ranking them. The problem is as follows:\n# The problem\n\n_Given two strings X \u0026 Y, what is the minimum number of edit operations that we must perform on X to transform it to Y? Here, an edit operation can be one of three things._\n\n1. _Insert character $c$ at any position $i$ in $X$_\n2. _Delete character $c$ at any position $i$ in $X$_\n3. _Substitute character $c$ at position $i$ in $X$ with any other character $c'$_\n\nThis computed quantity is also known as the **Levenshtein edit distance** between the two strings.\n\nIn essence, the Levenshtein distance is a very good heuristic to measure just how close two strings really are. It's a very good metric to rank words that the user might've wanted to type but accidentally misspelled due to one of the three possible edits. It is understandable why this is often used for spellchecking.\n\n## An alternate view\n\nAnother way to think about edit distance is as an alignment problem. Given two strings $X$ and $Y$, to what extent can they be matched up? An example should make this question more clear.\n\n$$ X = SNOWY, Y = SUNNY \\\\ S \\ \\_ \\ N \\ O \\ W \\ Y \\\\ S \\ U \\ N \\ N \\ \\_ \\ Y $$\n\nNotice that with this alignment,\n\n1. The `_` 's in $X$ represents an **insertion** edit\n2. The `_` 's in $Y$ represents a **delete** edit\n3. And a character mismatch represents a **replacement** edit.\n\nAt position 2, we have an insertion edit $(\\_, U)$. At position 4 we have a replacement edit $(O, N)$. At position 5 we have a delete edit $(W, \\_)$. This is in fact the optimal answer, and hence, the Levenshtein distance between the two strings.\n\nIn short, if we look at it as an alignment problem, the cost is the number of mismatched columns. The edit distance would then be the best possible alignment which minimizes mismatches.\n\n## Finding a recursive solution\n\nAt first glance, finding the solution to this question seems very difficult. There are a lot of different ways to convert say \"Dinosaur\" to \"Paragraph.\" It is not very clear how to solve this question without brute-forcing a lot of pairs. However, a key insight we can make here is that once we have optimally matched some prefix or suffix, we can discard away the matching prefix or suffix and recursively solve for the rest of the string.\n\nAn example will help illustrate this point. Consider the strings \"Dog\" \u0026 \"Dinosaur\". What the above point means is that the Levenshtein distance between **Dog** \u0026 **Dinosaur** will be the **same** as the Levenshtein distance between **\"og\"** \u0026 **\"inosaur\".** This key observation lets us write a nice recursive algorithm to calculate the Levenshtein distance for two strings.\n\n### The algorithm\n\n$$ Lev(X, Y) = \\begin{cases} |X| \u0026 \\text{if } |Y| = 0 \\\\ |Y| \u0026 \\text{if } |X| = 0 \\\\ Lev(tail(X), tail(Y)) \u0026 \\text{if } X[0] = Y[0] \\\\ 1 + min \\begin{cases} Lev(tail(X), Y) \\\\ Lev(X, tail(Y)) \\\\ Lev(tail(X), tail(Y)) \\end{cases} \u0026 \\text{otherwise} \\end{cases} \\\\ \\text{Here, } tail(X) \\text{ means the string X without the first symbol} $$\n\nThe top three cases are the base cases. If $Y$ is empty, we have to delete every character in $X$. If $X$ is empty, we have to insert every character in $Y$ in $X$. There is no other way to optimally transform $X$ to $Y$.\n\nThe third case is the key point discussed above. If the first characters match, we can simply discard it and compute the answer for the rest of the string.\n\nIf none of the above cases are true, we can do any of the three edit operations. Notice that there is sadly no way of _greedily_ picking what the best option would be here. Every operation influences the alignment of the rest of the substring and it is not possible to determine how a local choice affects the global structure we end up with. Hence the only possibility here is to recursively try out every possible combination and pick whichever gives us the minimum. Notice that each of the cases corresponds with an edit operation.\n\n1. $Lev(tail(X), Y) \\implies$**Insertion** operation. We are inserting a `_` in $X$ and computing the answer on the rest of the string.\n2. $Lev(X, tail(Y)) \\implies$**Delete** operation. We are inserting a `_` in $Y$ and computing the answer for the rest of the string.\n3. $Lev(tail(X), tail(Y)) \\implies$**Replacement** operation. We are substituting the character. This corresponds to letting the mismatch exist and align the rest of the string.\n\n#### Optimum substructure exists!\n\nThis algorithm has exponential complexity because in the worst case, it is trying out three different operations at every step. But the good thing about defining this problem recursively is that we have found an optimum substructure for this problem. If we brute force all possibilities at some position $i, j$ in both the strings, we can discard this character and recursively solve on the suffix. This hints us towards using DP to solve our problem more efficiently.\n\n## Coming up with a DP solution\n\nIn general, when we try to find a DP solution to some problem, the following is a good mental checklist to follow/answer.\n\n1. **Define the subproblem \u0026 count how many we'll have**\n2. **Guess some part of the solution \u0026 count how many possibilities we'll have to brute force over. This is the transition we want from the problem to its subproblem.**\n3. **Write the recurrence in terms of the guessed solution + the subproblem**\n4. **Figure out how to memoize/use a dp table for storing computed calculations. Notice that the recursive structure must follow a DAG structure as stated previously or we'll have an infinite recursion which implies our algorithm is wrong.**\n5. **We solve the problem**\n\nLet's go over them one by one.\n\nLooking at the recursive definition we have for edit distance, it becomes clear that we must be able to compute the edit distance between any two prefixes of string $X$ and $Y$. These are all the different subproblems encapsulated by the recursion.\n\nNote, from here on forth we denote prefixes of $X$ by $[\\ :i]$ and prefixes of $Y$ by $[ \\ :j]$. Here, we get the answer to the first point in our checklist.\n\n1. Computing edit distance for all possible pairings of prefixes between $X$ and $Y$. We will have of the order quadratic subproblems. For every value of $i$ we have $j$ possibilities to pair it with. Hence the number of problems is of the order $O(|X|.|Y|)$\n\nFor computing the answer at every point, we either have the base case or we have **three** possible operations to take.\n\n1. We can perform one of three operations. Substitute, insert, or delete. In essence, given two suffixes we have exactly three operations that we can use to transform the first character of $X$ to the first character of $Y$. Replace $X[i] \\to Y[j]$. Insert $Y[j]$. Delete $X[i]$.\n\n![edit-dist-1](/images/edit-dist-1.png)\n\n\nSince we already have a recursive expression of the algorithm, we already know the recurrence.\n\n3. The recurrence is the same as stated previously\n\n$$ \n1 + min \\begin{cases} Lev(tail(X), Y) \\\\ Lev(X, tail(Y)) \\\\ Lev(tail(X), tail(Y)) \\end{cases}\n$$\n\n1. We already said we will have $O(|X|.|Y|)$ subproblems where we match every $i$ with every $j$. This should have hinted at a 2D dp table. In this table, **every** cell corresponds to the edit distance computed between two suffixes of strings $X$ and $Y$.\n\nFor example, the highlighted yellow cell represents the edit distance between `LITY` and `ARITY`. Further, notice that each of the three highlighted boxes around it corresponds to an **edit operation.** This observation is key to figuring out the topological ordering of our problems.\n\n![edit-dist-2](/images/edit-dist-2.png)\n\n\n1. The `Substituion` box means we swap \"L\" with \"A\" and move to state $(i+1, j+1)$.\n2. The `Insertion` box means we insert \"A\" and move to state $(i, j+1)$\n3. The `Deletion` box means we delete \"L\" and move to state $(i+1, j)$\n\nHence for computing the answer at any cell, we only need the answers at cells $(i+1, j), (i, j+1) \\text{ and } (i+1, j+1)$. This is enough information to get the topological ordering. A simple nested for loop from $i :n \\to 0$ and $j:m\\to0$ should be sufficient.\n\nNotice that due to the nature of the problem I can go from $0\\to n$ and $0 \\to m$ as well and define the dp for the prefixes. However, the suffixes idea in my opinion makes the most sense and we'll be using the suffix definition for the dp.\n\nFurther, notice that in the real dp table we would have an extra row and column padding at the very ends to account for the base case where $|X| = 0$ or $|Y| = 0$.\n\n\u003eThus far, we have implicitly assumed that the **cost** associated with each operation is 0. However, this need not be true. Each operation can have any defined cost. In fact, we can even define the cost for conversion from one specific symbol to another and our algorithm would still work. The above DP table can simply be thought of as a DAG with $O(n^2)$ nodes and each edge $(u, v)$ can be **weighted** with the cost of the corresponding transformation from the symbol at position $u$ to the symbol at position $v$. Our final answer is in fact just the shortest path from position $(|Y|, |X|) \\to (0, 0)$\n\n### Visualization as a DAG\n\n**Note**: This is the image from the lecture slides and shows the path for the approach using prefixes. For the suffix-based state transformation used by me, simply reverse the direction of each edge in the graph and the problem remains the same.\n\n![edit-dist-3](/images/edit-dist-3.png)\n\n\n1. Now to solve the problem :) Notice that the runtime of the algorithm is $O(|X|.|Y|)$\n\n### Single row optimization\n\nThe time complexity of our algorithm was $O(|X|.|Y|)$ and the space complexity was also $O(|X|.|Y|)$. This is considerably better than exponential, but _can we do better?_\n\nAre there any redundancies that we may be computing/storing? It turns out that in fact, there is.\n\nNotice that to compute the value of $dp[i][j]$ at any location, we **only** care about the values of $dp[i][j+1]$, $dp[i+1][j]$ and $dp[i+1][j+1]$. However, notice that we are storing the **ENTIRE** dp table from $dp[0][0] \\to dp[n][m]$. This is redundant and can have great practical limitations on our algorithm.\n\nFor example, computing the edit distance between two strings of length $10^4$ would require 100 MB of memory. This in turn would give a lot of cache misses and slow down the algorithm as well. Further, if we wanted to compute the distance between a string of length $10^5$ and $10^4$, it would only take a few seconds to a minute on most machines but it would **require 1 GB memory.**\n\nThat's a lot of memory wasted for storing redundant information. The single row optimization for DP is as follows.\n\nWe only ever store two rows in our DP table. When computing $dp[i][j]$, we only store the dp table at row $dp[i]$ which we are computing, and the row $dp[i+1]$, which contains the already computed values (as enforced by the topological ordering).\n\nNotice that with this simple optimization,\n\n1. To compute any $dp[i][j]$, notice that all the required states are always in memory. We are never losing/erasing dp values that we require for the computation of $dp[i][j]$ before computing $dp[i][j]$.\n2. We have reduced the space complexity of our algorithm from $O(|X|.|Y|)$ which is quadratic, to $O(2*|X|) = O(|X|)$. Our space complexity is now **linear!**\n\n## Applications\n\nWhile we only discussed how Levenshtein distance was a great heuristic for spell checkers, it is also extensively used in the field of biology for comparing DNA sequences. The more general version where each transformation is given some cost $c_{transform \\ type,\\ s1 \\to s2}$ is used here.\n\nFor example, the mutation $C \\to G$ is more common than $C \\to A$.\n\nNotice that we can now give $C \\to G$ a low cost and $C \\to A$ a high cost. This represents that the first mutation is more likely than the other. This gives us a measure of how similar two DNA sequences are. Mutations also have insertions/deletions. This makes Levenshtein distance a great tool to use here.\n\nIf we wish to not use insertions or deletions, notice that we can simply give them $\\infty$ cost. In computational terms, they're given a very high value like\n\n## Code\n\nWhile it is much easier to visualize the bottom-up dp as finding the solution to suffixes, it is much easier to code the prefix definition of the dp. Note that there really isn't any difference in which direction we pick, at least not conceptually. It is just easier to implement the prefix solution in code.\n\nThe single row optimized dp code for calculating the Levenshtein distance between two strings can be found here: [Levenshtein Edit Distance](https://github.com/akcube/algorithms-notebook/blob/main/code/strings/levenshtein-edit-distance.cpp)\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [How do Spell Checkers work? Levenshtein Edit Distance - Creel](https://youtu.be/Cu7Tl7FGigQ?si=pFru3JaBAeKStvtz) (Excellent channel, do check him out. Has a lot of unique amazing content!)\n",
    "lastmodified": "2024-05-30T09:58:44.275358711+05:30",
    "tags": []
  },
  "/blog/master-s-theorem-strassen-s-matrix-multiplication-kth-order-statistics": {
    "title": "Master's Theorem, Strassen's Matrix Multiplication \u0026 KTH-Order Statistics",
    "content": "# Master's Theorem\n\nSolving recurrence relations can prove to be a difficult task, especially when there are many terms and factors involved. The master's theorem is a very useful tool to know about, especially when trying to prove operation bounds for _divide and conquer_ type algorithms. Most such algorithms have some constant factor by which they divide the initial input a certain number of times and recursively perform some operation.\n\nIn general, the master's theorem states that:\n\n$$ If \\ \\ T(n) = aT(\\frac{n}{b})+O(n^d) \\ for \\ some \\ constants \\ a\\gt 0, b \\gt 1, and \\ d \\geq 0 $$\n\n$$ T(n) = \\begin{cases} O(n^d) \\ if \\ d\\gt log_ba \\\\ O(n^dlogn) \\ if \\ d = log_ba \\\\ O(n^{log_ba} \\ if \\ d \u003c log_ba \\end{cases} $$\n\n## A visual depiction of the proof\n\n![masters-theorem-tree](/images/masters-theorem-tree.png)\n\n\n## Proof of Master's Theorem\n\nWe can see that the size is divided by $b$ on every level. Therefore, for the size $n$ to go to 1, it will require $log_{b} n$ times. Therefore, the depth of the tree is $log_{b} n$. Also, the number of nodes at level $k$ is $a^k$, therefore the number of leaf nodes is $a^{log_{b} n} = n ^{log_{b}a}$\n\nAt the root level, you have 1 node. This node needs to do the following recursive operations.\n\n$T(n) = O(n^d) \\ + \\ aT(n/b)$ The number of operations the algorithm performs is essentially defined for some input n by this quantity $T(n)$. We notice that for the Master's theorem, my function is recursively defined. The $O(n^d)$ term is the number of operations I'm doing at the node my recursive algorithm is on. So if I visualize this as a tree, it will have $log_ba$ depth because I'll have those many divisions of my original input **n**.\n\nIf I had 1 node at the root, at the 2nd level it will split into $a$ nodes on the next level. At the next \"level\" of my recursion tree, each of these $a$ nodes will split into another set of $a$ nodes. So the depth at the $k^{th}$ level is $a^k$. So at the $k^{th}$ level, I'll have work equal to the work accumulated by each of my $a^k$ nodes. $a^k$ nodes do $O(n^{'k})$ work. But this is the size of $n^{'}$ at the $k^{th}$ level. The input size $n^{'}$ at the $k^{th}$ level is $\\frac{n}{b^k}$ in terms of the original input $n$. (n has been divided by b at each level) So the accumulation of work done at the $k^{th}$ level is essentially\n\n$$ a^k \\times O(\\frac{n}{b^k})^d = O(n^d)\\times(\\frac{a}{b^d})^k $$\n\nNow if we take the sum of this quantity over all $log_bn$ levels, we notice that this is just a geometric series with first term $a = O(n^d)$ and ratio $r = \\frac{a}{b^d}$\n\nCalculating the geometric series will give us the following three results for three different cases of our ratio $\\frac{a}{b^d}$.\n\n### Cases:\n\n1. $\\frac{a}{b^d} \\lt 1 \\implies a\\lt b^d \\implies log_ba \\lt d$\n    \n    The series is decreasing and the dominant term is our first term. This gives us the result, $T(n) = O(n^d)$\n    \n2. $\\frac{a}{b^d}=1 \\implies a = b^d \\implies log_ba=d$\n    \n    In this case, there are exactly $O(log_bn)$ terms in the series (depth of the tree) and each term is equal to $O(n^d)$. This gives us a simple summation,\n    \n    $T(n) = O(n^dlog_bn)$\n    \n3. $\\frac{a}{b^d} \\gt 1 \\implies a \\gt b^d \\implies log_ba \\gt d$\n    \n    The series is increasing and the dominant term will be the last term of the series.\n    \n    $$ n^d(\\frac{a}{b^d})^{log_bn} = n^d(\\frac{a^{log_bn}}{(b^{log_bn})^d}) = n^d(\\frac{a^{log_bn}}{n^d}) \\\\ a^{log_bn} = a^{log_ba.log_an} = (a^{log_an})^{log_ba} = n^{log_ba} $$\n    \n    This gives us the result,\n    \n    $T(n) = O(n^{log_ba})$\n# Matrix Multiplication\n\nNaïve Algorithm: $O(n^3)$\n\n**Strassen's: $O(n^{log_{2}7})$**\n\nWe imagine the two matrices we have to multiply as consisting of 4 $\\frac{n}{2}$ matrices in each matrix.\n\n$$ X = \\begin{bmatrix} A \u0026 B \\\\ C \u0026 D \\end{bmatrix}, Y = \\begin{bmatrix} E \u0026 F\\\\ G \u0026 H \\end{bmatrix} \\\\ XY = \\begin{bmatrix} A \u0026 B \\\\ C \u0026 D \\end{bmatrix} \\begin{bmatrix} E \u0026 F\\\\ G \u0026 H \\end{bmatrix} = \\begin{bmatrix} AE+BG \u0026 AF+BH \\\\ CE+DG \u0026 CF+DH \\end{bmatrix} $$\n\nNotice that this multiplication ends up with us having to calculate the product of **8** such submatrices and the addition of 4. It is evident that multiplication is the bottleneck here. For such an algorithm, we have $T(n) = 8T(n/2) + O(n^2)$ which makes the time complexity $O(n^3)$ as per Master's theorem. (As $log_ba \\gt d$)\n\nHowever, using a method similar to the same technique used by the **Karatsuba** multiplication algorithm ([Analyzing Fibonacci \u0026 Karatsuba Multiplication](/blog/analyzing-fibonacci-karatsuba-multiplication)), we can bring down the number of products to just 7.\n\n**Note:** This observation is _not_ trivial and does _not_ have a simple construction. But for the sake of documentation, it is shown below.\n\n## Strassen's Matrix Multiplication\n\nThe algorithm is as follows. Given\n\n$$ X = \\begin{bmatrix} A \u0026 B \\\\ C \u0026 D \\end{bmatrix}, Y = \\begin{bmatrix} E \u0026 F\\\\ G \u0026 H \\end{bmatrix} $$\n\nCompute the following terms,\n\n$$ P_1 = A(F-H) \\quad P_5 = (A+D)(E+H) \\\\ P_2 = (A+B)H \\quad P_6 = (B-D)(G+H)\\\\ P_3 = (C+D)E \\quad P_7 = (A-C)(E+F) \\\\ P_4 = D(G-E) \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\ \\ $$\n\nNotice that to compute each of these 7 terms, we only need 7 multiplication operations in total. Now, once computed, we can write the expression $XY$ as follows:\n\n$$ XY = \\begin{bmatrix} P_5+P_4-P_2+P_6 \u0026 P_1+P_2 \\\\ P_3+P_4 \u0026 P_1+P_5-P_3-P_7 \\end{bmatrix} $$\n\nAgain as mentioned above, this construction is **not** intuitive or easy to come up with. With some working out on pen and paper it can be seen that the above construction does indeed yield us the correct result. While it is more complicated, notice that we now only have to perform **7** multiplication operations. The work done at each node is the $n^2$ additions. This lets us write, $T(n) = 7T(n/2)+O(n^2)$. Applying Master's theorem to this result, we find $log_ba \\gt d$ which implies that the time complexity of Strassen's Matrix multiplication is $O(n^{log_ba}) = O(n^{2.81})$\n\n# Finding Median in $O(n)$\n\nThe problem of Median finding is as follows. It simply asks, given a list of numbers $S$, find the $\\lfloor\\frac{n}{2}\\rfloor^{th}$ smallest element in the list. In fact, we can generalize this problem to asking, _\"Given a list of numbers $S$, find the $k^{th}$ smallest element in the list.\"_\n\nThe naïve solution to this problem is simply sorting the list and then just picking the $k^{th}$ smallest element. The correctness of this approach is fairly easy to prove. In a sorted list the condition $a_i\u003ca_{i+1}$ holds $\\forall a_{i\u003cn}\\in S$. Hence picking the $k^{th}$ element of the sorted list is equivalent to picking the $k^{th}$ smallest element in S. However, the time complexity of this algorithm is $O(nlogn)$. The question is, _can we do better?_\n\nLet's try to apply the same concepts of divide and conquer that netted us promising results in our previous endeavors. Notice that for this particular problem, since our list is **unordered**, there is nothing to gain by simply splitting the list into $\\frac{n}{2}$ halves and solving recursively. The notion of \"$k$\" does not exist in these halves as we do not have any information about them. Instead, let us consider the division that happens when we pick some arbitrary element $a\\in S$.\n\nMore formally, if we have some **unordered** list S, and we pick some element $a \\in S$, we can divide the set $S$ into **three** parts.\n\n$$ S \\begin{cases} S_L = \\{ x\\in S \\mid x \\lt a \\} \\\\ S_a = \\{ x\\in S \\mid x = a \\} \\\\ S_R = \\{ x\\in S \\mid x \\gt a \\} \\end{cases} $$\n\nOnce such a decision is made, we can recursively call our $k^{th}$ order statistic finding algorithm as follows. Let's call our algorithm `selection(S, k)` where $S$ is the input list and $k$ is the $k^{th}$ order statistic we wish to find. Then,\n\n$selection(S, k) = \\begin{cases} selection(S_L, k) \u0026 \\text{if} \\ k \\leq |S_L| \\\\ a \u0026 \\text{if} \\ |S_L| \\lt k \\leq |S_L|+|S_a| \\\\ selection(S_R, k-|S_L|-|S_a|) \u0026 \\text{if} \\ k \\gt |S_L|+|S_a| \\end{cases}$\n\nLet's try to parse this recursion.\n\n1. If $k$ is smaller than or equal to $|S_L|$, it means that our _\"pivot\"_ $a$, was pictorially too far to the right in the sorted list $S$. This essentially means that we _overshot_ our guess for the $k^{th}$ order statistic. Hence we discard every element to the right (pictorially) and recurse on the left part of the list.\n2. If $|S_L|$ is lesser than $k$ and $k$ is smaller than or equal to the ranges encompassed by $S_L$ and $S_a$, then it must be true that our $k^{th}$ order statistic **is** our pivot. Visually, $k$ lies in the range of elements equal to our pivot in the sorted list.\n3. If $k$ is greater than the range described in #2, then we _undershot_ our guess for the $k^{th}$ order statistic. This implies that we can discard the left portion of the list and recurse on the right portion. However, we will need to adjust the value of $k$ which is input to it as required.\n\nNotice that in the recursion, we have **no guarantee** about the size of $S_L$ or $S_R$. The approach of divide and conquer has allowed us to shrink the number of elements we're worrying about at some step $i$ from $S$ to $max\\{|S_L|, |S_R|\\}$. However, we have no guarantee about their size.\n\nIf we analyze the worst case of this algorithm, notice that the worst case occurs when we pick our pivot in sorted order. In this case, the complexity looks something like $T(n) = T(n-1) + O(n)$. This gives us $T(n) = O(n)+O(n-1)+\\dots+O(2)+O(1)$, which essentially gives us a $O(n^2)$ algorithm which is **worse** than the sort and pick approach.\n\n_However_, if we analyze the \"best\" case for this algorithm, notice that if we pick the **median** as the pivot at each step, we can divide $|S_L|=|S_R|=\\frac{n}{2}$. This gives us $T(n) = T(\\frac{n}{2}) + O(n)$. Evaluating this, we get $T(n) = O(n)+O(\\frac{n}{2})+\\dots+O(1)$. This would give us a time complexity of $O(2n) = O(n)$. So in our best case, our algorithm **outperforms** the sort and pick approach.\n\n\u003eNotice the similarity between this algorithm and quicksort. Both have a great best-case time complexity and a very poor worst-case time complexity. They are also very similar in the fact that both their running times are severely affected by the choice of the pivot.\n\u003e\n\u003eThis should clue us into the fact that perhaps trying a randomized approach would give us a desirable result. And we will soon see that a randomized approach does indeed give us a linear **expected** time complexity. However, it is also possible to provide a deterministic approach that can yield us the linear time complexity we desire. This approach is called the _Median of medians_ and we shall discuss this below. Moreover, notice that since this algorithm is _**linear**,_ it can be used as a **subroutine** in the quicksort algorithm to deterministically pick the median as the pivot. This would give us a _theoretically_ very fast quick sort as it can be proved to execute in $O(nlogn)$ for any input. However, practically speaking, the constant factor incurred from running the linear median finding algorithm for every step in quicksort makes it slower in real-time execution. This makes it more preferable to use the randomized quicksort. However, it is pretty cool to note that we can prove quicksort to have an upper bound of just $O(nlogn)$.\n\nNow, coming back to the original problem. If we can pick the median as our pivot, we would get good running time. However, this problem seems counter-intuitive. Our algorithm essentially _needs_ the answer to perform its computation. This isn't possible. But perhaps it's possible to put some bound on the size of $S_L$ and $S_R$. Notice that, if at every step of the division, we can guarantee $max\\{|S_L|, |S_R|\\}$ to be some **ratio** of $|S|$, we can guarantee linear running time of our algorithm. Evaluating $T(n) = T(\\frac{n}{r})+O(n)$, we get\n\n$$ T(n) = O(n)+O(\\frac{n}{r})+O(\\frac{n}{r^2})+\\dots+O(1) = O(n(1 + \\frac{1}{r}+\\dots)) = O(n(\\frac{1}{1-r})) = O(cn) = O(n) $$\n\nThis observation has simplified our problem a little and paved the path for the success of the _\"Median-of-Medians\"_ approach.\n\n## Median-of-Medians\n\nThe idea behind the algorithm is as follows. Given some input list $S$ with $n$ elements, perform the following operations recursively.\n\n1. Divide the n elements into groups of 5\n2. Find the median of each of the $\\frac{n}{5}$ groups\n3. Find the median $x$ of the $\\frac{n}{5}$ medians\n\nNotice that the time complexity of this is pretty similar to the \"linear\" running time proof of finding $k^{th}$ order statistics when we are able to divide the input into some ratio at every step. We get $T(n) = 5T(\\frac{n}{5})+O(1)$ . Here we consider median finding among 5 elements, a constant time operation. We can solve this recurrence using the Master's theorem. Notice that $log_ba\u003ed$ which implies the time complexity is $O(n^{log_ba}) = O(n)$.\n\nThis means that we now have a **linear** time algorithm, which can obtain the _Median of Medians_ for some input $n$. Now the question is, how does this help us split $S_L$ and $S_R$ in such a way that we can bound them to some ratio of the original input $n$?\n\nThey say a picture is worth a thousand words, and I think you will find the below image quite insightful for the explanation of this proof.\n\n1. Let us divide our set $S$ into $\\frac{n}{5}$ lists of 5 elements each and call them $\\sigma_1, \\sigma_2, \\dots, \\sigma_{\\frac{n}{5}}$.\n2. For visualization's sake, let us picture each of these lists in sorted order vertically. For example, in $\\sigma_1$, $a_1\\leq a_2 \\leq a_3 \\leq a_4 \\leq a_5$ holds. Notice that this implies that in every list, the third element must be the median.\n3. Now, let us sort the lists themselves horizontally by their median value. That is, in the picture below, $a_3\\leq b_3 \\leq c_3 \\leq d_3 \\leq e_3$ is true. Notice that this implies that in the below picture, the 3rd element in list $\\sigma_{\\frac{n}{10}}$is the median-of-medians.\n\nNow that our elements are now ordered both vertically and horizontally, let us try to place bounds on the division that picking the median of medians grants us.\n\n![median-of-medians-1](/images/median-of-medians-1.png)\n\n\nNotice that in the above picture, because $c_3$ is the median of medians, it **must** be greater than $a_3$ and $b_3$. More formally, $x_3\\in \\sigma_{\\frac{n}{10}} \\geq x_3\\in \\sigma_{i\u003c\\frac{n}{10}}$. Further, because $x_3\\in\\sigma_i$ is greater than or equal to all $x_1, x_2 \\in \\sigma_i$, our median-of-medians is greater than or equal to every $x_{i\\leq3}\\in\\sigma_{i\\leq\\frac{n}{10}}$. Or to put it more simply, it must be greater than equal to everything on this picture that is painted in pink.\n\n![median-of-medians-2](/images/median-of-medians-2.png)\n\n\nA similar statement can be made about everything it is lesser than or equal to. Everything painted blue in the above diagram must be greater than or equal to our median-of-medians.\n\nIf that makes sense, let's try to formalize and state our argument more _quantitatively_ now. Once we have chosen our pivot as the median-of-medians, the set of all elements lesser than equal to the pivot is essentially just $S_L$. So... how do we enumerate $|S_L|$ or $|S_R|$?\n\nNotice that there are $\\lceil \\frac{n}{5} \\rceil$ lists in total. Out of these, we can enumerate one-half of the lists (including the list containing the median-of-medians) as $M = \\lceil \\frac{1}{2} \\lceil \\frac{n}{5} \\rceil \\rceil$. Each of these $M$ lists contains 3 elements that are lesser than equal to the pivot. This gives us $|S_L| \\geq 3M.$ We can obviously remove the pivot itself while recursing and this would give us $|S_L| \\geq 3M-1$. Similarly, for $|S_R|$, we might have to remove the last set if $n$ was not perfectly divisible by 5. This would give us the same bound $\\pm c$. Since $c$ is pretty small we'll just choose to ignore it in our calculations.\n\nThis gives us\n\n$$ |S_L| \\geq \\frac{3n}{10}, \\quad |S_R| \\geq \\frac{3n}{10} \\\\ \\implies |S_L|\\leq n-|S_R|, \\quad |S_R|\\leq n-|S_L| \\\\ \\implies |S_L| \\leq \\frac{7n}{10}, \\quad |S_R| \\leq \\frac{7n}{10}, $$\n\n### Conclusion\nWe came up with an algorithm to find the median-of-medians in **linear time.** And we have managed to prove that picking the median-of-medians as the pivot, lets us divide the original set into $S_L$ and $S_R$ such that their size is always bound to be greater than some ratio of the input n. These 2 facts combined give us the linear time $k^{th}$ order statistics finding algorithm.\n\nTo state this more formally,\n\n- We can find the median of medians for some input $n$ in linear time.\n- Using the median-of-medians as pivot, we guarantee a division of $S$ into sets such that the next step of our $selection(S, k)$ algorithm will receive as input $S'$ which can be expressed as a ratio of the input $n$. $|S'| \\leq \\frac{7}{10}n$\n- This implies that the total runtime of our algorithm is **linear**\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.278692055+05:30",
    "tags": []
  },
  "/blog/more-greedy-algorithms-kruskal-s-disjoint-set-union": {
    "title": "More Greedy Algorithms! Kruskal's \u0026 Disjoint Set Union",
    "content": "# Greedy Algorithms\nPicking off from [Activity Selection \u0026 Huffman Encoding](/blog/activity-selection-huffman-encoding), the _Greedy_ idea is as follows. At every step, our algorithm picks the **locally optimum** choice in the hope that this choice will also be the **global** optimum. The greedy idea is often the easiest to come up with. Picking the local optimum, in some sense, is often a much easier problem to solve than picking the global minimum. Picking the global minimum often requires seeing _ahead_ to figure out if a global optimum can be reached by picking non-locally optimum choices.\n\nThis often requires recursively solving and there are techniques to speed up computation, but just looking at the local options and picking the best option is much easier in general. The implementation is simple as well. We only need to consider the local choices.\n\nThese properties of greedy algorithms make them quite desirable. An easy to implement an algorithm that is _also_ very fast? That's a great algorithm. Except, for one caveat. As with any algorithm, the first most important thing to prove about it is its **correctness**. This is sadly the case with most greedy algorithms, they fail this test. Picking the local optimum is often **not** the right way to proceed in many algorithms. They might give a _desirable_ result, something _close_ to the global optimum. But not the global optimum itself. And often, it will be possible to generate a counter-case where the greedy solution can be forced to produce a very poor result.\n\nIf a shoddy, quick solution that provides a \"good\" result in most cases is the desired result, then the greedy solution is a great choice! In fact, there are many \"hard\" problems today whose global optimums cannot be computed in feasible time, even with the best-known algorithms for them. In such situations, the best we can hope is to produce a _good_ greedy solution that generates a \"good\" result and hopes that it is _close_ to the global optimum.\n\n## Matroid Theory\n\nMatroid theory gives the sufficient condition for greedy strategies to be applicable to a problem. If we can express a problem in the terms described by Matroid theory, then we can be guaranteed that a greedy solution exists for this problem. From here on forth, when we say \"greedy solutions\", we will refer to greedy solutions that always give the global optimum. Also, note that the reverse is not true. Matroid theory is simply a sufficient condition. If a problem does not fit the matroid theory, it does not mean that it cannot have a greedy solution. Dijkstra is one such problem that does not fit the terms described by Matroid theory, yet has a greedy solution.\n\n# The Minimum Spanning Tree (MST) problem\n\nThe MST problem asks the following question, _\"Given some undirected graph G, find a fully connected tree such that it contains every vertex of the graph G and the set of all edges of the tree must be a subset of the edges of G and its total cost is minimized.\"_\n\nMore formally\n\nGiven some undirected graph $G = \\langle V, E\\rangle$ where each edge has some cost $w_e$ associated with it, find a tree $T = \\langle V, E'\\rangle$ where $E' \\subseteq E$ and the total cost of the tree $\\sum^{e \\in E} w_e$ is minimized.\n\nConsider the naïve approach which involves finding **every** possible spanning tree of the graph and finally outputting the one with the least cost. This is not feasible as the number of spanning trees we can generate for some graph $G$ grows exponentially.\n\nThis is where the idea of \"Greedy\" comes in. However, to facilitate proving the correctness of our solution later, let us cover an interesting property about graphs first.\n\n## The Cut Property\n\n### Cut\n\nIn graph theory, we define a cut as a partition that divides a **connected** graph into two disjoint subsets.\n\n![cut-1](/images/cut-1.png)\n\n\nNotice that in the above graph, the \"cut\" depicted by the pink line divides our graph into two connected **disjoint** subgraphs. A cut can remove multiple edges, but the end result is two disjoint connected subgraphs.\n\nHere we also define what is known as the **Cut Set**. It is simply the set of all edges in the cut. That is, it is the set of all edges which must be removed to achieve the result of the cut. In the above example, the cut set would be $E_c= \\{e_4\\}$\n\n### The Cut property - Statement\n\nLet's say $X$ is the set of all the edges belonging to the MST of some undirected graph $G$.\n\nNow, pick some subset of nodes $S$ such that none of the edges in $X$ provide a connection between 2 vertices in $S$ and $S^c$. In more intuitive terms, this subset must either all belong to the MST or all not. We can now imagine this as a _cut_ between all the nodes in the MST and all the nodes _not_ included in the MST yet.\n\nThe cut property states that the **minimum weight** edge in the cut set should be included in the minimum spanning tree of the graph $G$. That is, the minimum weight edge that crosses $S$ and $S^c$ must be a part of the MST of the graph.\n\n**Proof:**\n\n- We have some set of edges $X$ which belong to the MST $T$ of our undirected graph $G = \\langle V, E \\rangle$.\n- Let us begin by assuming that we have picked some edge $e$ which is _not_ the minimum weight edge in the cut set\n- If we do so, it will lead to constructing a _different_ MST $T'$ of our graph compared to the MST that we would generate if we included the minimum weight edge $e_{min}$.\n- Now, because $T'$ is an MST, it must be connected and acyclic. Also, by proof of its construction, $e_{min}$ does _not_ belong to $T'$\n- Now, since the graph is a tree, if we _include_ $e_{min}$ to $T'$, notice that there **must** exist some edge(s) in $e' \\in E'$ such that $e_{min}$ forms a cycle with $e'$. [$E'$ is the edge set of $T'$ ]. This must be true as every node is connected, and the graph is acyclic. This implies there is a unique path between any 2 pairs of vertices in the graph. If a new edge is added connecting two nodes, a new path is created between them which creates a cycle.\n- Now, by nature of how $T'$ was constructed, $w_{emin} \\lt w_{e'}$. If this was not true we would have picked $e'$ to be $e_{min}$\n- Next, _remove_ edge $e'$. Notice that removing an edge from a cycle does **not** make the graph acyclic. Further, we added and subtracted one edge each. This implies that the number of edges in the graph is still $|V|-1$. This implies that the graph **must** be acyclic as well. That is, our new graph is a **tree.**\n- The cost of our new tree is $W_{T'} - w_{e'} + w_{emin}$ .\n- $W_{T'} - w_{e'} + w_{emin} \\lt W_{T'}$ as $w_{emin} \\lt w_{e'}$. This implies that $T'$ is not the MST as a better tree can be constructed which includes $e_{min}$.\n\n## Kruskal's Algorithm\n\nKruskal's approach isolates all of the nodes in the original graph, forming a forest of single node trees, and then progressively merges these trees, merging any two of all the trees with some edge of the original graph at each iteration. All edges are sorted by weight before the algorithm is run (in non-decreasing order). The unification procedure then begins: choose all edges from first to last (in sorted order), and if the endpoints of the presently selected edge belong to separate subtrees, these subtrees are merged, and the edge is added to the answer. After iterating through all of the edges, we'll find that all of the vertices belong to the same sub-tree, and we'll have the solution.\n\nFurther, note that there may be multiple possible solutions. Kruskal will simply give us _one_ such solution.\n\n### Proof\n\nMost greedy algorithms often have their proof in induction, as it is a methodical and elegant way to approach the reasoning of picking the local optimum to get the global optimum. Notice that at every step of the algorithm, we pick the local optimum. That is, we pick the lowest weight edge that belongs to the cut set of the MST and the graph. Hence, by the cut property, the edge we pick **must** belong to the MST. Doing so repeatedly allows us to pick all $n-1$ edges for the graph.\n\n#### A _small problem_\n\nNotice that sorting takes $O(nlogn)$ time. But however, _checking_ if a chosen edge belongs to the cut set or not takes $O(n)$ for each edge. This is not ideal and pushes the algorithm to the time complexity of $O(n^2)$. However, it is possible to eliminate this cost by introducing a data structure that can perform a unification operation and parent lookup operation in an amortized constant time complexity. This will bring down the total time complexity of Kruskal's to $O(MlogN)$ where $|E| = M, |V| = N$.\n\n# Disjoint Set Union\n\nThe DSU is a data structure that allows for queries of two types.\n\n1. Merge 2 sets\n2. Query the root element of some set $S$\n\nThe idea is to maintain a structure that maintains the sets as nodes in a tree where the root is the primary identifier of any set and a merging operation is simply the unification of two trees.\n\n![dsu-1](/images/dsu-1.png)\n\n\nThe DSU is initially initialized as an array like so `dsu[i]=i`. `dsu[i]` essentially contains the parent element of set $i$. If $dsu[i]=i$, then $i$ is the root node. Following is the code for the DSU:\n\n**Querying for parent:**\n\n```cpp\nint parent(int i){\n    if(dsu[i]==i) return i;\n    else return parent(dsu[i]);\n}\n```\n\nLooking at just this, it is easy to come up with a case for which this algorithm will take $O(n)$ time. However, by introducing a small factor in the merging step, it is possible to guarantee $O(logn)$ complexity. Here is the code for the unification of two sets in the DSU.\n\n**Query to merge two sets:**\n\n```cpp\nvoid unify(int a, int b){\n    a = parent(a);\n    b = parent(b);\n    if(rank[a] \u003c rank[b])\n        swap(a, b);\n    dsu[b] = a;\n    if(a!=b \u0026\u0026 rank[a] == rank[b])\n        rank[a]++;\n}\n```\n\n## What is rank[x]?\n\nWe can think of `rank[x]` as simply a variable that helps us construct _balanced_ tree structures when we perform the merging operation. Notice that the following statements always hold true for `rank[x]`.\n\n1. For all $x$ in our DSU, $rank[x] \\lt rank[parent(x)]$\n2. Let's say some root node in our DSU has rank $k$. This implies that this root node has at least $2^k$ nodes in its subtree. Why? Notice that to make a tree of rank $k$, we need at least two trees of rank $k-1$. `if(a!=b \u0026\u0026 rank[a] == rank[b])` implies this. We can then extend this by induction to prove this.\n3. From statement 2, it is implied that if there are $n$ elements in the DSU, at most $\\frac{n}{2^k}$ nodes can have rank $k$\n\nThis gives us a balanced tree construction in the unification stage that ensures that our $parent(x)$ queries are no more than $log(n)$ per query.\n\nHowever... _can we do better?_\n\nIt turns out that indeed, we can!\n\n## Path compression\n\nLet's consider the following alternative to our initially proposed `parent(x)` function.\n\n```cpp\nint parent(int i){\n    if(dsu[i]==i) return i;\n    else return dsu[i] = parent(dsu[i]);\n}\n```\n\nNotice that the only line that has changed is the last line. We simply assign $DSU(i)$ to the parent of $DSU(i)$ at every query operation. This has the effect of shortening the path we must traverse on our journey to find the root node from any child.\n\nSay we break the numbers in intervals of $log^*n$. We get the following split.\n\n$$ [1],[2],[3, 4],[5,\\dots, 2^4],[2^4+1,\\dots,2^{16}],[2^{16}+1,\\dots,2^{65536}],\\dots $$\n\nNotice the following\n\n1. If a node $x$ on the path to the root is of the same rank as the parent, say in the interval $[k+1, \\dots, 2^k]$, then the parent can increase its rank a maximum of $2^k$ times. After these many jumps, it is incremented to the next interval.\n2. If a node $x$ on the path to the root has a rank lesser than the rank of the node's parent, then there can be only $log^*n$ nodes of this type.\n\nThis tells us that $2^k\\times|\\text{nodes with rank} \\gt k| \\leq nlog^*n$\n\nCombined with the unification via rank optimization, it is possible to prove that the amortized bound over all operations can be as low as $O(\\alpha(n))$ where $\\alpha(n)$ is the inverse Ackermann function. This can be reasonably approximated to a constant as the inverse Ackermann function is a function that grows _extremely slowly_. In fact, $\\alpha(n) \\lt 4$ for $n \\lt 10^{600}$.\n# Code!\nBelow are links to `C++` implementations of both the fully equipped Disjoint Set Union data structure and Kruskal's.\n[algorithms-notebook/dsu.cpp at main · akcube/algorithms-notebook](https://github.com/akcube/algorithms-notebook/blob/main/code/graph/dsu.cpp)\n[algorithms-notebook/kruskals.cpp at main · akcube/algorithms-notebook](https://github.com/akcube/algorithms-notebook/blob/main/code/graph/kruskals.cpp)\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [Disjoint Set Union - cp-algorithms](https://cp-algorithms.com/data_structures/disjoint_set_union.html)",
    "lastmodified": "2024-05-30T09:58:44.282025399+05:30",
    "tags": []
  },
  "/blog/network-flow-algorithms-ford-fulkerson": {
    "title": "Network-Flow Algorithms, Ford Fulkerson",
    "content": "Let's learn another really cool tool that can be used to solve optimization problems, network flows!\n\n# What is the network flow graph?\n\nA network flow graph $G = \\langle V, E \\rangle$ is nothing but a directed graph, with 2 distinctive features.\n\n1. It has 2 distinct vertices $S$ and $T$ marked. $S$ is the **source** vertex and $T$ is the **sink** vertex. These vertices are distinct.\n2. Every edge $e \\in E$ has some capacity $c_i$ associated with it. It is implicitly assumed that $\\forall e \\in E, c_i = 0$.\n\nAn example of one such graph is given below\n\n![nf-1](/images/nf-1.png)\n\n\nHere, $S = 1$ and $T = 6$. We will use this same example when discussing further ideas.\n\n## The problem\n\nThe problem that network flow attempts to solve is pretty simple. It asks the questions, _\"Given an infinite amount of \"flow\" at source $S$, what is the maximum amount of \"flow\" you can push through the network at any point of time and reach sink $T$?\"_\n\nAn intuitive way to think about this is to pretend that the source $S$ is an infinite source of water and that the capacities on each edge are sort of like the maximum amount of water that can flow through each of the \"pipes\" or edges. If we think of edges in terms of pipes, the question basically asks how much water we can push through the pipes so that the maximum amount of water reaches sink $T$ per unit time.\n\nWhy is this helpful? Think about traffic scheduling, for example, we could replace water with traffic and the problem would be similar to scheduling traffic through a busy set of streets. Replace it with goods flowing in a warehouse system and we begin to see how powerful this model of the optimization problem is.\n\nTo define this more formally, the only primary constraints are as follows:\n\n1. The flow through any edge **must** be $\\leq$ the capacity of that edge.\n2. The flow entering and leaving any given vertex (except $S$ or $T$) must be the same. (Pretty similar to Kirchhoff's current laws.)\n\nHere is an example of a valid network flow assignment:\n\n![nf-2](/images/nf-2.png)\n\n\nWe can manually go over every vertex and ensure that the two constraints are obeyed everywhere. Further, notice that the flow of this network $= 3$. (Just sum up the flow going to $T$, i.e., the edges incident on $T$)\n\nAn interesting observation is that we appear to have \"cyclic flow\" within our graph with this particular assignment of flow. Eliminating this does not change the total flow going to $T$, so this is pretty much the same assignment without that cyclic flow within the network:\n\n![nf-3](/images/nf-3.png)\n\n\nBut what about the max flow assignment for this network? is 3 the maximum flow we can achieve? Or can we do better? After a bit of fiddling around, we can notice that we can do better by pushing more flow on the bottom half of this network instead of sending 1 flow up to the top from node 3. Fixing this ends up giving this network:\n\n![nf-4](/images/nf-4.png)\n\n\nIt can be proven that we cannot do better than this for this particular network. The max flow of this network is 4.\n\nHopefully, the above examples have managed to convey the true difficulty that flow algorithms face. Solving network flow is **not** easy, primarily because from any given state, the optimal state might not be reached by just monotonically increasing flow through edges. We might have to reduce the flow through some edges to increase the flow in others. Changing the flow amount through any one edge ends up affecting the entire network. So we need to find ways to iteratively increase the flow in our network, **BUT** it is not a monotonic increase. So we must sometimes backtrack and reduce flow in some edges. But perhaps by focusing on monotonically increasing the _max flow_ of our network, we might be able to figure out a proper algorithm that incorporates this backtracking. This is the primary goal we keep in mind when trying to solve max flow.\n\n## Defining the problem formally\n\n### Some useful notation\n\nFor the remainder of this article, we will use \"implicit summation\" notation. All sets will be named by capital letters, and whenever we use sets in the place of elements like for example $f(s, V)$, this means the summation of flow $\\sum_{v\\in V} f(s, v)$. We use this notation to simplify the math we will be writing.\n\n### Formal definition\n\n**Flow:** We define the _flow_ of a network $G$ as a function $f:V \\times V \\to R$ satisfying the following 3 constraints,\n\n1. $\\forall u,v \\in V, f(u, v) \\leq c(u,v)$. That is, flow through any edge must be less than the capacity of that edge.\n2. $\\forall u\\in V - \\{ s, t \\}, f(u, V) \\implies \\sum_{v\\in V}f(u,v) = 0$. That is, flow entering and exiting every node except source and sink is 0. It is conserved.\n3. $\\forall u, v \\in V, f(u,v) = -f(u,v)$. This is not the flow between two vertices, given any two vertices on the network $u$ and $v$, the flow going from one vertex u to v should be the negation of the flow from v to u. This property is called _skew-symmetry._\n\n### Defining flow\n\nLet us denote the value of the flow through a network by $|f|$. Then we define this quantity as\n\n$$ |f| = f(s, V) $$\n\nIntuitively, this is essentially all the flow (sum) that is going from the source node to every other vertex on the graph. It is important to note that the summation is not of all positive terms, if there is flow going from some vertex $v$ to $s$, then this term would be negative (skew symmetry).\n\nUsing this, it is possible to prove that $|f| = f(s, V) = f(V, t)$. That is, it is all the flow going to vertex $t$ and is more \"intuitive\" to understand as the definition of flow. But before we can prove this, let's go over some key properties of flow-networks which we can derive from the constraints.\n\n**Properties:**\n\n1. $f(X, X) = 0 , X \\subset V$, this is derivable from skew-symmetry.\n2. $f(X,Y) = -f(Y,X), X,Y \\subset V$. Direct consequence of skew symmetry.\n3. $f(X \\cup Y, Z) = f(X,Z)+f(Y,Z) \\text{ if } X\\cup Y = \\phi$. If the intersection of $X$ and $Y$is null, then we can safely add the two flows separately as there is no risk of double counting.\n\nNow that we know these properties, let's prove it!\n\n$$ \n|f| = f(s, V) \\\\\n$$\n\nLet's start from the definition of our flow amount $|f|$. Using property 3, we can transform it to mean\n\n$$\n\\begin{aligned}\nf(V, V) = f(s \\cup (V-s), V) = f(s, V) + f(V-s, V) \\\\ \\implies |f| = f(s,V) = f(V,V)-f(V-s, V) \\\\ \\implies |f| = 0 - f(V-s, V) \\\\\n\\end{aligned}\n$$\n\nThis is intuitively just saying that flow from $s \\to V$ is the negative of the flow from other vertices to all vertices. This is because flow within non-source-sink vertices is 0 and they must all flow out the sink. Now, notice that we want to try to prove that $|f| =f(V,t)$. To do this, we will attempt to isolate $t$ from the above equation using the 3rd property again.\n\n$$ \n\\begin{aligned}\nf(V-s, V) = f(t \\cup (V-s-t), V) = f(t, V) + f(V-s-t, V) \\\\ \\implies |f| = -f(t, V) - f(V-s-t, V) \\\\ \\implies |f| = f(V,t) + 0 \\\\ \\implies |f| = (V,t) \n\\end{aligned}\n$$\n\nThe tricky part here is understanding why $f(V-s-t,V) = 0$. This is because of flow conservation. Flipping it around, we get $f(V, V-s-t)$. By the 2nd constraint imposed on our flow network, this quantity is constrained to be 0 always. Hence we have now proved that\n\n$$ |f| = f(s, V) = f(V,t) $$\n\n# Ford-Fulkerson\n\n## Residual networks\n\nWe denote the residual network of a flow network $G$ by $G_R(V_R, E_R)$.\n\nThe only constraints on the edges are that all the edges have strictly positive residual capacities. 0 means the edge is deleted. And, if $(u,v) \\notin E, c(v,u) = 0, f(v,u) = -f(v, u)$\n\nEssentially, $\\forall e\\in E_r, c_{Re} = c_e-f_e$. The residual edges represent edges that \"could\" admit more flow if required. Here $c_e$ is the capacity of the edge in the original flow graph and $f_e$ is the flow passing through the edge in the original flow network.\n\nThe idea behind these edges becomes more apparent when we actually construct the network.\n\nConsider the old suboptimal max flow network we had.\n\n![nf-5](/images/nf-5.png)\n\n\nWe'll begin by constructing the residual graph for this network. Remember, for each edge in the network, we add an edge with capacity $c_e - f_e$ as long as this quantity is $\\gt 0$. And now, to respect the last constraint, we must ensure that we add a back-edge in the opposite direction with value = $f_e$ as long $f_e \\gt 0$. This is the **key** idea behind what the residual network hopes to accomplish. Recall back when said one of the reasons the flow problem was very difficult was because it is very difficult to account for having to _reduce_ flow in some edges to increase max flow? This residual network is what helps the algorithm get around this problem. Here is the residual network:\n\n![nf-6](/images/nf-6.png)\n\n\nNow, the Ford Fulkerson algorithm becomes extremely simple. It simply says, use any graph traversal algorithm such as BFS or DFS to find _an augmenting path_ in this graph, and apply it to the original graph.\n\nWe formally define an augmenting path as a path from $s_R$ to $t_R$ in the residual graph. Recall that every edge in the residual graph **must** be a positive value. If such a path is found, then it **must** be possible to increment the value of max flow in the network by **at least** 1. This is because the residual graph is essentially an entire encoding of every possible increase/decrease in flow that we can perform on the original graph. The presence of a path with all edges $\\gt 0$ implies I can increase flow from $s_R$ to $t_R$ by at least 1.\n\nIf this is understood, the Ford Fulkerson algorithm becomes pretty simple.\n\n### Pseudocode\n\n1. Construct the residual graph for some given flow network $G$\n2. While we can find an augmenting path in the residual graph:\n    1. Get the `min` of the edges that constitute this path and increment the flow in the original graph by this value along the edges in the residual graph. If it is a direct edge, increment by `min`. If it is a back-edge, decrease flow by `min`.\n    2. Reconstruct residual graph.\n    3. Repeat. If no more augmenting paths are found, we have achieved max flow.\n\n## Proof\n\nWhy does this algorithm work optimally all the time? To prove the correctness of this algorithm, we will first prove the correctness of the Max-flow, Min-cut theorem.\n\n## Max-Flow, Min-Cut\n\nThe theorem states that the following statements are equivalent.\n\n1. $|f| = c(S, T)$ for some cut $(S, T)$.\n2. $f$ is the maximum flow.\n3. $f$ admits no augmenting paths.\n\nWe will prove this theorem by proving $1 \\implies 2 \\implies 3 \\implies 1$.\n\n### Proving $1 \\implies 2$\n\nWe know that $|f| \\leq c(s, t)$ for any cut $(s, t)$. Hence, if $|f| = c(s, t)$ then $f$ must be the maximum flow through this network.\n\n### Proving $2 \\implies3$\n\nWe can prove this by contradiction. Assume that there existed some augmenting path. Then this would imply that we could increase the max flow by some amount, hence contradicting the fact that $f$ is the maximum flow. Hence $f$ cannot admit any augmenting paths.\n\n### Proving $3 \\implies 1$\n\nLet us assume that $|f|$ admits no augmenting paths. That means, we have no path from $s$ to $t$ in $G_R$. We now define two sets $S = \\{v\\in V : \\text{ there exists a path in } G_R \\text{ from } s \\to v\\}$. The other set is defined as $T = V-S$. Trivially, $s \\in S$ and $t \\in T$, as I cannot reach $t$ from $s$. Therefore, these two sets form a cut $(S, T)$.\n\nNow, we pick two vertices $u \\in S$ and $v \\in T$. Now, by definition, there is a path from $s$ to $u$. But no path from $u \\to v$. Otherwise $v \\in S$, which is false.\n\nNow, $c_R(u,v)$ **must** be zero. $c_R(u,v)$ is by definition always positive. Now, if $c_R(u, v) \\gt 0$ it would imply that $v \\in T$. This is a contradiction. Therefore, $c_R(u,v) = 0$.\n\nNow, we know that $c_R(u, v) = c(u,v) -f(u,v) \\implies f(u,v) = c(u,v)$\n\nFor our arbitrary choices of $u \\in S$ and $v \\in T$, we arrive at the conclusion that $f(S, T) = C(S, T)$.\n\nSince $1 \\implies 2 \\implies 3 \\implies 1$, the Min-Cut Max Flow theorem is true.\n\n## Proving Ford-Fulkerson\n\nNow, the Ford Fulkerson algorithm terminates when there are no longer any more augmenting paths in $G_R$. According to the Maxflow MinCut theorem, this is equivalent to our network reaching maximum flow. Hence we have proved the correctness of our algorithm.\n\n## Complexity\n\nIt is easy to see that for integral capacities and flow constraints, finding an augmenting path implies increasing the value of maximum flow by **at least** one. This means that the algorithm will at least increment flow in network by 1 per iteration. Hence it will terminate and we can bound the complexity to $O((V+E)U)$ where $V+E$ is the complexity of the BFS and $U$ is max flow.\n\nFor non-integer capacities, the complexity is unbounded.\n\nThis... isn't great. Because our complexity depends on the maxflow of the graph. If we construct a graph such that at each iteration, we have worst case and the algorithm increases flow in the network by only one unit and the capacity on the edges is large, we might end up doing millions of iterations for a small graph.\n\n## Edmond-Karp \n\nEdmond and Karp were the first to put a polynomial bound on this algorithm. They noticed that BFS implementations of Ford Fulkerson's outperformed DFS versions a lot. Upon analyzing these implementations, they were able to put a polynomial bound on the problem. Their were able to reduce it the following bound: $O(VE^2)$. **The coolest part about this is that this is true even for _irrational_ capacities!**\n\nThe intuition is, that every time we find an augmenting path one of the edges becomes saturated, and the distance from the edge to $s$ will be longer, if it appears later again in an augmenting path. And the length of a simple paths is bounded by $V$.\n\n## Dinics\n\nDinic's algorithm solves the maximum flow problem in $O(V^2E)$.\n\n## More recent research\n\nThe asymptotically fastest algorithm found in 2011 runs in $O(VElog_{\\frac{E}{VlogV}V})$ time.\n\nAnd more recently, Orlins algorithm solves the problem in $O(VE)$ for $E \\leq O(V^{\\frac{16}{15}-\\epsilon})$ while KRT (King, Rao and Tarjan)'s does it in $O(VE)$ for $E \\gt V^{1+\\epsilon}$\n\nThere's a lot of research going on in this field and we know no proven lower bound for this algorithm. Who knows, we might be able to get even faster! Techniques like push-relabel, with a greedy optimization have managed to get a lower bound of $O(V^3)$. This modification was proposed by Cheriyan and Maheshwari in 1989.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. [Incremental Improvement: Max Flow, Min Cut - MIT OCW 6.046J - Srinivas Devadas](https://youtu.be/VYZGlgzr_As?si=iPvwN-x_4ac5yQHG)\n",
    "lastmodified": "2024-05-30T09:58:44.285358742+05:30",
    "tags": []
  },
  "/blog/new-and-delete": {
    "title": "`New` and `Delete`",
    "content": "# `new` \u0026 `delete`\n\nBefore we go ahead and figure out what smart pointers are, let's take a moment to look back to how we handle heap allocated memory in C++. Back in C, we had the `malloc` and `free` functions to handle heap memory. `new` and `delete` are C++ operators that try to do the same task, but cleaner. \n\n```cpp\nint *b = (int*) malloc(sizeof(int)); // Old C-style heap allocation\nint *c = new int; // New C++ heap allocation\n```\n\n`malloc` returns `void*`. You will notice that back in C, we did not have to explicitly cast a `void*` to `int*`, however in C++, implicit pointer type conversion is a compile error. `int *b = malloc(sizeof(int))` will throw:\n\u003e `error: invalid conversion from ‘void*’ to ‘int*’ [-fpermissive]`\n\nIn C++, `new` and `delete` are **operators.** They are **not** functions like their C counterparts. This means that just like any other operator, they can be overloaded to do pretty much anything. This means that the behaviour of `new` \u0026 `delete` are dependent on the C++ library and compiler that you are using. \n\nHowever, most implementations just implement calling `new` to call the underlying `malloc` function. And similarly `free` for `delete`. But one key additional task that `new` does is it **will** also **call the constructor** for the object it is creating. And `delete` the destructor.\n\nWe'll use the following `Entity` class as a toy-example when playing around with `new` / `delete`.\n\n```cpp\nclass Entity{\npublic:\n\tEntity(){\n \t\tstd::cout \u003c\u003c \"Constructor!\" \u003c\u003c std::endl; }\n\tEntity(const std::string \u0026name) : name(name) {\n\t\tstd::cout \u003c\u003c \"P-Constructor!\" \u003c\u003c std::endl; }\n\t~Entity() {\n \t\tstd::cout \u003c\u003c \"Destructor!\" \u003c\u003c std::endl; }\nprivate:\n\tstd::string name;\n};\n```\n\nHere are three ways to use `new` in C++.\n\n```cpp\nint main(void){\n\tEntity *obj = new Entity; // Output: Constructor!\n\tEntity *same_thing = new Entity(); // Output: Constructor!\n\tEntity *pobj = new Entity(\"abcd\"); // Output: P-Constructor!, name: abcd\n\tEntity *obj_arr = new Entity[5];\n\t/**\n\t * Output:\n\t * Constructor!\n\t * Constructor!\n\t * Constructor!\n\t * Constructor!\n\t * Constructor!\n\t */\n\t// This is called \"placement new\"\n\tstd::cout \u003c\u003c sizeof(Entity) \u003c\u003c std::endl; // Output: 32\n\tint *space = new int[10];\n\tEntity *placement_new = new(space) Entity[2];\n\t/*\n\t * Output:\n\t * Constructor!\n\t * Constructor!\n\t */\n}\n```\n \nSo the first 4 examples are the basic ones. You'll notice that `new` always makes it a point to call the constructor of the class we're allocating memory for. This is an attempt to work around the uninitialized memory problem we have with `malloc`. Links back to [RAII - Resource Acquisition Is Initialization](/blog/raii-resource-acquisition-is-initialization) principles as well. We **don't** want uninitialized memory. We can use `new` to also initialize an object with it's parameterized constructor instead of the default one. However we can't parameter initialize an array of them :) \n\nGetting a pointer to an array of `Entity` objects is also quite simple. `obj_arr` is a pointer to a contiguous chunk of memory that points to an array of 5 `Entity` objects.\n\nThe interesting `new` use-case here is the \"placement new\". Here, `new` isn't actually allocating a block of memory. It simply uses the previously allocated memory for `space` and just initializes `Entity` in that memory by calling it's constructor. \n\nFor `delete`, it's pretty similar. \n\n```cpp\ndelete obj; // Output: Destructor!\ndelete[] obj_arr;\n/**\n * Output:\n * Destructor!\n * Destructor!\n * Destructor!\n * Destructor!\n * Destructor!\n*/\n// Note! It's also possible to compile\ndelete obj_arr; // Output: Destructor!\n```\n\nYou'll notice the last way to call `delete` actually just calls the destructor once. So when de-allocating a pointer to an array of elements in memory it's important to always remember to use `delete[]` instead of `delete` to properly clean this memory. \n\n## Why `new` \u0026 `delete`?\n\nOne, it's a lot cleaner than the C-style way. Two, it is a paradigm that avoids the uninitialized memory issue we can have when using the C-style `malloc` and `free` functions. `new` and `delete` prevent this from ever happening by **always** calling the constructor and destructor. \n\nHowever, a problem they still don't solve is the problem of memory leaks and dangling pointers. To solve this, we have the idea of [Smart Pointers](/blog/smart-pointers).\n",
    "lastmodified": "2024-05-30T09:58:44.288692086+05:30",
    "tags": []
  },
  "/blog/p-vs-np": {
    "title": "P vs NP",
    "content": "This is one of the most famous unsolved questions in computer science. I mean, seriously, the clay math institute offers a reward of a **million** dollars to the first person that is able to solve this problem. [https://www.claymath.org/millennium-problems/p-vs-np-problem](https://www.claymath.org/millennium-problems/p-vs-np-problem)\n\nWhy? What's so special about this problem and what even _is_ the problem?\n\nLet's begin by defining the problem. The problem asks, is $P = NP$? That is, is the set of _all_ the problems in $NP$ the same as the set of all the problems in $P$? A more intuitive way to phrase this question would be asking, \"Are all problems that can be _verified_ in polynomial time, also be _solved_ in polynomial time?\"\n\nBut why is this one of the most famous unsolved problems in computer science? What are the implications of such a result? Why is this even a question, do we even have _any_ reason to believe that $P$ _might_ equal $NP$?\n\nHere are a few, _interesting_ answers to these questions.\n\n1. If $P$ _did_ equal $NP$, it would mean that simply being able to _check_ if a solution is correct, would be **no harder** than solving the problem itself. Optimization problems like transport routing, production of goods, circuit design, etc. are **all** $NP$ problems. We would be able to get optimal answers to these solutions _much_ faster than we are able to today. The economy could be made so much more efficient. Protein folding is an $NP$ problem. If we could make protein folding a problem in $P$ then we would be able to make huge breakthroughs in biology. We would be able to cure cancer! One of my favorite quotes describing the implications of $P=NP$ is from an MIT researcher,\n    \n    \u003e _\"If P=NP, then the world would be a profoundly different place than we usually assume it to be. There would be no special value in “creative leaps,” no fundamental gap between solving a problem and recognizing the solution once it’s found. Everyone who could appreciate a symphony would be Mozart; everyone who could follow a step-by-step argument would be Gauss; everyone who could recognize a good investment strategy would be Warren Buffet.\"_ - Scott Aaronson\n    \n    One small downside is that RSA is also a $NP$ problem. If $P=NP$, all known security encryption measures would breakdown and none of our passwords would be safe :)\n    \n2. But the truth is, most computer science researchers do **not** believe that $P=NP$. Consider the first definition of $NP$ problems that we gave. We are essentially relying on non-determinism in our Turing machine. We are relying on the fact that the Turing machine is somehow able to \"magically\" or \"luckily\" _always_ pick the right path of traversal. Luck or magic is not something we can model in a deterministic Turing machine. However, despite all this, no one has been able to prove $P \\neq NP$.\n    \n3. Finally, problems in $NP$ have indeed been shown to be in $P$. Consider sorting an array by going through all its different permutations, such an algorithm would take $O(n!n)$ time. It is not in $P$. However, after we cleverly came up with a better algorithm such as bubble sort or merge sort, we managed to reduce this problem to be in $P$ by coming up with an $O(nlogn)$ algorithm for solving it. Similarly, problems we once thought to be in $NP$ have been shown to be in $P$ after someone managed to come up with a clever algorithm to solve the problem faster. But just because some problems we thought to be in $NP$ were later found to be in $P$ , does not mean that the two classes are equal. In fact, that the question $P=NP$ ? is really asking is if $P = NP-Complete$. Recall that $NP-Complete$ problems are the hardest problems in $NP$. Every single problem that belongs in $NP$, including the $NP-Complete$ problems are reducible to an $NP-Complete$ problem. This means that if we could somehow reduce even **one** problem belonging to the $NP-Complete$ class to $P$, we would be able to prove $P=NP$. So far, problems in $NP$ were found to be reducible to $P$, but never an $NP-Complete$ problem. As mentioned on the Clay institute website, _\"However, this apparent difficulty may only reflect the lack of ingenuity of your programmer.\"_ Someday, someone just might be able to come up with a radical new algorithm to reduce one of the $NP-Complete$ problems to $P$. There is a possibility, even if highly unlikely.\n    \n\nThis is a view of the complexity classes as we know it, depending on the result of the $P$ vs $NP$ problem.\n![pnp-x](/images/pnp-x.png)\n\n\nCourtesy: [https://brilliant.org/wiki/complexity-classes/](https://brilliant.org/wiki/complexity-classes/)\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n3. [Why We Might Use Different Numbers in the Future - Up and Atom](https://youtu.be/JS40jPaogM4?si=2DM7YS6xnipvUO5C) (Great Channel, recommend checking out)\n4. [Complexity Classes - Brilliant.org](https://brilliant.org/wiki/complexity-classes/)\n",
    "lastmodified": "2024-05-30T09:58:44.29202543+05:30",
    "tags": []
  },
  "/blog/public-key-cryptography-coming-up-with-rsa": {
    "title": "Public Key Cryptography, Coming Up With RSA",
    "content": "# Mathematical operations across number systems\n\nWhy do we use the decimal number system? We as humans have been taught to count, add, subtract, multiply and divide all in the base 10 number system. We could've just as easily used binary, or maybe even roman numerals. But we chose 10. Why?\n\nIn fact, humans didn't always use the decimal number system. Back in the day, counting began with something akin to tally marks. However, it was pretty much impossible to do much more than count small numbers. Working with large numbers meant we had to make a LOT of marks.\n\nEventually, this lead to the birth of roman numerals. But even this wasn't great. It was very difficult to add and multiply numbers. Comparisons were not easy either. Over the evolution of number systems, at some point, we decided to settle on decimal because it provided a convenient representation to add and compare numbers in. Multiplication is still slow but not as bad as it was with Roman or the Tally systems.\n\n![pkc-1](/images/pkc-1.png)\n\n\nA rough comparison of the different systems of representation.\n\nI watched this video recently and I think it provides a great overview of how our choice of number systems came to be what it is and how they might change in the future.\n\n[Why We Might Use Different Numbers in the Future - Up and Atom](https://www.youtube.com/watch?v=JS40jPaogM4)\n\nOur choice of number systems might change. If we find a different base that gives us something more, there's a very good chance that we might indeed ditch the decimal number systems and switch to something else altogether.\n\n2 cool number systems to consider are the base 8 and base 12 number systems.\n\n## Making multiplication _faster?_\n\nWell, we might not change how fast computers are able to multiply two numbers, but what about humans?\n\nConsider multiplying in base 10, we're able to multiply numbers quickly because we remembered the times tables for the different numbers as kids. The _easiest_ times tables to remember are the ones that are a factor of the base number. For decimal, we have easy 2 and 5 times tables.\n\nThis is intuitively understood from the fact that dividing 10 by those numbers gives us an integer. This has the overarching implication that their times tables follow a regular pattern that is easier to remember.\n\nConsider octal (base 8) and duodecimal (base 12)\n\nWith octal, it has a smaller set of times tables and has the same number of factors. 2 and 4. Further, it gives us an additional property, simple halving of base. $\\frac{8}{2} = 4 \\text{ and } \\frac{4}{2} = 2 \\text{ and } \\frac{2}{2} = 1$.\n\nDuodecimal has a bigger times tables but has even more factors. 4! We have: 2, 3, 4 and 6. It also has better halving than decimal. $\\frac{12}{2} = 6 \\text{ and } \\frac{6}{2} = 3$.\n\nThese simple properties might not make them any better to work with for computational purposes, but for humans, it might make handling different computations easier :)\n\n# Public Key Cryptography \u0026 RSA Encryption\n\nSo far we only talked about ways to make these operations faster, because in general, we always consider faster to be better. However, there is a field where making things slower is better. That is the field of public-key cryptography.\n\nLet's say we have 2 people Alice and Bob trying to communicate with each other. Most cryptography revolving around how they can securely send each other a message relies on the fact that they both had previously agreed upon a secret key. If this setup was possible, they could do something as simple as a Caesar cipher to encrypt the message. However, in many situations, it is not possible for Alice and Bob to previously agree upon such a key.\n\nThe public key cryptography problem is the problem of sending this key itself privately between Alice and Bob.\n\n**Diffie-Hellman** key exchange is a well-known algorithm that proposes a great solution to this problem. It is theoretically nice to hear, but it relies on the fact that we can mathematically come up with a construct where we are able to generate a trapdoor function with the following properties.\n\n1. Let's say Alice and Bob each get to keep a public key and a private key. $A_{public}, A_{private}, B_{public}, B_{private}$\n2. We want to have a method where we can **encrypt** a message using the public key quickly.\n3. But at the same time, the **decryption** process must be very slow using the public key.\n4. However, the decryption process must be very quick using the private key.\n\n## The trapdoor\n\nWe use the idea of modular inverse here to come up with a sound mathematical model of such a trapdoor function.\n\n### The slow operation\n\nFor our trapdoor to work, we need some operation that is extremely slow to compute. The operation that we will be looking at is the factorization of prime numbers. Integer prime factorization is a **hard** problem. There is no known polynomial-time algorithm that can factorize a number into its primes.\n\n_However,_ multiplying the factors to get the original number is **easy**.\n\nI found [this visualization](https://www.khanacademy.org/computer-programming/time-complexity-exploration/1466763719) quite nice to understand the idea from.\n\n![pkc-2](/images/pkc-2.png)\n\n\n![pkc-3](/images/pkc-3.png)\n\n\nFor smaller inputs, integer prime factorization is quite fast. But with an increasing number of bits in the input, the algorithm shows its exponential complexity. It becomes pretty much unfeasible for any computational device that we have today to solve the problem in a reasonable amount of time. Notice that multiplication, however, remains quite fast.\n\nNow, to understand the RSA algorithm better, it is important to have an understanding of [Wilson's Theorem, Fermat's Little Theorem \u0026 Euler's Totient Function](/blog/wilson-s-theorem-fermat-s-little-theorem-euler-s-totient-function). Once we have those tools to help us, we can build the rest of the devices we need to build our algorithm.\n### One last trapdoor\n\nLet's suppose we had some integer $m$ and we performed the following operation on it.\n\n$$ m^e \\ mod \\ n \\equiv c $$\n\nNotice that computing $c$ is **easy**. We can compute the above expression quickly using techniques like binary exponentiation. However, given just $c$, $n$ and $e$, it is **very hard** to compute $m$. Any algorithm that attempts to try this would have to perform a lot of trial and error.\n\n**Notice that there are no proofs for why these trapdoor functions are like so. If it can be proven that we can compute these \"inverse\" operations in polynomial time, we would be able to break the RSA encryption algorithm. The safety of RSA hinges on the hope that $P \\neq NP$.**\n\n## The RSA Algorithm\n\nNow all that is left to do is to tie up these mathematical trapdoors we've constructed to an algorithm that can effectively solve the key exchange problem.\n\nWe will begin by demarcating the different variables used in the algorithm and the domain they are visible in.\n\n**Private domain:**\n\n1. The private key $d$. This contains info about the prime factorization of $n$.\n2. The decoded message $m$.\n\n**Public domain:**\n\n1. The public encryption key $E$, which consists of the following two things\n    - A public exponent $e$\n    - The product of two large primes $n$. Note that the factorization is not known in the public domain. Only the product is visible.\n2. The encrypted message $c$.\n\nNote that the variables in the private domain are visible **ONLY** to their owner. They must never be sent in the public domain. Only the public encryption key and encoded message are sent in the public domain.\n\n### Working\n\nLet's suppose that Bob wants to send a secret message to Alice. The secret message here is represented by an integer $m$. Notice that because Alice's encryption key is available in the public domain, Bob can use Alice's encryption key to encrypt the message as follows.\n\n1. **Encryption**\n    \n    Bob performs the following operation to his message $m$ to encrypt it.\n    \n    $$ m^e \\ mod \\ n \\equiv c $$\n    \n2. **Sending the message**\n    \n    Bob now sends his encrypted message $c$ in the public domain. Notice that in the public domain, only the values of $c$, $e$ and $n$ are known. This is **not** enough to compute the value of $m$ easily. It is a hard problem and computationally not feasible to solve. Hence no potential attacker in the public domain can compromise / gain access to the secret message $m$.\n    \n3. **Decoding**\n    \n    Once Alice has received the message $c$, she needs a fast way of computing back $m$. Recall that $n$ was the product of two huge primes and Alice knows the prime factorization of $n$. Now, she needs to somehow use this additional knowledge to quickly compute the inverse of the encryption. For this, we will go back to Euler. Notice that,\n    \n    $$ \n    \\begin{aligned} \n    m^{\\phi(n)} \\equiv 1 \\ mod \\ n \\\\ \\implies m^{k\\phi(n)} \\equiv 1 \\ mod \\ n \\\\ \\implies m\\times m^{k\\phi(n)} \\equiv m \\ mod \\ n \\\\ \\implies m^{k\\phi(n) + 1} \\equiv m \\ mod \\ n \n    \\end{aligned}\n    $$\n    \n    Recall that Alice needed an easy way to get the **inverse** of the encryption that Bob performed. That is, if Bob raised $m^e$ to mod $c$, Alice needed an integer $d$ such that $(m^e)^d \\ mod \\ n \\equiv m$.\n    \n    Notice that this means that she needed $m^{e \\times d} \\equiv m \\ mod \\ n$. From the above-derived equation, we can see how the puzzle finally fits together.\n    \n    If we set\n    \n    $$ d = \\frac{k \\times \\phi(n) + 1}{e} $$\n    \n    Notice that the value of $d$ **depends on $\\phi(n)$.** And $\\phi(n)$ is a **hard** problem to compute if the factorization of $n$ is unknown. Therefore, even if $n$ and $e$ are visible in the public domain, an attacker cannot compute $d$ as he/she cannot compute the value of $\\phi(n)$ easily without knowing the prime factorization of $n$.\n    \n    However, Alice knows the prime factorization of $n$! This means that she can compute and store the value of $d$ privately and use it to **decode** any encrypted message sent to her **quickly**.\n    \n\nAnd that's it! We have an algorithm that solves the key exchange problem effectively by using the idea of modular inverse and number theory to generate trapdoor functions that allow us to construct this beautiful cryptography algorithm, RSA.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [Time Complexity (Exploration) - Khan Academy](https://www.khanacademy.org/computer-programming/time-complexity-exploration/1466763719)\n3. [Why We Might Use Different Numbers in the Future - Up and Atom](https://youtu.be/JS40jPaogM4?si=2DM7YS6xnipvUO5C) (Great Channel, recommend checking out)",
    "lastmodified": "2024-05-30T09:58:44.29202543+05:30",
    "tags": []
  },
  "/blog/randomization-primality-testing-algorithms": {
    "title": "Randomization, Primality Testing Algorithms",
    "content": "# Randomized Algorithms\nSo far, we've discussed a lot of cool algorithms which we can use to solve many different problems. However, all these problems had a polynomial-time solution that we were able to come up with. This is not true for all problems. There are many **hard** problems for which there exists **no known** polynomial-time algorithm. A lot of these problems are quite important and an efficient way to solve them is a must. Without a polynomial-time solution to them, it is not feasible to compute their solutions for large inputs on any known computational device that man has access to.\n\nIn situations like these, we try to _probabilistic-ally_ solve the problem. We sacrifice being 100% accurate for an immense boost in speed. In some sense our algorithm is _wrong_. It does not pass the test of giving the correct answer to _every_ test case. But if it can do so with great accuracy, it might be the best \"solution\" we have.\n\n## A more \"mathematical\" explanation\n\nLet's suppose we're given a **hard** problem that has no known polynomial-time solution. However, what we do have is a set of $n$ _efficient_ but **not-correct** algorithms which output the correct answer for only $\\frac{2k}{3}$ of all valid inputs. Let us denote the set of these \"probabilistic\" algorithms by\n\n$A = \\{ a_1, a_2, \\dots,a_n\\}$\n\nNow, let's say I pick some random input $i$ and give it to my algorithm $a_1$. Since it gives me the correct answer for $\\frac{2k}{3}$ of all possible inputs, my chances of getting a wrong answer are equal to $1-\\frac{2}{3} = \\frac{1}{3}$.\n\nThis is not terrible, but still not great. However, notice that I still have $n-1$ other algorithms that give me a probabilistic-ally correct answer. If we run some $m \\leq n$ such algorithms on the same input, notice that the chances of getting a wrong answer diminish to $\\frac{1}{3}^m$. After running just 5 such algorithms, our chances of getting a wrong answer are as low as 0.243%. This is a **very** good approximation and we can always do this since running 5 such efficient algorithms is _always_ much faster than running an exponential-time algorithm.\n\n# Primality testing\n\nA known computationally hard problem is primality testing. There is no easy way to test if a number is prime or not without iterating through at least all its factors $\\leq \\sqrt n$. Notice that here, the number is given as input in bits and for every added bit we have an exponential increase in complexity. The actual complexity is $\\sqrt{2^n} = 2^{\\frac{n}{2}}$. This is assuming that we are able to test divisibility in $O(1)$, which might not be true for large numbers.\n\nPrimality testing is also a very important algorithm. Algorithms like RSA are used worldwide to secure communication in web browsers, email, VPNs, etc. and it relies on us knowing very large prime numbers. Large prime numbers are difficult to find, especially if we don't have an algorithm that can test primality very quickly. But since it is a **hard** problem, we have come up with probabilistic algorithms to efficiently \"almost\" solve this problem.\n\n## Fermat Primality Test\n\nJust recently, we talked about Fermat's little theorem ([Wilson's Theorem, Fermat's Little Theorem \u0026 Euler's Totient Function](/blog/wilson-s-theorem-fermat-s-little-theorem-euler-s-totient-function)) which gives us the following equation. For any prime $p$ and _any_ integer $a$ co-prime to $p$,\n\n$$ a^{p-1} \\equiv 1 \\ mod \\ p $$\n\nThis equation always holds for primes and in general does _not_ hold for composite numbers. Notice that for different values of $a$, we essentially have an all-new algorithm to test the primality of $p$. If the equation does not hold for _any_ value $a$ co-prime to $p$, then we know for sure that the number is not prime. We can prove that by trying all values of $2 \\leq a \\leq p-2$ we can indeed guarantee that $p$ is prime. However, doing that would be worse than just iterating over all its factors and testing primality. Hence we can choose to just try the algorithm for many different values of $a$. This turns out to be a _very efficient_ probabilistic test for checking the primality of some number $p$.\n\nBelow is the implementation of such an algorithm that relies on randomness.\n\n```cpp\nbool fermatPrimalityTest(int p, int rep=10){\n\t\tif(p \u003c= 3) return p == 2 || p == 3;\n\t\t\n\t\tfor(int _=0; _\u003crep; _++){\n\t\t\t\tint a = rand()%(p-3) + 2;\n\t\t\t\tif(binpow(a, p-1, p) != 1) return false; // Fermat witness\n\t\t}\n\t\treturn true;\n}\n```\n\nThis algorithm will return the right answer most of the time. Further, notice that we don't care if $a$ is co-prime to $p$ or not. The condition is imposed on this version of Fermat's because if $p$ divides $a$ then $a \\equiv 0 \\ mod \\ p$. But this will not be an issue for the values of $a$ that we are picking.\n\nWhile performing the check, if our equation fails for some base $a$, then we call $a$ the **Fermat witness** for the compositeness of $p$. If our number $p$ passes the test for some base $a$ but $p$ is actually composite, then we call base $a$ a **Fermat liar**.\n\nA natural question to ask here is, how many such composite numbers pass this test very frequently. Are there any composite numbers that pass this test for **all** $a$ co-prime to $n$ maybe?\n\n### Carmichael numbers\n\nSadly, there are such composite numbers for which this test returns true for **all** $a$ co-prime to $p$. They are the ****[Carmichael numbers](https://en.wikipedia.org/wiki/Carmichael_number). We can identify these false positives only if try bases that are not co-prime to $p$. This makes Fermat's primality test a weak prime test. However, it is not very bad and Carmichael numbers are fairly rare. There exist only 646 such numbers $\\leq 10^9$ and only 1401644 such numbers $\\leq 10^{18}$. This is still reasonable for such a fast and efficient algorithm.\n\n## Miller-Rabin Primality Test\n\nThe idea behind this primality test is somewhat an extension of Fermat's. Let's say we are testing the primality of some integer $p$. If $p$ is even, it is obviously not prime for all values of $p \\neq 2$.\n\nLet us eliminate all even numbers (excluding the trivial case of 2). Now, given that $p$ is a odd number who's primality we're testing, $p$ being odd $\\implies p-1$ is even. This means that it has _at least_ one factor of two.\n\nLet us write $p-1 = 2^k\\cdot q$, we are essentially factorizing all the $2$ factors from the number $p$. From this construction, it must be true that $q$ is odd. Substituting this back in Fermat's test we can write it as\n\n$$ a^{p-1} \\equiv 1 \\ mod \\ p \\iff a^{2^k \\cdot q}-1 \\equiv 0 \\ mod \\ p $$\n\nNotice that we can factorize this expression further. Any term of the form $x^2-1=(x+1)(x-1)$. So we can write the above term as\n\n$$ a^{2^k \\cdot q}-1 \\equiv 0 \\ mod \\ p \\iff (a^{2^{k-1}\\cdot q}+1)(a^{2^{k-1}\\cdot q}-1) \\equiv 0 \\ mod \\ p $$\n\nNotice that the 2nd term on the RHS can be factorized further until we run out of powers of 2. That is, we can factorize it $k-1$ times to get the following expression.\n\n$$ \n\\begin{aligned}\n(a^{2^{k-1}\\cdot q}+1)(a^{2^{k-1}\\cdot q}-1) \\equiv 0 \\ mod \\ p \\\\ \\iff (a^{2^{k-1}\\cdot q}+1)(a^{2^{k-2}\\cdot q}+1)\\cdots(a^q+1)(a^q-1)\\equiv 0 \\ mod \\ p \n\\end{aligned}\n$$\n\nThis equation must be true for $p$ to be prime. This means that _at least_ one of these terms must be divisible by $p$. That is, either\n\n$$ a^q - 1 \\equiv 0 \\ mod \\ p \\iff a^q \\equiv 1 \\ mod \\ p $$\n\nholds or for some $0 \\leq r \\leq k-1$ we check if\n\n$$ a^{2^rq}+1 \\equiv 0 \\ mod \\ p \\iff a^{2^rq} \\equiv -1 \\ mod \\ p $$\n\nholds.\n\nIf none of these statements hold (for all values of $r$) then we know that $p$ **must** be composite. We call the base $a$ a _witness_ for the compositeness of $p$. However, recall that this test is only probabilistic. It is possible for certain bases to pass this test even for composite $p$. We call such bases a _strong liar._\n\n## Why Miller-Rabin over Fermat?\n\nThe nice part about this test is that unlike with Fermat, there are _no_ numbers like the Carmichael numbers where all non-trivial bases lie.\n\nWe give the name [Strong pseudoprime](https://en.wikipedia.org/wiki/Strong_pseudoprime) to composite numbers which pass the Miller-Rabin test. From the Wiki,\n\n\u003e A composite number $n$ is a strong pseudoprime to at most one quarter of all bases below $n$. Of the first 25,000,000,000 positive integers, there are 1,091,987,405 integers that are probable primes to **base 2**, but only 21,853 of them are pseudoprimes, and even fewer of them are strong pseudoprimes\n\nThe proof for the bound that for any composite $p$, the probability that a random integer $a \\in [1, N-1]$ is a witness for the compositeness of $p$ is at least $\\frac{3}{4}$ can be found here: [Primality Proving - Lecture Notes 12 from MIT](https://math.mit.edu/classes/18.783/2017/LectureNotes12.pdf)\n\nWe can take this even further!\n\nThe Miller-Rabin primality test can be made **deterministic** by _only_ testing all bases $a \\leq 2ln(p)^2$. The proof for this claim relies on the [Generalized Riemann Hypothesis](https://en.wikipedia.org/wiki/Generalized_Riemann_hypothesis) being true. However, if it does hold true, then we have a polynomial time deterministic test for the primality of some number $p$!\n\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.295358774+05:30",
    "tags": []
  },
  "/blog/set-cover-approximation-algorithms": {
    "title": "Set Cover \u0026 Approximation Algorithms",
    "content": "# Greedy (cont.)\n\nWe previously discussed how the greedy strategy to solving problems is often the **best** way to solve a problem ([More Greedy Algorithms! Kruskal's \u0026 Disjoint Set Union](/blog/more-greedy-algorithms-kruskal-s-disjoint-set-union), [Activity Selection \u0026 Huffman Encoding](/blog/activity-selection-huffman-encoding)). It, almost always, provides a very simple implementation of an algorithm which is also very efficient. This is because we are able to reduce the overarching problem to a simple local problem that we can solve quickly at every step. This makes it a great solution when it works.\n\nHowever, as is the case with all things that appear amazing, not all problems can be broken down be solved for a local optimum which restructures the problem into smaller versions of itself.\n\nGreedy algorithms can also often trick the person into believing that they are right. This is because it appears to always do \"the right\" thing. Often changes taken locally affect the global optimum. They are enticing but often **not** optimal. Hence it is quite important for an algorithm analyst to ensure that his greedy strategy is indeed optimal and avoid getting baited.\n\n## Use as approximation algorithms\n\nThat said, greedy algorithms often give us a very _good_ answer. The answer may not be optimal, but it gives us a _\"decent\"_ approximation of the answer for an average case. This is somewhat intuitively understood from the fact that since the greedy is taking the optimal path at every step, it should at least give a decent result. While this is also the reason for it baiting people into believing it is optimal, it is also a good approximation algorithm and comes in clutch when we are tasked with **hard** problems.\n\nConsider the set of **NP-Complete** problems. The **Set Cover** problem belongs to the set of NP-Complete problems. This means that it is one of the hardest problems to solve in NP. There exists no polynomial-time algorithm to solve **Set Cover** _deterministically_. (At least, as of now.)\n\nComputers take a long long long time to solve NP-Complete problems. It is not feasible to expect a computer to solve the set cover problem for n \u003e 100000 anytime in a few hundred years even. However, Set Cover is a common problem, and solving it could be _very_ useful to us.\n\n- [Solving Sudoku can be reduced to an exact cover problem](https://en.wikipedia.org/wiki/Exact_cover#Sudoku)\n- Companies (ex: airlines) trying to plan personnel shifts, often find themselves tasked with solving this exact problem\n- Many tiling problems and fuzz-testing of programs also need to solve set cover\n- Determining the fewest locations to place Wi-Fi routers to cover the entire campus\n\nBut it is not physically feasible for a computer to solve Set Cover. In cases like these, we turn to our savior, the enticing greedy algorithms. The greedy solutions for this problem are **not** optimal. But they run quickly and in most cases, provide a _\"close-to-optimal\"_ answer.\n\nBecause the strategy is not optimal and relies on picking the local optimum, it is obviously going to be possible to reverse engineer a test case against our greedy which makes it often output a not-very-optimal answer, but the point is, in the real world, we have a high probability of not facing such specific cases. This makes them a great solution to our problem.\n\n# The Set Cover Problem\n\nWe mentioned why the set cover problem is useful \u0026 said that it belonged to the **NP-Complete** set of problems. But we never stated the problem formally. The Set Cover problem asks the following question, _Given a set of elements $U$(called the universe) and a collection $S$ of $m$ sets whose union equals the universe, the set cover problem is to identify the smallest sub-collection of $S$ whose union is the universe $U$_\n\nThe brute force for this problem is $O(m^n)$. Since this is not feasible to compute, let us consider greedy approximations.\n\n## A greedy approximation algorithm\n\nAn intuitive greedy that comes to mind is the following, _\"at every local step, pick the set which covers the most uncovered elements in the universe.\"_ This intuitively makes sense because we are trying to pick the set $s_i$ which contributes the most towards completing the set cover. However note that this is not optimal and it can, in fact, be _tricked_ into picking the wrong solution at every step.\n\n### Code \u0026 Complexity\n\nThe following code snippet is a C++ implementation of the greedy algorithm. Let's try to put a bound on the complexity.\n\n- The initial sorting step takes $O(nlogn) + [O(|s_1|log|s_1|)+\\dots+O(|s_m|log|s_m|)]$\n- The outer while loop may run as many as $O(n)$ iterations in the worst case. (Consider all disjoint singleton sets)\n    - The loop inside may run as many as $O(m)$ iterations\n        - Finally, applying two pointers on these strings will again take linear time. We can write this as $O(max\\{|s_1|, \\dots, |s_m|\\})$.\n\nThe dominant term in this definitely comes from the nested while loop and not the sorting. Discarding the complexity from sorting and focusing on the loop, we see that the total complexity is\n\n$O(nm*max\\{|s_1|,\\dots, |s_m|\\})$\n\nIn general, we can say the greedy runs in **cubic** time complexity. This is a **huge** improvement from our NP-Hard $O(m^n)$.\n\n```cpp\n// Input\n\tstring U = \"adehilnorstu\";\n\tvector\u003cstring\u003e S = {\"arid\", \"dash\", \"drain\", \"heard\", \"lost\", \"nose\", \"shun\", \"slate\", \"snare\", \"thread\", \"lid\", \"roast\"};\n\n\t// Sort to allow 2 pointers later\n\tsort(U.begin(), U.end());\n\tfor(auto \u0026s:S) sort(s.begin(), s.end());\n\n\tint left = U.size();\n\tint ans = 0;\n\t// The brute force loop\n\twhile(left){\n\t\tint max_covered = 0;\n\t\tint best_pick = -1;\n\t\t// Go through all subsets of S and pick best one\n\t\tfor(int i=0, covered=0; i\u003cS.size(); i++){\n\t\t\t// Do two pointers to count new elements we are covering\n\t\t\tfor(int j=0, k=0; j\u003cS[i].size() \u0026\u0026 k\u003cU.size(); j++){\n\t\t\t\tif(S[i][j]==U[k]) covered++, k++;\n\t\t\t\telse k++, j--;\n\t\t\t}\n\t\t\t// Update pick choice\n\t\t\tif(covered\u003emax_covered) best_pick = i;\n\t\t\tmax_covered = max(max_covered, covered);\n\t\t}\n\t\t// Cleanup / Updates. Unimportant\n\t\tans++;\n\t\tstring new_string;\n\t\tset\u003cchar\u003e temp; for(auto \u0026c:S[best_pick]) temp.insert(c);\n\t\tfor(auto \u0026c:U) if(temp.find(c)==temp.end()) new_string += c;\n\t\tswap(U, new_string); left = U.size();\n\t}\n\tcout\u003c\u003cans\u003c\u003cendl;\n```\n\n### Tricking the greedy\n\nHowever, since greedy is not optimal, we can trick it into always giving the wrong answer.\n\nConsider this following case,\n\n![set-cover-1](/images/set-cover-1.png)\n\n\nOur greedy strategy will end up picking $\\{ s_1, s_2, s_3 \\}$ while the optimal answer is actually $\\{ s_4, s_5 \\}$. Notice that this logic for \"hacking\" the algorithm can be extended to work for any power of 2 $\\geq 3$\n\nThis isn't a very specifically tailored case and something we might even end up finding in real life. This is a little worrying and naturally raises the question, _\"What is the worst approximation that the algorithm can give?\"_\n\nThis might seem a little difficult to put a bound on, but it is possible to do so with just one key observation.\n\n### Putting a bound on the approximation\n\nLet's suppose that our universe set is $U$ and we are attempting to cover $U$ using the $n$ sets belonging to the collection $B$.\n\nNow, let us suppose that we **know** the optimal answer beforehand. Let this optimal answer be $k$. This means that we can always pick some $k$ sets from $B$ such that $\\cup_{b_i}^k = U$.\n\nNow, following along with the greedy strategy, we know that there will be a certain number of elements left **uncovered** after the $t^{th}$ iteration. Let's call this number $n_t$. In the beginning, the entire set is uncovered, and hence $n_0 = 0$.\n\n\u003e The pigeonhole principle states that if $n$ items are put into $m$ containers, with $n\\gt m$, then at least one container must contain more than one item.\n\nNote that at the $t^{th}$ iteration, **if** we have $n_t$ elements left and the optimal answer is $k$, then by the pigeon hole principle, there **must** be a set that has not been picked yet that can **at least** cover $\\frac{n_t}{k}$\n\nelements. This is the key observation which we can use to bound our approximation strategy. Our greedy will (by definition) pick the largest such set which covers $\\geq \\frac{n_t}{k}$ elements. This lets us put the following bound,\n\n$$ n_{t+1}\\leq n_t - \\frac{n_t}{k} = n_t . \\left( 1-\\frac{1}{k} \\right) \\\\ \\implies n_t \\leq n_0 \\left(1-\\frac{1}{k}\\right)^t \\\\ \\text{Now, } 1-x\\leq e^{-x} \\text{ and this equality only holds for } x=0\\\\ \\implies n_t \\leq n_0\\left(1-\\frac{1}{k}\\right)^t \\lt n_0(e^\\frac{-1}{k})^t=ne^{\\frac{-t}{k}} $$\n\nFurther, if we substitute $t = k \\ ln(n)$\n\n$$ n_t \\lt ne^\\frac{-t}{k} = ne^{\\frac{-k\\ ln(n)}{k}} \\\\ = ne^{-ln(n)} = ne^{ln(\\frac{1}{n})} = n.\\frac{1}{n} = 1 $$\n\nNote that $n_t$ is the number of elements left at the $i^{th}$ iteration. Therefore it must be a non-negative integer $\\lt 1$. The only possible answer is 0. When $n_t=0$, notice that the set has been completely covered and we have our answer.\n\nThis **must** mean that the algorithm will terminate after $t=k\\ ln(n)$ iterations. Our algorithm picks exactly 1 set per iteration. This also implies that if our optimal answer is $k$, our greedy strategy will pick at most $k \\ ln(n)$ sets. Hence we have successfully managed to put a bound on the approximation.\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.298692117+05:30",
    "tags": []
  },
  "/blog/shortest-common-superstring-de-brujin-graphs": {
    "title": "Shortest Common Superstring \u0026 De Brujin Graphs",
    "content": "# Preface \u0026 References\nI document topics I've discovered and my exploration of these topics while following the course, [Algorithms for DNA Sequencing, by John Hopkins University](https://www.coursera.org/learn/dna-sequencing) on [Coursera](https://www.coursera.org/). The course is taken by two instructors [Ben Langmead](https://scholar.google.com/citations?user=2JMaTKsAAAAJ\u0026hl=en) and [Jacob Pritt](https://www.coursera.org/instructor/jacobpritt).\n\nWe will study the fundamental ideas, techniques, and data structures needed to analyze DNA sequencing data. In order to put these and related concepts into practice, we will combine what we learn with our programming expertise. Real genome sequences and real sequencing data will be used in our study. We will use Boyer-Moore to enhance naïve precise matching. We then learn indexing, preprocessing, grouping and ordering in indexing, K-mers, k-mer indices and to solve the approximate matching problem. Finally, we will discuss solving the alignment problem and explore interesting topics such as De Brujin Graphs, Eulerian walks and the Shortest common super-string problem. \n\nAlong the way, I document content I've read about while exploring related topics such as suffix string structures and relations to my research work on the STAR aligner.\n# Shortest Common Superstring (SCP)\n\nWe will now attempt to model the assembly problem ([De-Novo Assembly \u0026 Overlap Graphs](/blog/de-novo-assembly-overlap-graphs)) as computational problems. Our first attempt at this will be modelling it as solving the SCP problem.\n\nA **shortest common superstring** is a string that is a combination of two or more strings, such that the resulting string is the shortest possible string that contains all of the original strings as sub-strings.\n\nThe problem of finding the shortest common superstring of these sequences is equivalent to finding the original genome sequence, as it is the shortest possible sequence that contains all of the original sequences as sub-strings. Thus, solving the shortest common superstring problem can be used to assemble a genome from a set of overlapping DNA sequences. However, a sad reality is that this problem is **NP-Complete**.\n\n\u003e***Proof sketch:** The shortest common superstring problem is NP-Complete because it is a generalization of the NP-Complete Shortest Hamiltonian Path problem. In the Hamiltonian Path problem, we are given a graph and must find a path that visits every vertex exactly once. To reduce the Hamiltonian Path problem to the shortest common superstring problem, we can represent the graph as a set of strings, where each string corresponds to a vertex in the graph. We can then create a new string for each possible path in the graph by concatenating the corresponding strings in the order that they appear in the path. The resulting set of strings will contain all possible paths in the graph as sub-strings. Finally, we can find the shortest common superstring of these strings, which will be the shortest possible path that visits every vertex in the graph exactly once.  Because the shortest common superstring problem is at least as hard as the Shortest Hamiltonian Path problem, it is NP-Complete.*\n\n## Greedy Approach\n\nA greedy approach to solving the shortest common superstring problem involves iteratively selecting the pair of strings that overlap the most, and merging them into a single string. This process is repeated until all of the strings have been merged into a single superstring. It selects the pair of strings that appears to be the best choice without considering the overall optimality of the solution. This can give us a decent reconstruction but is sadly still pretty inaccurate in practice.\n\n## 3rd Law of Assembly: Repeats Are Bad\n\nThis is probably the most **frustrating** problem in genome assembly and what makes it pretty much impossible to solve the assembly problem with $100\\%$ certainty.\n\nConsider the following example,\n\n![greedy-repeat-fail](/images/greedy-repeat-fail.png)\n\n\nOur greedy solution gave us a shorter sequence than the original genome, this is due to the presence of overlapping reads from a repeating portion of our genome which is **extremely hard** to unambigiously solve. The primary problem here is that we are aware of its existence due to the pieced together multiple reads but we are not sure about the **frequency** of these repeats.\n\n# De Brujin Graphs\n\nDe Bruijn graphs are a mathematical construct that is often used in the field of computational biology, particularly in the context of genome assembly. In a De Bruijn graph, each vertex represents a k-mer, which is a sub-sequence of length $k$ from a given string. Edges in the graph represent overlaps between k-mers, such that two vertices are connected by an edge if the corresponding k-mers overlap by $k-1$ bases. The graph can then be used to efficiently represent the overlaps between the k-mers in the original string, and can be used to reconstruct the original string by finding a path through the graph that visits every vertex exactly once. This is also called an **Eulerian Walk.** \n\nIt has exactly one node per *distinct* k-mer and one edge per *each* k-mer.\n\n![euler-walk-in-de-brujin-graph](/images/euler-walk-in-de-brujin-graph.png)\n\n\nHowever, we have still not dealt with the problem of repeats. For example, if the graph contains multiple cycles, then it may not be possible to find an Eulerian walk that correctly reconstructs the original genome sequence, as the path may not be able to distinguish between the different cycles. Additionally, if the graph contains errors or missing k-mers, then an Eulerian walk may not be able to correctly reconstruct the original genome. This is all mainly caused due to the presence of repeats in the original genome. \n\n![debrujin-fail](/images/debrujin-fail.png)\n\n\nThe issue in the above example occurs primarily due to the repeating term *AB*. This gives us multiple reshuffles of the sequence and we cannot deterministically figure out which reconstruction is correct. \n\n## Fixing What We Can\n\n![prune-useless-edges](/images/prune-useless-edges.png)\n\n\nWe often have edges like these showing up in the De Brujin graph where the existence of the blue edges nullify any information we might gain from the green edge. We can prune these from the graph.\n\n![dbg-mp-fail](/images/dbg-mp-fail.png)\n\n\nMaternal / Paternal chromosomes can have one different base in a read causing cycles like these to form. We can attempt to prune these from the graph as well. \n\n![independent-solving](/images/independent-solving.png)\n\n\nBecause repeats **always** cause ambiguity, we can attempt to break up the graph into parts and solve only the deterministic chunks first and mark the chunks with repeats as *ambiguous*. In fact, this is how most assemblers work in practice nowadays. Excluding small genomes, it is very difficult to get accurate reconstructions of a complete genome. Even the Human Genome, the most widely studied genome on the planet still has many gaps in it today due to the uncertainties caused by repeats in the genome.\n\n## Attempts at Discerning the Ambiguity\n\nOne simple solution we could provide here is not from a computational point of view but from the point of view of the technology that generates the sequences. Increasing the lengths of the reads could allow the repeating fragments to also contain some potion of distinct / unique read fragments which allows them to now be uniquely matched with better certainty. Another type of sequencing which gathers some metadata from the surrounding reads is also making it's way into the mainstream. In the end, we'll need to get more data than we already have and then develop algorithms to solve these new problems with the additional metadata to try to get better certainty about the ambiguous portions of sequenced Genomes. ",
    "lastmodified": "2024-05-30T09:58:44.302025461+05:30",
    "tags": []
  },
  "/blog/shortest-reliable-path-floyd-warshall-max-independent-set-tree": {
    "title": "Shortest Reliable Path, Floyd Warshall \u0026 Max-Independent Set (Tree)",
    "content": "Last time we discussed [A Deep Dive into the Knapsack Problem](/blog/a-deep-dive-into-the-knapsack-problem). Today, we'll look at three more interesting problems with cool Dynamic Programming solutions.\n# Shortest Reliable Path\n\nConsider the following dispatch problem. Often when trying to schedule deliveries of goods, it is not good enough to only determine the shortest path from source to destination. One needs to also take into account the number of points at which the goods must switch transport vehicles. This could have an effect on the quality of goods received. We can have similar applications in networking where we do not want to switch _edges_ multiple times. In these cases, we try to solve a slight variation of the shortest path problem.\n\nThe shortest reliable path problem asks the following question, _\"Given a graph $G$ with weighted edges, what is the shortest path from location $s$ to location $t$ such that the path consists of **at most** k-edges?\"_\n\nWe can solve this problem using dynamic programming.\n\n## The DP solution\n\nLet's think about the following recurrence. If I know what the shortest path to some vertex $v$ is using $i$ edges, I can just go over all my edges again in a \"relaxation\" step and find out what the shortest path to vertex $v$ is using $i+1$ edges. We have identified our subproblem!\n\nLet's define $dp[i][j]$ as the shortest path to reach vertex $i$ using just $j$ edges.\n\n1. **Number of subproblems**\n    \n    Notice that we have $|V|$ number of vertices and will have to compute the answer for $i:1\\to k$ edges. Therefore we will have $|V|k$ subproblems. $k$ can be around $m$. This would then require $O(|V|m)$ problems solved.\n    \n2. **Finding how to brute force the solution to some subproblem state**\n    \n    To go from knowing the shortest paths using $i$ edges, to know the solution when using $i+1$ edges, we will have to \"relax\" all the edges once. We can solve **all** the subproblems for some number of edges $k'$ by just iterating over the entire edge list in $O(m)$\n    \n3. **Finding the recurrence**\n    \n    As mentioned previously, relaxing all edges will net us the desired result.\n    \n    We can write the recurrence as\n    \n    $$ dp[v][i] = min_{(u,v)\\in E}(dp[u][i-1]+l(u,v), dp[v][i]) $$\n    \n    Notice that this implies that we initially consider all distances from $v$ to any other vertex as $\\infty$.\n    \n4. **Figuring out DAG structure**\n    \n    We can visualize this as a simple linear chain. We solve the problem for **all** vertices using $i+1$ edges in one go. So we can just think of it as a linear chain going from $i=1\\to 2 \\to 3\\to \\dots \\to k$ edges.\n    \n5. **Completing the solution**\n    \n    Armed with all the information we need, all we need to do now is calculate the final solution. Since we're computing $k$ problems in $O(m)$ iterations each, the solution has overall $O(km)$ complexity where $m = |E|$. In the worst case when $k \\to m$ we can have $O(m^2)$ complexity.\n    \n\n### A tighter bound\n\nNotice that our solution is **very** similar to the Bellman-Ford algorithm. It's because Bellman-Ford and our algorithm work on the same principle. Both the algorithms solve the very same subproblems. But notice that **any spanning tree** of our graph will connect all vertices and this implies that there will always be a path between two vertices using just $n-1$ vertices. This means repeating our algorithm $n-1$ times will converge at the optimal shortest distance solution.\n\nFrom this fact, we can naturally conclude that the bound on the value of $k$ is $|V|$. Therefore our solution will not have $O(m^2)$ complexity as we can bound $k = min(k, |V|-1)$. This gives our algorithm a better runtime of $O(|V||E|)$.\n\n### 1D Row Optimization\n\nNotice that again, we are computing the answer for all $v \\in V$ using $i$ edges using the answer for $i-1$ edges. This means that we in fact do not need to store the solution for **all** $O(|V|k)$ subproblems. Simply storing the answer for $O(|V|)$ subproblems would be enough.\n\nHence we can optimize it to just using 1 row.\n\n### Code\n\nThe code for this DP solution is quite beautiful and short. Vector `d` stores the DP values for any given state. Here, we assume the graph is stored in edge list representation. `e` is the edge list.\n\n```cpp\nvector\u003cint\u003e d (n, INF);\nd[v] = 0;\nfor (int i=0; i\u003cmin(k, n-1); ++i)\n    for (int j=0; j\u003cm; ++j)\n        if (d[e[j].a] \u003c INF)\n            d[e[j].b] = min (d[e[j].b], d[e[j].a] + e[j].cost);\n```\n\n## An alternate Greedy + DP solution\n\nDijkstra is a greedy algorithm that computes the shortest paths solution in $O(ElogV)$ with the help of a priority queue implementation using some heap. Notice that we can modify how the heap stores its top element and eliminate some skipping to arrive at a solution for the shortest reliable paths problem!\n\nLet's say I said that my new criteria for highest priority were a pair $(i, dis[v])$. In this notation, I first sort priority using $i$. The pair with the lowest $i$ is given the highest priority. Once sorted by $i$, we assign priority based on the smallest $dis[v]$.\n\n### Our claim\n\nI claim that with this additional bookkeeping, we will be able to solve this problem once we eliminate a speedup check in the original Dijkstra.\n\nLet's think about what this additional bookkeeping is doing. By enforcing this constraint, we are essentially saying that we **must** first update all reachable vertices using the smallest number of edges $i$. So we are just simply running Dijkstra for a more constrained graph. This means that I will be able to compute the solution using $i$ edges.\n\nHowever, Dijkstra skips over all the nodes already visited. This is essential in keeping the complexity down. Consider this case.\n\n![srp-1](/images/srp-1.png)\n\n\nWe will not be able to update the third node from the left do distance 3 once it has already been processed for reachability using 2 edges. Hence we will have to eliminate this skipping and force the algorithm to process new vertices again.\n\n### How is this different from the previous solution?\n\nNotice that in the previous solution, for any randomized sparse graph, we would, in the beginning, be iterating over many edges that are from the reach of the source node using a small number of edges. This is redundant work that we were doing. Here, we are **only** iterating over the edges that are reachable.\n\n**The complexity of this solution**\n\nAssuming we are using a priority queue, our solution has the worst time runtime of $O(kElogV)$. However, notice that because we are not iterating over every edge on every iteration, for sparse graphs where $k$ is small, we might have a better/faster runtime using this solution.\n\n# Floyd Warshall\n\nThe problem is as follows, _\"Given a graph G, find the shortest distance between all pairs of points.\"_\n\nNotice that we can compute the answer to this problem simply by running Dijkstra $|V|$ times. This would have an overall runtime of $O(|V||E|log|V|)$. For dense graphs, the complexity might reach $O(n^3logn)$ where $n = |V|$. We also require at the very minimum, a binary heap implementation of a priority queue.\n\nFurther, this solution will **not** work if the graph contains any negative edge weights.\n\n## The DP Solution\n\nThe first step to solving it with DP is identifying a subproblem. Let's say I order my nodes in some arbitrary fashion. This implies that my nodes are always in some order and the concept of _\"first k nodes\"_ can be applied to them. Now, I can define by DP state as follows:\n\nLet $dp[i][j][k]$ represent the length of the **shortest** path from nodes $i \\to j$ using _just_ $k$ nodes as _intermediaries._ Notice that now, we can define a recurrence between subproblems as follows:\n\n$$ dp[i][j][k] = min(dp[i][k][k-1] + dp[k][j][k-1], dp[i][j][k-1]) $$\n\nLet's see what this means. When computing the shortest distance between any two nodes $i, j$ using $k$ intermediary nodes, we assume that we **know** the optimal solution to the distance between them when using just $k-1$ intermediaries.\n\nIf these subproblems have been solved, then when computing the shortest distance between $i,j$ using $k$ intermediaries, the question essentially boils to asking _\"Should we include intermediate node $k$ in the shortest path?\"_\n\nTo answer this, we check what the shortest path from $i \\to k$ is and $k \\to j$ is using $k-1$ intermediate nodes. If the sum of these distances is lesser than the min computed so far, we can include node $k$. Notice that we are simply including 1 node. Therefore our computation for the DP state will be correct.\n\n![fw-1](/images/fw-1.png)\n\n\nThis is a visual representation of the sub-problem we're attempting to solve.\n\nNow for the base case, the distance between any two nodes using 0 intermediary nodes will be $\\infty$ when they're not connected and $l(u,v)$ when they are connected. It's essentially the adjacency matrix representation of the graph with disconnected vertices marked with $\\infty$.\n\n### Time complexity\n\nWe have $i\\cdot j \\cdot k$ subproblems to solve and each sub-problem takes $O(1)$ computation to solve.\n\nTherefore overall time complexity of our algorithm will be $O(n\\times n\\times n) = O(n^3)$ . Here $n = |V|$.\n\n### Space complexity\n\nNotice that naively, we must store the computation for $O(n^3)$ subproblems and hence require $O(n^3)$ space. However, notice that we can do something very similar to 1D row optimization. Notice that for computing all subproblems with DP state $k$, we only require the solution of all-pairs shortest paths using $k-1$ intermediaries. This means we only need to store $O(n^2)$ solutions at any point in time. Hence we can reduce the space complexity down to $O(n^2)$.\n\n### Code\n\nAgain, as with most DP solutions, the code is quite short and sweet :)\n\n```cpp\nfor (int k = 0; k \u003c n; ++k) {\n    for (int i = 0; i \u003c n; ++i) {\n        for (int j = 0; j \u003c n; ++j) {\n            d[i][j] = min(d[i][j], d[i][k] + d[k][j]); \n        }\n    }\n}\n```\n\n# Independent Set in a tree\n\nThe problem we're trying to solve here is as follows, _\"Given a tree G, find the **largest** independent set of vertices belonging to the tree. Here, we define a subset of vertices $S$ of $V$as independent if there are **no** edges belonging to G which connect **any** two pair of vertices in the subset $S$.\"_\n\nFrom the definition of \"independent set\", we can easily conclude that the set $S$ must be a bipartite subset of $G$. However, notice that any bipartite coloring won't do. More specifically, a bipartite coloring where we color one node then skip it's children and proceed won't do.\n\n![ist-1](/images/ist-1.png)\n\n\nThis is a simple counter case to that solution. We require both the lumps of vertices and the bottom and the top for the optimal solution. Notice that this hints us towards the sub-problem we require to solve. \n\n## The DP Solution\nWe can define our sub-problem as follows, _\"Should we include node $u$ in the answer or not?\"_ To further this and make this more useful, we can define a DP state as follows: _\"How many nodes would I get in my optimal matching if I included node $u$ in the subset?\"_\n\nIf our DP stores this, notice that every node $u$ is the root of some subtree. This means we can calculate the answer for each subtree of $G$ and the answer will be the $DP$ state for the root of the tree.\n\nNow, how do we find the recurrence?\n\nThis can be done greedily.\n\nNotice that **if** we include $u$ in the answer, we **cannot** include any child of $u$. The next **best** option is to include the grandchildren (children of children) of $u$.\n\nNotice that this is optimal. Because every $DP$ state stores some positive quantity and **not** choosing to include a grandchild would imply we missed a chance to increase the value. Further, each DP state is only dependent on its children and grandchildren. Hence this decision does not affect future DP states.\n\nIf we **do not** include $u$ in the answer, then we **must** pick all its children. The reasoning for this is the same as the above.\n\nNow, we have a recurrence.\n\nWith our $DP$ state defined as\n\n$$ DP[i] = \\text{ size of largest indepdendent set in subtree rooted at i} $$\n\nwe can define the recurrence as follows\n\n$DP[i] = max(1 + \\sum_{grandchildnren \\ x} DP[x] , \\sum_{children \\ y} DP[y])$\n\nThe first term is the maximum answer attainable when **including $i$.** The second term is the maximum attainable when **not including $i$.** These are the only two conditions possible.\n\n### Time complexity\n\nNotice that we have $O(n)$ where $n = |V|$ subproblems to solve for and each subproblem takes $O(1)$ complexity. Therefore the overall time complexity of our algorithm is $O(|V|)$.\n\n### Space complexity\n\nWe have $O(n)$ subproblems to solve. This gives us a space complexity of $O(n)$.\n\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.305358805+05:30",
    "tags": []
  },
  "/blog/smart-pointers": {
    "title": "Smart Pointers",
    "content": "Before reading this section, I recommend reading the previous section on [`new` and `delete`](/blog/new-and-delete) to get a better idea of the problem(s) we have with memory allocation and manipulation and how we're trying to fix them. We figured out how to workaround / solve the uninitialized memory problem, but we still have to deal with the issue of memory leaks and dangling pointers.\n# Preface\n\nAs programmers, when working in a large code base, it is often difficult to manually keep track of all the memory allocations and remember to free them correctly. Tools like `valgrind` can help identify memory leaks, but it's still a pain to run a massive project on it. Even worse, sometimes programmers are just too lazy to free memory correctly. \n\nJava and other more \"modern\" languages have the idea of keeping [garbage collectors](/blog/garbage-collectors). Its primary purpose is to automatically manage memory by identifying and reclaiming memory that is no longer needed or accessible by the program, thus preventing memory leaks and ensuring efficient memory usage. However, the existence of a garbage collector means that the program can only be run on a system on a runtime. For Java, this would be the JVM. This introduces a performance overhead because the garbage collector is a tool that is actively working in the background to identify unreachable objects and freeing them. Being C++ nerds, we don't want performance bottlenecks. \n\n![Pasted image 20230909082901](/images/pasted-image-20230909082901.png)\n\nsource: [Back to Basics: Smart Pointers and RAII - Inbal Levi - CppCon 2021](https://www.youtube.com/@CppCon)\n\nInstead, the goal for C++ is to introduce an \"API\" of sorts that programmers can use to manage their memory right. Smart pointers are a cool interface provided by the C++ standard library to leverage the power of runtime stack allocation to manage memory efficiently by automating the process of calling `new` and `delete`. In essence, smart pointers are just simple wrappers around raw pointers.\n\n# The ownership model\n\nThe way C++ tries to solve the automatic memory management problem is by introducing the ownership model. Smart pointers **enforce** this model for dynamically allocated heap memory. \n- `std::unique_ptr` =\u003e Represents a \"single owner\" model. Memory managed by an `std::unique_ptr` can only \"be owned by\" by that one instance of an unique pointer. It cannot be owned (copied) by multiple unique pointer instances. The only way to *change* ownership is to **move** the ownership to a different instance of an unique pointer. Original pointer releases, new pointer acquires. \n- `std::shared_ptr` =\u003e This builds on the unique pointer by now allowing a resource to be shared / \"owned\" by multiple `std::shared_ptr` instances. Multiple shared pointers can now share and copy the ownership rights over a shared pointer. The memory being managed is freed only when *every last* owner of the shared pointer has gone out of scope.\n- `std::weak_ptr` =\u003e This is a weaker version of the shared pointer. Weak pointers can copy / gain weak ownership over a shared pointer. This means that it does have *strong* ownership rights. If the strong owners go out of scope, the weak pointer will be invalidated. In essence, it does not hold any power over when the pointer may be invalidated / cleaned up.\n## `std::unique_ptr`\n\nAn `unique_ptr` is perhaps the simplest type of a smart pointer. It's a scoped object, which just means that when the pointer goes out of scope, it gets destroyed. Unique pointers are called unique pointers because you **cannot copy** an unique pointer. Why? Because all an unique pointer really is, is just a `class` wrapper around your raw pointer. If it was copied, we would now have **2** instances of this *manager class*. When they go out of scope, we call the destructor twice, the second one attempting to free a `nullptr`.\n\n```cpp\n// My sample simple implementation of an unique_ptr\ntemplate\u003ctypename T\u003e\nclass unique_ptr{\npublic:\n\texplicit unique_ptr() : obj(nullptr) {}\n\texplicit unique_ptr(T* obj) : obj(obj) {}\n\texplicit unique_ptr(unique_ptr \u0026other) = delete;\n\t~unique_ptr() { delete obj; }\nprivate:\n\tT* obj;\n};\n```\n\nLet's go back to our toy-class `Entity` (from [`new` and `delete`](/blog/new-and-delete)) and see how it works now:\n\n```cpp\nstd::unique_ptr\u003cEntity\u003e oobj(new Entity); // Output: Constructor!\\nDestructor!\\n\nunique_ptr\u003cEntity\u003e fobj(new Entity); // Output: Constructor!\\nDestructor!\\n\n```\n\nNote that the `explicit` constructors and deleted copy-constructors means that the following code will not compile.\n\n```cpp\nstd::unique_ptr\u003cEntity\u003e obj = new Entity; \n```\n\u003e `conversion from ‘Entity*’ to non-scalar type ‘std::unique_ptr\u003cEntity\u003e’ requested`\n\n\u003chr\u003e\n\n### `std::make_unique`\n\nThe 'recommended' way to initialize an unique pointer in C++ is using `std::make_unique\u003cT\u003e`. The primary reason this is recommended is because of **exception safety.** If the constructor happens to throw an exception we'll now not end up with a memory leak or dangling pointer. `make_unique` is basically a way to shorten writing:\n\n```cpp\n// allocation. `new` can throw an exception if constructor fails.\nEntity *b = new Entity(\"42\"); \n// Handle the memory to unique_ptr to manage the memory\nstd::unique_ptr\u003cEntity\u003e uptr(b); \n\n// OR do both in one-step with std::make_unique\nstd::unique_ptr\u003cEntity\u003e uptr = std::make_unique\u003cEntity\u003e(\"42\"); \n```\n\nHere is an example where this exception safety is of good use:\n\n```cpp\n// unsafe\nfoo(std::unique_ptr\u003cint\u003e(new int(4)), std::unique_ptr\u003cint\u003e(new int(2))); \n// safe\nfoo(std::make_unique\u003cint\u003e(4), std::make_unique\u003cint\u003e(4));\n```\n\nIn the first example, first of all we have no guarantee of order of evaluation. From [cppreference.com](https://en.cppreference.com/w/cpp/language/eval_order):\n\u003e Order of evaluation of any part of any expression, including order of evaluation of function arguments is _unspecified_ (with some exceptions listed below). The compiler can evaluate operands and other sub-expressions in any order, and may choose another order when the same expression is evaluated again.\n\n[IBM Docs](https://www.ibm.com/docs/en/zos/2.2.0?topic=only-stack-unwinding-c)\n\u003e When an exception is thrown and control passes from a try block to a handler, the C++ run time calls destructors for all automatic objects constructed since the beginning of the try block. This process is called stack unwinding. The automatic objects are destroyed in reverse order of their construction.\n\nSay the 2nd one throws an exception in the constructor, its guarding destructor will not be called and we'll be left with a memory leak. Because order of evaluation is not guaranteed we can't even easily determine the leak.\n\n\u003chr\u003e\n\nComing back to unique pointers, we know that it is unique because the copy and copy-assignment constructors have been deleted and that the memory gets freed only when the destructor is called. Not true! There are 4 cases when the memory that is managed by a shared pointer can be freed. \n1. When the object goes out of scope\n2. When we **`move`** a resource from one unique pointer to another\n\n```cpp\n\tauto e1 = std::make_unique\u003cEntity\u003e(\"abcd\");\n\tauto e2 = std::move(e1); \n\t// Leaves e1 in an 'invalidated' state (implementation defined). Accessing e1 is UB.\n```\n\n3. Explicitly `release` ownership. This stops the unique pointer instance from actively managing the raw memory and returns the raw pointer.\n\n```cpp\n\tT *raw_ptr = e1.release(); // Frees the memory\n```\n\n4. Terminate the object and replace the ownership\n\n\t```cpp\n\te1.reset(new Entity(\"efgh\")); // replace the ownership\n\t```\n\n#### Custom Destructors!\n\nA cool thing about smart pointers is that they accept custom destructors. For example:\n\n```cpp\nEntity *e = new Entity;\nstd::unique_ptr\u003cEntity, std::function\u003cvoid(Entity*)\u003e\u003e uptr(e, [\u0026](Entity *e){\n\tstd::cout \u003c\u003c \"Custom destructor!\" \u003c\u003c std::endl;\n});\n// Output: Constructor!\\nCustom destructor!\\n\n```\n### A revised implementation of `std::unique_ptr`\n\n```cpp\n// A slightly superior implementation.\ntemplate\u003ctypename T\u003e\nclass unique_ptr{\npublic:\n\texplicit unique_ptr() noexcept : obj(nullptr) {}\n\texplicit unique_ptr(T* obj) noexcept : obj(obj) {}\n\texplicit unique_ptr(unique_ptr \u0026other) = delete;\n\tunique_ptr\u0026 operator=(const unique_ptr\u0026) = delete;\n\tunique_ptr(unique_ptr\u0026\u0026 other) noexcept : obj(other.release()) {}\n\tunique_ptr\u0026 operator=(unique_ptr\u0026\u0026 other){\n\t\tif(this != \u0026other)\n\t\t\treset(other.release());\n\t\treturn *this;\n\t}\n\t~unique_ptr() noexcept { delete obj; }\n\n\tT* get() { return obj; }\n\tT* release() {\n\t\tT* cpy = obj;\n\t\tobj = nullptr;\n\t\treturn cpy;\n\t}\n\tvoid reset(T *upd) noexcept {\n\t\tdelete obj;\n\t\tobj = upd;\n\t}\nprivate:\n\tT* obj;\n};\n```\n\n## `std::shared_ptr`\n\nLike previously mentioned, a `std::shared_ptr` is an unique pointer that allows 'sharing' ownership. This means that shared pointers can be copied and assigned. \n\n```cpp\nstd::shared_ptr\u003cEntity\u003e outer;\n{\n\tstd::cout \u003c\u003c \"Start of inner scope\" \u003c\u003c std::endl;\n\tstd::shared_ptr\u003cEntity\u003e inner = std::make_shared\u003cEntity\u003e(\"abcd\");\n\tstd::cout \u003c\u003c \"End of inner scope\" \u003c\u003c std::endl;\n}\n/**\n * Output:\n * Start of inner scope\n * P-Constructor!, name: abcd\n * End of inner scope\n * Destructor!\n */\n```\n\nYou will notice, that we were now able to use the copy-assignment operator with our shared pointer object. And further, even though the inner shared pointer is out of scope, the destructor is called only after the outer shared pointer (which received ownership via the copy-assignment operator) goes out of scope.\n\n\u003chr\u003e\n### `std::make_shared`\n\nApart from the same reasons listed for `std::make_unique`, there are more reasons to use `std::make_shared` instead of `std::shared_ptr\u003cEntity\u003e inner(new Entity())`. The reason somewhat comes down to the implementation and overhead associated with shared pointers.\n\n\u003chr\u003e\n### Implementation Notes\n\nHow `std::shared_ptr` is implemented is ultimately up to the compiler and what standard library we are using. It is implementation specific, and there is no standard defined for *how* the sharing must be implemented. However, it is almost always implemented in the popular libraries using **reference counting.**\n\nWhat this means is that a shared pointer essentially manages two blocks of memory. There is a \"control block\" which contains information regarding to the reference count and then there's the memory that's being managed. In essence, you can think of the control block as being a dynamically allocated integer object that keeps reference count of the number of shared pointers which hold ownership over the managed memory that are still in scope. \n\nThis memory only needs to be allocated once, in the normal constructor of a shared pointer. Because this is when we are stating the existence of a new ownership. Now when ownership is being copied, we just need to increment this reference count. This can be done in the copy constructor and copy-assignment operator calls. Finally, as each shared pointer goes out of scope and it's destructor is called, it can just decrement the value of the reference count. When the final shared pointer goes out of scope, decrementing the reference count to zero, we now that there exist no more shared pointers which ownership of the memory being managed, and hence we can then safely de-allocate both blocks of memory. \n\n\u003chr\u003e\n\nWhy does this matter? Unlike with `std::unique_ptr`, there is an overhead associated with using a `std::shared_ptr` in the form of the control block memory that it must additionally allocate and share. If we call `std::shared_ptr\u003cEntity\u003e inner(new Entity())`, there is first an allocation in the inner call to the `new` operator. This is then followed by an extra call to allocate the control block memory. That's 2 allocations. \n\nHowever, with `std::make_shared`, it can actually construct them **together**, essentially halving the allocation requests and also keeping them close by in memory. This is significantly faster. (Remember, memory allocation cost is often **very** expensive in comparison to the other book-keeping operations here). Hence it's almost always a good idea to use `std::make_shared` instead of passing an already allocated memory-pointer to `std::shared_ptr`.\n\n\u003chr\u003e\n\n## `std::weak_ptr`\n\n`std::weak_ptr` is the final member of our little group of smart pointers which completes C++'s ownership ideology. A `std::weak_ptr` can copy ownership from a `std::shared_ptr`, except this ownership is *weak*. You can imagine it as a shared pointer which when copying, does **not** increase the reference count of the original shared pointer. Due to this, it is possible the weak pointer is still in scope but because all the *strong* owners of the managed memory have gone out of scope, the memory has been freed and the weak pointer is now in an **invalidated** state.\n\nIt's like saying, I don't actually *want ownership* of the object, but I just want to keep a reference to the allocated entity. This means `std::weak_ptr` has member functions that allow querying things like \"is the memory that the weak pointer is pointing to still alive?\" \n\n```cpp\nstd::weak_ptr\u003cEntity\u003e outer;\nstd::cout \u003c\u003c \"Outer weak_ptr use count: \" \u003c\u003c outer.use_count() \u003c\u003c std::endl;\nstd::cout \u003c\u003c \"Outer weak_ptr expired: \" \u003c\u003c outer.expired() \u003c\u003c std::endl;\n{\n\tstd::cout \u003c\u003c \"Start of inner scope\" \u003c\u003c std::endl;\n\tstd::shared_ptr\u003cEntity\u003e inner = std::make_shared\u003cEntity\u003e(\"abcd\");\n\tstd::cout \u003c\u003c \"Inner shared_ptr use count: \" \u003c\u003c inner.use_count() \u003c\u003c std::endl;\n\touter = inner;\n\tstd::cout \u003c\u003c \"Outer weak_ptr use count: \" \u003c\u003c outer.use_count() \u003c\u003c std::endl;\n\tstd::cout \u003c\u003c \"Outer weak_ptr expired: \" \u003c\u003c outer.expired() \u003c\u003c std::endl;\n\tstd::cout \u003c\u003c \"End of inner scope\" \u003c\u003c std::endl;\n}\nstd::cout \u003c\u003c \"Outer weak_ptr use count: \" \u003c\u003c outer.use_count() \u003c\u003c std::endl;\nstd::cout \u003c\u003c \"Outer weak_ptr expired: \" \u003c\u003c outer.expired() \u003c\u003c std::endl;\n/**\n * Output:\n * Outer weak_ptr use count: 0\n * Outer weak_ptr expired: 1\n * \t\tStart of inner scope\n * \t\tP-Constructor!, name: abcd\n * \t\tInner shared_ptr use count: 1\n * \t\tOuter weak_ptr use count: 1\n * \t\tOuter weak_ptr expired: 0\n * \t\tEnd of inner scope\n * Destructor!\n * Outer weak_ptr use count: 0\n * Outer weak_ptr expired: 1\n */\n```\n\nThe above code block shows the working of the two pointers succinctly. The weak pointer does not increase the `use_count` of the shared pointer. And as soon as the shared pointer exits the inner scope, the memory is freed and our shared pointer now points to invalidated memory, as shown in the output of `outer.expired()`.\n# References\n1. [SMART POINTERS in C++ (std::unique_ptr, std::shared_ptr, std::weak_ptr) - The Cherno](https://www.youtube.com/@TheCherno)\n2. [Back to Basics: Smart Pointers and RAII - Inbal Levi - CppCon 2021](https://www.youtube.com/@CppCon)\n3. [Back to Basics: C++ Smart Pointers - David Olsen - CppCon 2022](https://www.youtube.com/@CppCon)\n",
    "lastmodified": "2024-05-30T09:58:44.308692148+05:30",
    "tags": []
  },
  "/blog/stock-multiples": {
    "title": "Stock Multiples",
    "content": "When deciding how to invest in stocks, we all know that it's best to \"buy low and sell high\", but when is a stock price *low*? And when is it considered *high*? Are there more quantifiable ways to measure these qualitative terms? Stock multiples try to solve this problem by helping traders figure out how much you pay for a stock's underlying business and if this price has changed over time. Essentially, a stock multiple is a ratio that compares the current stock price to some *fundamental* quantity of the stock's underlying business. In general, the higher the multiple, the more expensive the stock is considered to be. The idea can be more intuitively explained via the following example:\n\u003eIn shopping for Pork, Beef, or Chicken it's difficult to compare the total prices since the quantity you get is different for each cut. But if we look at the *price per pound*, you can easily figure out which cut is the best bang for your buck. Multiple's work in a similar way, allowing us to compare the price of a stock to the underlying fundamentals you get with the purchase - [Stock Multiples: How to Tell When a Stock is Cheap/Expensive - The Plain Bagel](https://www.youtube.com/watch?v=21STUhQ-iP0)\n# PE Ratio\nA PE Ratio is a stock multiple which compares a company's current stock price to its earnings per share (EPS). It's one of the most popularly used stock multiples and it helps assess the relative value of a company's stock. It's very useful when used to compare a company's valuation against it's *historical performance*, or even other firms in the industry or the entire market in general. ([What is the Stock Market?](/blog/what-is-the-stock-market))\n\nFor example, let's say company $A$ was split into 20000 shares and each share was currently trading for ₹10 in the open market, and the company's earnings (net income) for the *previous* year was say ₹10,000, then the EPS is $\\frac{10000}{20000} = 0.5$. Computing the PE from this is, $PE = \\frac{\\text{Price}}{\\text{EPS}} = \\frac{10}{0.5} = 20$. \n\nNote that in this example, we used the *previous* year's earnings. But this might not be very representative of how the company will do this year. Perhaps it's an oil company and the company has just placed several environmental restrictions on it that might restrict the profit making abilities of the company significantly. Or perhaps the company was involved in some massive scandal which caused consumers to lose faith in the company's products. Regardless, this measure of EPS is a *trailing* measure. And hence this computation of the PE ratio is called as the **Trailing P/E**. However, it is also possible for analysts to try to estimate the earnings of the business for the current financial year using publicly available data and compute a new PE using these *expected* earnings per share. It is then called the **Forward P/E**. While forward P/E can escape the traps that trailing P/E is susceptible to, it has it's own drawbacks. The primary one being that expected EPS is, as the name says, expected. These estimations may not pan out and then we would have some unexpected error margin to deal with. \n\nNow, let's say we have decided to use one of the P/E measures and compute the value to be $x$. What can we infer about a stock's price based on this value? Using just $x$, we can't say much. This is because P/E is a relative measure. It does not make sense by itself. But we *can* compare the stock's current P/E to it's historical P/E values, or even with the stocks of other companies in the same sector. \n## Historical Comparison\n\u003e\n\u003e![Pasted image 20240507113833](/images/pasted-image-20240507113833.png)\n\n\u003e\n\u003eIf the company's earnings are expected to increase, but the price of the stock has fallen, it would mean that the multiple has contracted, and investors don't value the profitability of the firm as much as they used to. Alternatively, if earnings are falling but the price has risen, the multiple has expanded, meaning people are paying more for less profit. We could also compare the P/E multiple to the stock's long-term average to see whether the margin is larger or smaller than normal. If the stock's ten-year average P/E is 15 times, for example, we can assume that the stock's multiple is temporarily cheaper than normal and may want to buy if we pick up the stock in the multiple later expands back to its long-term average. Then we could earn a return even if the company's earnings are flat. A key assumption here is that a multiple is expected to revert to its mean over time, and while that doesn't always hold true, investors sometimes look for extreme variations from the mean, with many believing that short-term volatility in the stock, which could be caused by a bad press release or negative near-term headwind, will eventually subside, causing the multiple to return to its normal level. - [Stock Multiples: How to Tell When a Stock is Cheap/Expensive - The Plain Bagel](https://www.youtube.com/watch?v=21STUhQ-iP0)\n## Industry Comparison\nComparison of the multiple w.r.t to it's historical performance is useful, but it's important to compare it against other companies in the industry as well. Of course, a company might have higher P/E than it's peers simply due to having a better culture, better marketing team, etc. but it's still useful when comparing two companies to directly compare how two companies are valued against each other simply due to the fact that the underlying product they are selling remains the same. If a stock $A$ has higher multiple than stock $B$, where both the underlying companies only sell footballs, it does not immediately imply that company $A$'s stock is worse bang for your buck. P/Es are limited in the information that they are able to capture. Company $A$ might be growing at a much faster pace than company $B$, or it might have the necessary comparative advantage to quickly explore and capture market share in other industries, say football pumps or soccer shoes. This is why stocks in the tech space usually have higher multiples than their peers since they tend to have high growth potential. \n\nA multiple contraction might imply that it is trading at a lower price than it's supposed to, and might signal a potential buying opportunity, but it could also be a value trap. A value trap is where an investor buys a cheaper lower-quality item just because it's cheaper. Sometimes the contraction could be justified, and in other situations it might signal a good buying opportunity. It is therefore important to use stock multiples along with a strong understanding of the fundamentals of the company to decide which of the two we as an investor believe the contraction to be a result of. Multiples are a very handy way to quickly understand a stock's price values. Some investors even contest that the P/E ratio in particular is very limited since EPS can easily be manipulated by accounting decisions and manipulation. \n\n\u003e\"It's far better to buy a wonderful company at a fair price... than a fair company at a wonderful price.\" - Warren Buffet\n\n# References\n1. [Stock Multiples: How to Tell When a Stock is Cheap/Expensive - The Plain Bagel](https://www.youtube.com/watch?v=21STUhQ-iP0)",
    "lastmodified": "2024-05-30T09:58:44.312025492+05:30",
    "tags": []
  },
  "/blog/technical-analysis": {
    "title": "Technical Analysis",
    "content": "# Systematic vs Discretionary Trading\nTrading strategies can be classified into two broad categories:\n## Systematic Trading\nSystematic trading involves adhering to a predefined set of rules or algorithms to execute trades. These rules are usually based on historical data analysis and quantitative modelling. The advantage of these systems is that you can provably verify your hypothesis / system on real-world data using back-testing / forward-testing and obtain quantifiable metrics to evaluate and build trust in your algorithm. It is dependable. \n### Caution\n- It is however **very** important to note that such an algorithmic system is usually NOT one that can self sustain unmonitored. Essentially, having an algorithmic system in place does not mean that the trader employing the strategy can just expect it to work in all markets and leave it unmonitored. Some systems might work very well in bull markets and work very poorly in ranging markets. It is important to identify the right situations to use the right model and have sufficient risk management policies in place to protect against any catastrophic failures. There are very few systems which work on all types of markets. Focus on a portfolio of strategies; no single strategy will save you. Your strategy will stop working at some point. Don't grow attached; it won't feel anything when it loses you money.\n- It is also important to note that even in algorithmic trading, a strategy is only as good as your discipline to follow it. Many algorithmic traders have made large losses due to changing their strategy when it was live before ample testing and not trusting their own back-tested data. You must have belief in your system and not make discretionary choices.\n## Discretionary System\nDiscretionary trading, on the other hand, relies on the trader's judgement, intuition, and subjective interpretation of the market conditions to make trading decisions. Discretionary trading is subject to heavy emotional bias and they only have their own experience, market knowledge and instinct to make trading decisions on. Most retail investors do discretionary trading. This is not advised unless you are extremely experienced in the field and have a lot of experience and intuition to rely on when making trading decisions. A common form of professional discretionary trading is Scalp Trading. The primary disadvantage here being that as a human you are extremely susceptible to your decisions being swayed by news, social media and other such irrelevant factors negatively affecting your ability to making better trading decisions.\n# Trading System\nA system in trading refers to a structured framework of rules that guides the trader in making decisions regarding when to enter and exit positions in the market. These rules are developed through market analysis and optimization to maximize profit potential while minimizing risk. To test whether a system is effective or not we back-test it on old data. Back-testing is the backbone of creating your system. After this, we can also forward test the model on live data to further build confidence. \n\n\u003e Good systems generally have a very small difference between the profit and loss percentage, but that small margin is enough to make big profits because in any good system when you earn, you earn big and when you lose you lose small, so effectively you end up with a pretty high profit. - [Basics of Stock Trading - Honestly by Tanmay Bhat](https://www.youtube.com/playlist?list=PLhKwz7hYMTDVUXV-hkJ2wnwnQECzn-egm)\n\nThe idea here is that we step strict stop losses. This might increase our loss percentage overall but it ensures that we can set a strict upper limit on our max loss even before we enter any trade. We trade some wins for the security of limiting our loss amount in the losing trades. And with a good system we should ensure that the money we gain in winning trades is much higher than the losses realized in losing trades. One trick to handle both greed and risk in winning trades is to incorporate a trailing stop-loss. \n## Trailing Stop-Loss\nThe idea is pretty simple. Let's say our stop loss on some trade is 10% from entry and now let's say the stock has gained 10% profit. We can then dynamically move the stop loss to the entry point. This way we can greedily stick with the trade in hopes of getting more profit while ensuring that the maximum possible loss on the trade is 0%. Similarly if we get to 20% profit we can move the stop loss to 10% and so on. We greedily increase the stop loss by some function while sticking with the trade. \n## How to Make a System\nTo make a system we need to learn technical analysis. Technical analysis is essentially a fancy term for \"finding patterns in charts and price-volume data and using that to predict price movements.\" In theory, technical analysis is bullshit. The stock market ([What is the Stock Market?](/blog/what-is-the-stock-market)) is just a market for companies to request investment from speculative investors and the stock price is supposed to be reflective of the company's earnings and growth in that period. It should theoretically not be possible to obtain this number simply from reading candlestick chart patterns. But the arguments given for technical analysis are as follows:\n- Large fundamental analysis firms, who control most of the stock price movements (since they're the massive institutional investors) do their research well in advance and they are far more accurate than the research an average retail investor can afford to conduct. As a consequence, they are likely to take decisions long before indicators that retail investors use for investment come out. Essentially, the stock price should've already corrected for the negative/positive news or earnings report that might be released to the public later as we can trust these large institutional investors to have done their due diligence well in advance. When an institutional investor makes a large volume trade on the market, it is often broken down into smaller trades over a longer period of time, and it is these patterns that technical analysis hopes to find. Essentially indicators of institutional investors investing and trusting their well-funded research over your own. \n- As someone once said, *\"Technical analysis works because other people believe in technical analysis.\"* If we model the stock market as a game that we are designing an algorithm to win money in, our algorithm which follows some strategy $A$ will always have a counter strategy $A'$ that it will lose a lot of money to. Because there are so many players with large amounts of money playing this \"game\", it is often possible to design a strategy that beats the strategy of other investors and hence, technical analysis is able to make profit. \n# How to Do Technical Analysis? \nTechnical analysis is the study of price movement.\n- **Recognition of Patterns:** Technical analysis operates on the premise that historical price patterns tend to repeat themselves due to recurring human emotions such as greed and fear. Patterns can also indicate institutional investment as mentioned previously. These patterns manifest as identifiable trends in the market. \n- **Emphasis on Price:** Contrary to fundamental analysis, which considers external factors such as news and economic indicators, technical analysis prioritizes price action. The price reflects all available information, making it a reliable indicator of market sentiment and future price movements. Stems from [THE EFFICIENT MARKET HYPOTHESIS](/blog/the-efficient-market-hypothesis). Slightly paradoxical. \n## Trends\n\n   ![Pasted image 20240311214037](/images/pasted-image-20240311214037.png)\n\n   \n- **Uptrend →** Higher lows\n- **Downtrend →** Lower highs\n- **Sideways →** Consistent highs and lows around the same value\n\n**Don’t follow the news, follow the price.** Retailers and institutional investors involved in the market, who do fundamental analysis have done far more research than you could hope to do and have good deductions about the condition of the market say even, 6 months down the line. This is not something that is feasible for us to do. However, based on these deductions, big groups of retailers starts buying/selling stocks and this will be reflected in the price. The news always follows the market. The news finds should've already been reflected and accounted for in the stock price. \n## Candlestick Charts\nThe solid body of the candlestick represents the price range between the open and close of that day’s trading. When the real body is filled in black / red then the close was lower than the open. When it is not filled in / filled in green then the close was higher than the open. The two “wicks” on either end represent the **highest / lowest** price at which the stock was traded for on that day.\n\n![Pasted image 20240311214150](/images/pasted-image-20240311214150.png)\n\n\n### Time-Frames\nChoosing different time-frames can greatly alter the granularity at which data is visualized and can show varying trends. Shorter time-frames, like hourly or even minute-by-minute charts, provide detailed insights into intraday price fluctuations. Conversely, longer time-frames, such as weekly or monthly charts, smooth out noise and highlight broader trends, making them useful for identifying significant market movements and potential long-term patterns. Patterns in larger scale time-frames are often an indication that there is a lot of money behind it. This is interesting because when a lot of money is invested, it is very likely to be backed by big institutional investment. In weekly time-frames, the open is the open of the market on Monday morning and close is the close on Friday end.\n## Todo: Support \u0026 Resistance\n\n## Indicators\n### Moving Average (`Ma`)\nAn $n$ day moving average is a single number used to obtain the average of a financial instrument's price over the last $n$ days. Computing the moving average over several days just be thought of sliding a window containing the terms we're averaging along the time axis. It's essentially a constantly updated *average price*.\n$$\n\\begin{aligned}\n\\text{MA} = \\frac{1}{N} \\sum_{i=0}^{N-1} P_{i} \\\\\n\\end{aligned}\n\\begin{aligned}\n\\quad \\quad \\quad\n\u0026MA \\text{ is the Moving Average} \\\\\n\u0026N \\text{ is the number of time periods used to calculate the } MA \\\\\n\u0026P \\text{ is the representative price of the stock at time period } i \\\\\n\\end{aligned}\n$$\nThe moving average can be thought of as a more smoothed-out representation of all the noise and fluctuations inherent to a stock's price movement at shorter timescales. Also note, the moving average is a *trend-following* or *lagging* indicator. This is because it is based on past prices and is not dependent on future prices. \n### Use Case #1: Identifying Trends\nMoving averages are very useful visual indicators to identify the general trend of stock movements. There's multiple ways you can use the $MA$ to judge trends. \n#### Price vs Ma\nIf the price is consistently and currently above the moving average line, then you can consider it an uptrend and should look for buying opportunities. On the other hand, if the price is consistently below, then it's a downtrend and you should look for exit opportunities. \n\n![Pasted image 20240506185056](/images/pasted-image-20240506185056.png)\n\n\nAs can be seen with the sideways ranging markets, it's important to wait for a consistent showing of price above or below the trend line before categorizing a stock to be in an upward or downward trend. \n#### Using Two Moving Averages\nWe can also use 2 MA's to identify trends. For example, the 20 MA is a short term moving average which gives you the short term trend of the market. And the 200 MA is a long term moving average which gives you the long term trend of the market. If we just used the 20 MA, the few times the price negatively crossed the 20 MA might have been identified as possible sell opportunities. But comparing it with the 200 MA, we can tell that the price is still in an overall uptrend and hence conclude that selling in those short term dip periods is risky. \n\n![Pasted image 20240506190104](/images/pasted-image-20240506190104.png)\n\n\n*\"Using two moving averages will not provide you with better trades. But it will help you avoid bad trades.\"*\n\nWhen working with moving averages, it is very important to understand that moving averages should not be used as a buy or sell signal, but primarily as a confirmatory indicator to ensure that we aren't entering a bad trade against the trend when using trend based strategies. As you might note from the above figures, the moving average strategy works exceedingly terribly in sideways ranging markets and will often flip continuously between short-term buy / sell opportunities. A trader acting on these impulses in a sideways ranging market is likely to make loss. It is hence imperative to note that this strategy works only in **trending** markets. As mentioned before, it is very important to be able to decide when to employ what strategy. \n### Use Case #2: Identifying Trend Reversals\nThis idea is extremely similar to the idea of using two moving averages. Say we use the 50 MA and 200 MA, we always want a smaller and a larger MA. The cross over points of these lines could often signify potential trend reversals in the market. This is one of the most commonly used / beginners 'learn-indicators' introduction to the stock market. In practice, do not place trades at these reversals, but use these signals along with other factors to make better trade decisions. \n\n![Pasted image 20240506191205](/images/pasted-image-20240506191205.png)\n\n\n### Use Case #3: Identifying Dynamic Support and Resistance\nWhen talking about support and resistance, we usually talk about a fixed price level at which we expect the price to reverse and bounce back and forth between. However, these 'fixed' levels are often not fixed, especially in trending markets. \n\n![Pasted image 20240506192308](/images/pasted-image-20240506192308.png)\n\n\nIn these situations it makes a lot of sense to also use the moving average lines as a dynamic moving set of support and resistance points. You can then (in an uptrend) choose to buy close to the MA line. Similarly in a downtrend, you can identify selling opportunities when the stock price is close to the MA line. \n### Conclusion\nIn short, this indicator works relatively well only in trending markets. It is hence imperative to use it only in markets that you can clearly classify as trending. Some pros include that the indicator is very simple and easy to build or incorporate into strategies. There are clear entry OR exit points. Primary disadvantage is requiring to be able to identify a market as being trending or sideways ranging. Further, you also don't have any easy way to identify target take profits using just this indicator. Another possible pro with this indicator is that in practice, you are likely to misclassify several entry / exit points and take several short term minor losses, but the few times when the trend following strategy works, you are likely to enter in long term investments that generate a lot of profit. \n\n\u003e![Pasted image 20240506193412](/images/pasted-image-20240506193412.png)\n\n\u003e- [11. Moving averages - Zerodha Varsity](https://www.youtube.com/watch?v=810jmf7drFw)\n\nThe moving average shines best when trading on less-volatile stocks like the NIFTY and other index / large-cap stocks on large time-frames (weekly / monthly).  \n### Exponential Moving Average (`Ema`)\nThe exponential moving average is similar to a moving average, but works on the idea that recent prices should be given more weight than older prices in the average. For example, a large volume move on a single day (due to either institutional trading or world events) would be better captured in an EMA than a MA. Hence it is more suited to acting as a crossover in the crossover strategy for more volatile stocks or for shorter time frame swing trading. \n$$\n\\begin{aligned}\nEMA_t = \\Big [ P_t \\times \\Big(\\frac{s}{d+1}\\Big) \\Big] + EMA_{t-1} \\times \\Big[ 1 - \\Big(\\frac{s}{d+1}\\Big) \\Big]\\\\\n\\end{aligned}\n\\begin{aligned}\n\\quad \\quad \n\u0026EMA_t \\text{ is the EMA at time period } t \\\\\n\u0026P_t \\text{ is the representative price of the stock at time period } t \\\\\n\u0026s \\text{ is the smoothing factor } \\\\\n\u0026d \\text{ is the number of time periods } \\\\\n\\end{aligned}\n$$\nEssentially, the quantity $\\frac{s}{d+1}$ is the constant factor (or multiplier) used for smoothing and calculating the EMA. If we look at how the terms are expanded, \n$$\n\\begin{aligned}\nEMA_0 = \\alpha P_0 \\\\\nEMA_1 = \\alpha P_1 + (\\alpha P_0) \\times (1 - \\alpha) = \\alpha \\times (P_1 + P_0) - \\alpha^2 P_0 \\\\\nEMA_2 = \\alpha P_2 + (\\alpha \\times (P_1 + P_0) - \\alpha^2 P_0) \\times (1 - \\alpha) = \\alpha \\times (P_2 + P_1 + P_0) - \\alpha^2(2P_0 + P_1) - \\alpha^3 P_0 \\\\\n\\end{aligned}\n$$\nFor a 20-day moving average with $s = 2$, the factor $\\alpha$ would be around $0.0952$. The below diagram represents the difference between using the MA and the EMA to study stock trend movements. \n\n\u003eIn the figure below, the number of periods used in each average is 15, but the EMA responds more quickly to the changing prices than the SMA. The EMA has a higher value when the price is rising than the SMA and it falls faster than the SMA when the price is declining. This responsiveness to price changes is the main reason why some traders prefer to use the EMA over the SMA.\n\u003e\n\u003e![Pasted image 20240507020420](/images/pasted-image-20240507020420.png)\n\n\u003e\n\u003e- [Moving Average (MA): Purpose, Uses, Formula, and Examples](https://www.investopedia.com/terms/m/movingaverage.asp)\n### Conclusion\nIn short, the EMA is a more responsive indicator than the MA. The whole idea with a MA was to smooth out the volatility in daily price changes over larger time-frames. However, when working with more volatile stocks / shorter time periods it might make sense to consult the EMA instead. It is also especially useful to detect trend reversals using the crossover strategy faster. It is one of the most famous known strategies used, similar to the MA crossover.  \n## Moving Average Convergence Divergence (`Macd`)\nThe Moving Average Convergence Divergence (MACD) is a technical analysis tool utilized by traders to gauge the relationship between two exponential moving averages (EMAs) of an asset's price. It helps in identifying potential trend reversals, momentum shifts, and confirming the strength of a trend. The MACD is calculated by subtracting the 26-day EMA from the 12-day EMA: \n  $$\\text{MACD}=\\text{12-day EMA}−\\text{26-day EMA}$$\nThe MACD line is then smoothed by calculating a 9-day EMA, known as the signal line:\n  $$\n  \\text{Signal Line}=\\text{9-day EMA of MACD}\n  $$\nThis signal line assists in identifying potential buy or sell signals and to confirm the strength of a trend. Also, since MACD uses just EMAs, it is also a lagging indicator. \n\n\u003e The MACD lines, however, do not have concrete overbought/oversold levels like the RSI and other oscillator studies. Rather, they function on a relative basis. An investor or trader should focus on the level and direction of the MACD/signal lines compared with preceding price movements in the security at hand, as shown below. \n\u003e \n\u003e ![Pasted image 20240507023344](/images/pasted-image-20240507023344.png)\n\n\u003e \n\u003e MACD measures the relationship between two EMAs, while the RSI measures price change to recent price highs and lows. Both measure momentum in a market, but because they measure different factors, they sometimes give contrary results. The RSI may show a reading above 70 (overbought) for a sustained period, indicating a market is overextended to the buy side of recent prices. In contrast, the MACD indicates that the market is still increasing in buying momentum. Either indicator may signal an upcoming trend change by showing divergence from price (price continues higher while the indicator turns lower, or vice versa).\n\u003e - [What Is MACD? - Investopedia](https://www.investopedia.com/terms/m/macd.asp)\n### Use Case #1: Crossover Strategy\nThe most common use of this indicator it to identify buy / sell signals using the crossover of the MACD and Signal lines. Let's first try to understand what the MACD terms signify. If the MACD value is positive, it implies that the short-term average (12-day EMA) is above the long-term average (26-day EMA). As discussed in the explanations of EMA and MA above, this implies that in the short term, there is an upward momentum in the price and signals a likely uptrend. Similarly, negative MACD values signal likely downward momentum. Now, we can use the MACD line as a sort of indicator of short term price trends. Combining this with the signal line, if we see the 9-day EMA of the MACD crossover with the MACD line, when the MACD is also below the 0-line, it implies that the trend until now was a downtrend (since MACD was below 0), and the crossover with the signal line (which is the EMA of the MACD) implies that there was sudden reversal and upward momentum. Hence it is a very useful indicator to identify swing trading opportunities in the short-term. This is also a very popular strategy.\n\n![Pasted image 20240507022648](/images/pasted-image-20240507022648.png)\n\n\nThe green line on top of the price movement chart is the 50-day MA, the yellow line is the 200-day MA and at the bottom, the blue line is the MACD indicator and the orange line is the signal line. The histogram represents the difference between the signal and the MACD, serving as a measure of strength of the trend. It is also centered on the 0 line. \n### Use Case #2: Confirming Trend Strength \nYou can also use the difference / gap between the MACD line and the signal line as an identification of trend strength. A large gap implies a fast upward momentum push, whereas a smaller gap would imply a weakening trend that is losing steam. However, you should note that often when this happens, it implies that the underlying financial security is probably overbought or oversold and might soon pull back to normal levels. You can combine this analysis with the RSI to verify such conditions. \n### Use Case #3: Macd Divergence\n\u003eWhen MACD forms highs or lows that exceed the corresponding highs and lows on the price, it is called a divergence. A bullish divergence appears when MACD forms two rising lows that correspond with two falling lows on the price. This is a valid bullish signal when the long-term trend is still positive. Some traders will look for bullish divergences even when the long-term trend is negative because they can signal a change in the trend, although this technique is less reliable.\n\u003e\n\u003e![Pasted image 20240507024424](/images/pasted-image-20240507024424.png)\n\n\u003e\n\u003eWhen MACD forms a series of two falling highs that correspond with two rising highs on the price, a bearish divergence has been formed. A bearish divergence that appears during a long-term bearish trend is considered confirmation that the trend is likely to continue. Some traders will watch for bearish divergences during long-term bullish trends because they can signal weakness in the trend. However, it is not as reliable as a bearish divergence during a bearish trend.\n\u003e- [What Is MACD? - Investopedia](https://www.investopedia.com/terms/m/macd.asp)\n### Conclusion\nMACD is a very versatile indicator that attempts to compress all the info you could get from MAs / EMAs into a single, easy to use, user-friendly indicator. This is also one of the reasons why it's very popular among those just getting into trading. Since it relies completely on EMAs, it has the same advantages / disadvantages as the MA strategies. It works very well in trending markets and will definitely help avoid bad trades that rely on trends, however, it will do poorly in ranging sideways markets. MACD is best used with daily periods, where the traditional settings of 26/12/9 days is the default.\n## Relative Strength Index (`Rsi`)\nThe RSI is a *momentum* indicator that was developed by J. Welles Wilder Jr. and introduced in his 1978 book, *New Concepts in Technical Trading Systems*. The RSI was developed as an indicator that is able to judge the strength on days when prices go up to its strength on days when prices go down. It is used in strategies often to check when a stock is *overbought* or *oversold*. \n\n\u003eThe RSI uses a two-part calculation that starts with the following formula:\n\u003e $$\n\\begin{aligned}\nRSI_{\\text{step one}} = 100 - \\Big[ \\frac{100}{1 + \\frac{\\text{Average Gain}}{\\text{Average Loss}}}\\Big]\n\\end{aligned}\n\\quad\n\\begin{aligned}\n\\text{The average gain or loss used in this calculation is the average percentage} \\\\ \\text{gain or loss during a look-back period.} \\\\ \\text{The formula uses a positive value for the average loss.} \\\\ \\text{Periods with price losses are counted as zero in the calculations of average gain.} \\\\ \\text{Periods with price increases are counted as zero in the calculations of average loss.}\n \\end{aligned}\n $$ \n\u003e The standard number of periods used to calculate the initial RSI value is 14. For example, imagine the market closed higher seven out of the past 14 days with an initial average gain of 1%. The remaining seven days all closed lower with an initial average loss of −0.8%. The first calculation for the RSI would look like the following expanded calculation:\n\u003e $$\n RSI_{\\text{step one}} = 100 - \\Big[ \\frac{100}{1 + \\frac{\\frac{1\\%}{14}}{\\frac{0.8\\%}{14}}} \\Big] = 0.55\n $$\n\u003e Once there are 14 periods of data available, the second calculation can be done. Its purpose is to smooth the results so that the RSI only nears 100 or zero in a strongly trending market. \n\u003e $$\n RSI_{\\text{step two}} = 100 - \\Big[ \\frac{100}{1 + \\frac{(\\text{Previous Average Gain} \\times 13) + \\text{Current Gain}}{(\\text{Previous Average Loss} \\times 13) + \\text{Current Loss}}}\\Big]\n $$\n\u003e- [Relative Strength Index (RSI) Indicator Explained With Formula - Investopedia](https://www.investopedia.com/terms/r/rsi.asp)\n\nLet's start by clearing a common misconception. The RSI crossing a value of 70 does imply that it is overbought, but that does not imply that it cannot stay overbought for long periods of time. The below image is a clear example of this:\n\n![Pasted image 20240507055535](/images/pasted-image-20240507055535.png)\n\n\nSimilarly, it can also remain oversold for long periods of time. The RSI is a *momentum indicator*. So if it is overbought, that implies that the momentum is to the upside. It should not be used as a *reversal indicator*. \n### Use Case #1: Trend Strength\nDuring trends, the RSI should frequently stay near the upper band of 70. Similarly during a downtrend, it is likely to frequently hover around 30. For example, if the RSI can’t reach 70 on a number of consecutive price swings during an uptrend, but then drops below 30, the trend has weakened and could be reversing lower.\n### Use Case #2: Rsi Divergence\n\u003eAn RSI divergence occurs when price moves in the opposite direction of the RSI. In other words, a chart might display a change in momentum before a corresponding change in price. A bullish divergence occurs when the RSI displays an oversold reading followed by a higher low that appears with lower lows in the price. This may indicate rising bullish momentum, and a break above oversold territory could be used to trigger a new long position. A bearish divergence occurs when the RSI creates an overbought reading followed by a lower high that appears with higher highs on the price.\n\u003eAs you can see in the following chart, a bullish divergence was identified when the RSI formed higher lows as the price formed lower lows. This was a valid signal, but divergences can be rare when a stock is in a stable long-term trend. Using flexible oversold or overbought readings will help identify more potential signals.\n\u003e\n\u003e![Pasted image 20240507060616](/images/pasted-image-20240507060616.png)\n\n\u003e\n\u003e- [Relative Strength Index (RSI) Indicator Explained With Formula - Investopedia](https://www.investopedia.com/terms/r/rsi.asp)\n### Conclusion\nSimilar to MACD, RSI is also a momentum indicator that works well in trending markets. There is a common misconception among new traders that RSI crossing 70 / 30 mark is a reversal indicator. This is not true, the RSI simply measures the relative strength in gains to the relative strength of losses. In an uptrend for example, you would *expect* the RSI to constantly be very high as the relative gains must be higher than the relative losses in an uptrend. Hence we should instead use it as a confirmation for determining trends. Similar to MACD and EMA, the RSI will also do poorly in ranging markets as we would expect the relative strength of gains and losses to be roughly equal with a few random spikes here and there. \n## Lorentzian Classifier \nA Lorentzian Distance Classifier (LDC) is a Machine Learning classification algorithm capable of categorizing historical data from a multi-dimensional feature space. This indicator demonstrates how Lorentzian Classification can also be used to predict the direction of future price movements when used as the distance metric for a novel implementation of an Approximate Nearest Neighbors (ANN) algorithm.\n\nIn physics, Lorentzian space is well-known for its role in Einstein's General Relativity, describing the curvature of space-time. Interestingly, this abstract concept has practical applications in trading. Recent studies suggest Lorentzian space's suitability for analyzing time-series data. Empirical evidence shows that Lorentzian distance handles outliers and noise better than Euclidean distance and outperforms other distance metrics like Manhattan distance, Bhattacharyya similarity, and Cosine similarity. It consistently yields higher mean accuracy across various time series datasets compared to other metrics, except for Dynamic Time Warping approaches, which are too computationally demanding for current PineScript capabilities.\n  \nEuclidean distance is commonly used as the default distance metric for NN-based search algorithms, but it may not always be the best choice when dealing with financial market data. This is because financial market data can be significantly impacted by proximity to major world events such as FOMC Meetings and Black Swan events. This event-based distortion of market data can be framed as similar to the gravitational warping caused by a massive object on the space-time continuum. For financial markets, the analogous continuum that experiences warping can be referred to as \"price-time\".\n\nBelow is a side-by-side comparison of how neighborhoods of similar historical points appear in three-dimensional Euclidean Space and Lorentzian Space:\n\n![Pasted image 20240507100741](/images/pasted-image-20240507100741.png)\n\n\nThis figure demonstrates how Lorentzian space can better accommodate the warping of price-time since the Lorentzian distance function compresses the Euclidean neighborhood in such a way that the new neighborhood distribution in Lorentzian space tends to cluster around each of the major feature axes in addition to the origin itself. This means that, even though some nearest neighbors will be the same regardless of the distance metric used, Lorentzian space will also allow for the consideration of historical points that would otherwise never be considered with a Euclidean distance metric.  \n  \nIntuitively, the advantage inherent in the Lorentzian distance metric makes sense. For example, it is logical that the price action that occurs in the hours after Chairman Powell finishes delivering a speech would resemble at least some of the previous times when he finished delivering a speech. This may be true regardless of other factors, such as whether or not the market was overbought or oversold at the time or if the macro conditions were more bullish or bearish overall. These historical reference points are extremely valuable for predictive models, yet the Euclidean distance metric would miss these neighbors entirely, often in favor of irrelevant data points from the day before the event. By using Lorentzian distance as a metric, the ML model is instead able to consider the warping of price-time caused by the event and, ultimately, transcend the temporal bias imposed on it by the time series.\n\nSource: [https://www.ig.com/en/trading-strategies/16-candlestick-patterns-every-trader-should-know-180615](https://www.ig.com/en/trading-strategies/16-candlestick-patterns-every-trader-should-know-180615)\n# References\n1. [Basics of Stock Trading - Honestly by Tanmay Bhat](https://www.youtube.com/playlist?list=PLhKwz7hYMTDVUXV-hkJ2wnwnQECzn-egm)\n2. [https://www.ig.com/en/trading-strategies/16-candlestick-patterns-every-trader-should-know-180615](https://www.ig.com/en/trading-strategies/16-candlestick-patterns-every-trader-should-know-180615)\n3. [Relative Strength Index (RSI) Indicator Explained With Formula - Investopedia](https://www.investopedia.com/terms/r/rsi.asp)\n4. [What Is MACD? - Investopedia](https://www.investopedia.com/terms/m/macd.asp)\n5. [Moving Average (MA): Purpose, Uses, Formula, and Examples - Investopedia](https://www.investopedia.com/terms/m/movingaverage.asp)\n6. [11. Moving averages - Zerodha Varsity](https://www.youtube.com/watch?v=810jmf7drFw)",
    "lastmodified": "2024-05-30T09:58:44.315358836+05:30",
    "tags": []
  },
  "/blog/the-black-scholes-merton-equation": {
    "title": "The Black-Scholes-Merton Equation",
    "content": "This single equation spawned multi-trillion dollar industries and transformed everyone's approach to risk.\n$$\n\\frac{\\partial V}{\\partial t} + rS\\frac{\\partial V}{\\partial S} + \\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2V}{\\partial S^2}-rV = 0\n$$\nBut to understand how we arrived here, we need to go back and understand what options are, and understand the evolution of this equation over time.\n# Phase 1 - Louis Bachelier - Théorie De La Spéculation\nLouis Bachelier (born in 1870) stands as a pioneer in the application of mathematics to financial markets, particularly in the realm of option pricing. Both of his parents died when he was 18, and he had to take over his father's wine business. He sold the business a few years later and moved to Paris to study physics, but since he needed a job to support himself and his family financially, he took up a job at the Paris Stock Exchange (the Bourse). This experience, exposed him to the chaotic world of trading. In particular, his interest was drawn to a specific type of financial instrument that was being traded, contracts known as **options**. (Covered in [Derivatives - Options](/blog/derivatives-options))\n\nEven though options had been around for hundreds of years, no one had found a good way to price them. Traders would solely rely on bargaining and feel to come to an agreement about what the price of an option should be. Pricing an 'option' to buy an asset at some fixed strike price in the future was difficult, primarily due to the inherent randomness in stock price movements. Bachelier, who was already interested in probability, thought that there had to be a mathematical solution to this problem, and proposed this as his PhD topic to his advisor Henri Poincaré. Although finance wasn't really something mathematicians looked into back then, Poincaré agreed. It was this doctoral thesis, that would later lay the foundation for applying mathematical pricing models to options trading. \n\nAs mentioned previously, the difficulty in pricing options is primarily due to it being pretty much impossible for any individual to account for a multitude of unpredictable factors responsible for influencing the price of a stock. It's basically determined by a tug of war between buyers and sellers, and the numbers on either side can be influenced by nearly anything from weather, politics, competitors, etc. Bachelier's key insight here was to model stock prices as a random walk, with each movement up or down equally likely.  Randomness is a hallmark of an **efficient market** ([THE EFFICIENT MARKET HYPOTHESIS](/blog/the-efficient-market-hypothesis)). It essentially states that the more people try to make money by predicting stock prices and trading, the less predictable those prices are. The argument is essentially that if you were able to predict that some stock $A$ would go up tomorrow and we buy it, our actions would make the stock price go up today. The very act of predicting essentially influences the stock price. That said, there are plenty of instances throughout history of mathematicians, physicists, etc. finding 'edges' in the stock market ([What is the Stock Market?](/blog/what-is-the-stock-market)) and using them to make consistent profits over long periods of time. The most famous example being Jim Simon's Medallion fund, averaging a $71.8\\%$ annual return (before fees) for almost a decade. \n\nAn important property of random walks is that over time, the expected outcomes of a random walk take up the shape of a normal distribution. \n\n![Pasted image 20240311040835](/images/pasted-image-20240311040835.png)\n\n![Pasted image 20240311040740](/images/pasted-image-20240311040740.png)\n\n\nEssentially, over a short period of time, there's not much influence on the stock price by random-walk steps to allow it to reach extreme deviations from the stock's current price. But over a period of time, the probability of it reaching more extreme prices increases, but the majority of the expected stock price is still close to the stock's current price. This may not be very consistent with our observation of the general trend of the market to increase over a long period of time, but back then, there wasn't a lot of data available and this is how Bachelier modeled it. So after a short time, the stock price could only move up or down a little, but after more time, a wider range of prices is possible. He modeled the expected future price of a stock by a normal distribution, centered on the current price which spreads out over time. \n\n\u003e**Side note**: He realized that he had rediscovered the exact equation which describes how head radiates from regions of high temperature to regions of low temperature, originally discovered by Joseph Fourier in 1822. Thus, he called his discovery the 'radiation of probabilities'.  Bachelier's random walk theory would later find application in solving the longstanding physics mystery of Brownian motion, the erratic movement of microscopic particles observed by botanist Robert Brown. Remarkably, Albert Einstein, in his explanation of Brownian motion in 1905, unknowingly built upon the same random walk principles established by Bachelier years earlier.\n\nBachelier's crowing achievement, was that he had finally figured out a mathematical way to price an option by applying his random walk theory. \n\n![Pasted image 20240311042335](/images/pasted-image-20240311042335.png)\n\n\n- The probability that the option buyer makes profit is the probability that the **stock price increases by more than the price paid for the option**. We call this the **stock price at exercise**. Otherwise the buyer would just let the option expire.  This is the green shaded area.\n\n![Pasted image 20240311042301](/images/pasted-image-20240311042301.png)\n\n\n- The probability that the option seller makes profit is the probability that the **stock price stays low enough that the buyer doesn't earn more than they paid for it**. Note that this is sufficient, because even if the stock price has increased from the strike price, but not by enough to increase past an amount that allows the buyer to exercise the option, the premium payed for by the buyer is enough to give the seller more profit than what would be obtained if he didn't sell the option. This is the red shaded area.\n\nNote that you can influence the region of probabilities simply by changing the premium (price) of the option. Increase the premium, and the stock price required for the option buyer to exercise the option increases. Pushing the probability region where he makes a profit further toward the edges. You can calculate the expected return of buying / selling an option simply by multiplying the profit / loss each individual stands to gain / lose by the probability of each outcome. Note that each probability here is just a function of the price of the option. Bachelier argued that a fair price for an option is what makes the expected return for buyers and sellers equal. \n\n![Pasted image 20240311042939](/images/pasted-image-20240311042939.png)\n\n\n\u003eWhen Bachelier finished his thesis, he had beaten Einstein to inventing the random walk and solved the problem that had eluded options traders for hundreds of years. But no one noticed. The physicists were uninterested and traders weren't ready. The key thing missing was a way to make a ton of money.\n\n## The Bachelier Model\nWhat Bachelier essentially gave us, was a closed form equation for pricing a call / put option under the Bachelier model. The Bachelier model is basically representing a forward price contract (process) as a stochastic differential equation. Here, $\\sigma$ is **volatility**. \n\n$$dF_t = \\sigma dW_t, \\ t \\in [0, T]$$\nYou can think of $[0, T]$ as sort of representing a single time-step. Although this is a continuous process, we can think of it as a discrete process where we're using very small values for the time-step $(T = dt)$. Solving for the forward price process, we get:\n$$ \n\\begin{align}\n\u0026 \\int_0^TdF_t = \\int_0^T\\sigma dW_t \\\\ \\\\\n\u0026 F_t - F_0 = \\sigma(W_t-W_0) \\quad | \\ W_0 \\text{ is 0 by the definition of brownian motion} \\\\ \\\\\n\u0026 F_T = F_0 + \\sigma W_t\n\\end{align}\n$$\nAnd that's it. An elegant way to model the future price and derive the closed form for pricing options. More generally, we can write the above result as $F_{t+1} = F_t + \\sigma W_t$. We can even prove that $F_t$ is a **martingale**. That is:\n$$\n\\mathbb{E}[F_{t+1}|F_t] = F_t\n$$\nIt's essentially saying that the forward price process at some point in the future is expected to be $F_t$. Our best guess for the next step in the process, is just the latest point computed in the process. Proof: \n$$\n\\mathbb{E}[F_{t+1}|F_t] = \\mathbb{E}[F_t + \\sigma W_{t+1}|F_t] = \\mathbb{E}[F_t + \\sigma W_{t+1}] = F_t\n$$\n### Pricing a Call Option\nWe are going to be pricing European style options, that is, we will be considering the payoff at **maturity**, at time $T$. We don't know what the future holds for the derivative, but we know what the value of that derivative **could be** at some time $T$ in the future. Essentially, based on the price of the underlying asset that the derivative is tracking at expiration, we know that the payoff is going to take the shape of a hockey-stick figure as shown previously. A call option at time $T$, will give us:\n$$\n\\begin{align*}\n\u0026 K \\text{ - Strike Price} \\\\\n\u0026 T \\text{ - Time to Maturity} \\\\\n\u0026 C_T = max((F_t-K), 0)=(F_T - K)^+\n\\end{align*}\n$$\nWe use the $(\\cdots)^+$ notation just to simplify the expression. At time $T$, this is a deterministic expression to how much payoff we make. But the issue is we do not know what $F_T$ will be. So the best thing to do today would be to compute the expectation of that payoff and hope to derive a closed form equation to compute the call price. The call price today is given by the expectation of the future:\n$$\n\\begin{align*}\n\u0026 C_0 = \\mathbb{E}[(F_T - K)^+] \\\\\n\u0026 = \\mathbb{E}[(F_0 + \\sigma W_T - K)^*]\n\\end{align*}\n$$\nNow, $W_T$ is still an increment in Brownian motion, that is, it is **distributed normally** with a mean of 0 and a variance of $dt$. Note $dt = T$. And since variance is equivalent to the square of the standard deviation, we can write the equation as:\n$$\n= \\mathbb{E}[(F_0 - \\sigma \\sqrt{(T - 0)}Z - K)^+]\n$$\nWhere $Z \\sim N(0, 1)$, $Z$ is a **standard normal random variable**. Essentially, we use the fact that we have independent stationary increments with mean 0 and variance $dt$ to substitute for $W_T$. Let's rearrange some terms to get:\n$$\n= \\mathbb{E}[(F_0 - K - \\sigma\\sqrt{T}Z)^+]\n$$\nWe want some more algebraic / better mathematical tools to substitute for the $max$ function. We will use indicators to make this equation easier to solve. Recall that:\n$$\n\\mathbb{1}(x)  = \\begin{cases}\n1 \u0026 \\text{condition of } x\\\\\n0 \u0026 \\sim \\text{condition of } x\\\\\n\\end{cases}\n$$\nThe $max$ function in this context essentially just implies that when exercising an option, if there is positive payoff, take it, otherwise don't take it (let it expire). And the indicator function let's us imply the same thing in the equation. So we can substitute the indicator function in for the $max$ function be defining our indicator $\\mathbb{1}$ as follows:\n$$\n\\mathbb{1}(Z) = \\begin{cases}\n1 \u0026 Z \\leq \\frac{F_0 - K}{\\sigma \\sqrt T} \\\\\n0 \u0026 Z \\gt \\frac{F_0 - K}{\\sigma \\sqrt T} \\\\\n\\end{cases}\n$$\nSubstituting this in:\n$$\n= \\mathbb{E}[((F_0 - K - \\sigma\\sqrt TZ))\\mathbb{1}_{Z \\leq\\frac{F_0-K}{\\sigma\\sqrt T}}]\n$$\nDistributing the indicator function yields:\n$$\n= \\mathbb{E}[(F_0 - K)\\mathbb{1}_{Z \\leq\\frac{F_0-K}{\\sigma\\sqrt T}} - \\sigma \\sqrt TZ\\mathbb{1}_{Z \\leq\\frac{F_0-K}{\\sigma\\sqrt T}}]\n$$\nNow, since we know that $Z$ is distributed standard normally, the expectation that $Z$ is less than some quantity can be found by using the cumulative distribution function for the normal distribution. Essentially, the first term indicator function can be replaced by just substituting it with the normal cumulative distribution, $\\Phi$, up to the indicator function value:\n$$\n= (F_0 - K) \\Phi(\\frac{F_0 - K}{\\sigma \\sqrt T}) - \\sigma \\sqrt T \\mathbb{E}[Z\\mathbb{1}_{Z \\leq \\frac{F_0 - K}{\\sigma \\sqrt T}}]\n$$\nUsing properties of normal distributions, the derivative of the CDF $\\Phi'(x) = -x\\phi(x)$, where $\\phi$ is the probability density function of the normal distribution. \n$$\n\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}\n$$\nWe can use this property to solve the second term since:\n$$\n\\mathbb{E}[Z\\mathbb{1}_{Z \\leq y}] = \\int_{-\\infty}^y x\\phi(x)dx = -\\phi(y)\n$$\nApplying this to the original equation by letting $y = \\frac{F_0 - K}{\\sigma \\sqrt T}$, we get:\n$$\nC_0 = (F_0 - K)\\Phi(\\frac{F_0 - K}{\\sigma \\sqrt T}) + \\sigma\\sqrt T\\phi(\\frac{F_0 - K}{\\sigma \\sqrt T})\n$$\nA closed form equation for pricing a call option given the current asset price $F_0$, the strike price $K$, the volatility $\\sigma$ and the time to maturity $T$ of the option!\n\nWe can similarly use the Bachelier model to price all other kinds of future contracts, including put options, call / put futures, etc. \n\n# Phase 1.5 - Brownian Motion $B_t$ (Wiener Process)\n\n\u003eSo Brown discovered that any particles, if they were small enough, exhibited this random movement, which came to be known as Brownian motion. But what caused it remained a mystery. 80 years later in 1905, Einstein figured out the answer. Over the previous couple hundred years, the idea that gases and liquids were made up of molecules became more and more popular. But not everyone was convinced that molecules were real in a physical sense. Just that the theory explained a lot of observations. The idea led Einstein to hypothesize that Brownian motion is caused by the trillions of molecules hitting the particle from every direction, every instant. Occasionally, more will hit from one side than the other, and the particle will momentarily jump. To derive the mathematics, Einstein supposed that as an observer we can't see or predict these collisions with any certainty. So at any time we have to assume that the particle is just as likely to move in one direction as an another. So just like stock prices, microscopic particles move like a ball falling down a galton board, the expected location of a particle is described by a normal distribution, which broadens with time. It's why even in completely still water, microscopic particles spread out. This is diffusion. By solving the Brownian motion mystery. Einstein had found definitive evidence that atoms and molecules exist. Of course, he had no idea that Bachelier had uncovered the random walk five years earlier. - [The Trillion Dollar Equation](https://www.youtube.com/watch?v=A5w-dEgIU1M\u0026t=148s)\n\nThe random walk that Bachelier came up with and the Brownian motion that Robert Brown discovered are both pretty similar and following the developments that occurred in mathematically developing Brownian motion will help us understand more complex future contracts pricing models. \n\n\u003e **Definition**: A *standard (one-dimensional)*  *Brownian Motion* (also called *Wiener Process*) is a stochastic process $\\{W_t\\}_{t \\geq 0+}$ indexed by non-negative real numbers $t$ with the following properties:\n\u003e1. $W_0 = 0$.\n\u003e2. With probability 1, the function $t \\to W_t$ is continuous in $t$. \n\u003e3. The process $\\{W_t\\}_{t \\geq 0+}$ has *stationary, independent increments*.\n\u003e4. The increment $W_{t+s} - W_s$ has the $\\text{NORMAL}(0, t)$ distribution\n\u003e   - [BROWNIAN MOTION - Galton UChicago](https://galton.uchicago.edu/~lalley/Courses/313/BrownianMotionCurrent.pdf)\n\nI'll explain these properties in more details below. Let's call them the axioms that govern all Wiener processes / Brownian motion. \n## Axioms\n1. **Brownian Motion has independent increments.** \n\tSay we have a time value $r$, $s$ and $t$. We have some Brownian motion associated with each of these time values. The time from $s \\to t$ is an increment. So is the time from $r \\to s$. We're essentially saying that the increment from $s \\to t$ is **totally** independent of other time periods, not even the previous $r \\to s$ time period. In short, this axiom essentially says that whatever happens in any given time period is **totally random** and does not depend on what happens in any other time period. \n\t\n\t![Pasted image 20240312084016](/images/pasted-image-20240312084016.png)\n\n2. **Brownian Motion has stationary increments.**\n\tIt's sort of related to the previous axiom. But what it essentially says that the distirbution in the time between $s \\to t$ only depends on the time values $t$ and $s$ and nothing else. \n3. **Brownian Motion has Normal Distribution.** \n   If we look at the distribution in any time-step, the data points will be normally distributed. That is:\n   $$\n\tB_t - B_s \\sim N(\\mu(t - s), \\sigma^2(t-s))\n\t$$\n\tHere, the term $\\mu (t-s)$ is the **mean** of the normal distribution. This term is also often called **drift**. The $\\sigma^2(t-s)$ term is the **variance** of the normal distribution. $\\sigma$ is just the standard deviation.\n4. **Brownian Motion has continuous sample paths**.\n\tThis simply just means that at any time value, the Brownian motion graph is continuous at all points. \n### Standard Brownian Motion\nStandard Brownian Motion is a specialized case of Brownian Motion. It is the case that Bachelier studied and used to model future stock prices in his PhD Thesis. Here, Brownian motion has a **standard normal distribution**. A standard normal distribution has mean $(\\mu) = 0$ and variance $\\sigma^2 = 1$.  \n$$\nB_t - B_s \\sim N(0, t-s)\n$$\n## Random Walks\nA **symmetric** random walk is a mathematical model that describes a path consisting of a series of random steps, where each step has an equal probability of being taken in either direction. We will limit our discussion to **symmetric** random walks. Here symmetric just means that the probability of each step being chosen is equal. \n\nLet $S_n$ denote the position of the walker after $n$ steps. Then, a symmetric random walk can be defined recursively as: $$ X_n = X_{n-1} + Z_n$$\nHere, $Z_n$ are independent and identically distributed random variables taking values $+1$ or $-1$ with equal probability, i.e., $P(Z_n = 1) = P(Z_n = -1) = \\frac{1}{2}$.\n\n![Pasted image 20240312091108](/images/pasted-image-20240312091108.png)\n\n- [Eight different random walks - Wikipedia](https://en.m.wikipedia.org/wiki/File:Random_Walk_example.svg)\n\nEffectively, when we consider the discrete case, we call it a random walk. But as we keep reducing our time-steps, that is, $\\Delta t \\to 0$, it's the same as Brownian motion. The summation formula is the mean by definition, so we can write $Z_k = \\pm\\frac{t}{n}$, where $n$ is the number of time steps. For convenience, let us write $Z_k = \\pm \\sqrt \\frac{t}{n}$. \n### Expectation\nThe expectation of $Z_k$, $\\mathbb{E}[Z_k] = 0 \\iff \\mathbb{E}[X_n] = 0$. The expectation, $\\mathbb{E}[Z_k^2] = \\frac{t}{n}$. Now when working with expected values, due to [LINEARITY OF EXPECTATION](/blog/linearity-of-expectation), $\\mathbb{E}[Z_i Z_j] = \\mathbb{E}[Z_i] \\cdot \\mathbb{E}[Z_j] = 0$ . \n$$\n\\begin{align}\n\u0026 \\mathbb{E}[X_n^2] = \\mathbb{E}[(\\sum Z_k)^2] \\\\\n\u0026 = \\mathbb{E}[(Z_1 + Z_2 + \\cdots + Z_n)(Z_1 + Z_2 + \\cdots + Z_n)] \\\\\n\u0026 = \\mathbb{E}[Z_1^2 + Z_1Z_2 + \\cdots + Z_1Z_n + Z_2Z_1 + \\cdots + Z_2Z_n + \\cdots + Z_nZ_1 + Z_nZ_2+\\cdots+Z_n^2] \\ | \\text{Since } \\mathbb{E}[Z_iZ_j] = 0 \\text{ for } i \\neq j \\\\\n\u0026 = \\mathbb{E}[\\sum Z_k^2] \\\\\n\u0026 = \\mathbb{E}[Z_1^2] + \\mathbb{E}[Z_2^2] + \\cdots + \\mathbb{E}[Z_n^2] \\\\ \n\u0026 = \\frac{t}{n} + \\frac{t}{n} + \\cdots + \\frac{t}{n} \\\\ \n\u0026 \\implies \\mathbb{E}[X_n^2] =  t\n\\end{align}\n$$\nThe important property here is that this expectation is completely independent of $n$. No matter how many time steps we take, the expectation is just $t$. To go from the discrete case to the continuous case, we can indicate the size of time-steps going to 0 as $n \\to \\infty$. Because $\\mathbb{E}[X_n] = 0$ and $\\mathbb{E}[X_n^2] = t$ (both are independent of $n$), we know that the exact same expectations apply to the Brownian Motion case as well. \nAs $n \\to \\infty$, our random walk becomes Brownian Motion. Therefore, we get:\n$$\n\\begin{align}\n\u0026 \\mathbb{E}[B_t] = 0 \\\\\n\u0026 \\mathbb{E}[B_t^2] = t\n\\end{align}\n$$\n- **Brownian Motion is bounded**. This just says that, in the context of share prices, a share price cannot go to $\\infty$. \n- **Brownian Motion is a Markov process**. This follows from the definition. \n- **Brownian Motion is a Martingale**. This is sort of like saying, *the best guess for what happens next (in the future), is what's happening now*. More formally, $\\mathbb{E}[Z_{t+1}|Z_t] = Z_t$. Kind of paradoxical. \n## Geometric Brownian Motion\nRemember that in Bachelier's Thesis, he modeled share prices using a standard normal distribution. But looking at share prices almost immediately indicates an issue with his model. We notice that over time, stocks tend to *drift* in one direction or the other, with total markets having an overall upwards drift. This is sort of like having the normal distribution have it's mean drifted up from 0. This is the idea that we want to model using geometric Brownian motion. \n\nWe sort of expect share prices to grow in an exponential manner. We mathematically write this as $S_t = S_0e^{\\alpha t}$. Just the formula to denote standard exponential growth. But we know that share prices follow Brownian motion (random walk), and the price keeps constantly fluctuating. Effectively, we need to introduce a parameter in this equation to account for the Brownian motion. So to take this into account, we can do this by modifying the model slightly to $S_t = S_0 e^{\\alpha t + \\beta B_t}$. The term $\\beta B_t$ accounts for the Brownian motion. $\\beta$ is a constant, which is very difficult to measure for a stock. The term is essentially supposed to be a measure of volatility. You can see that with higher $\\beta$, you have more contribution from the Brownian motion term and hence have more random volatility. \n\n![Pasted image 20240312094517](/images/pasted-image-20240312094517.png)\n\n\nIf we play around with the formula a bit, we can do the following:\n$$\n\\begin{align}\n\u0026 \\frac{S_t}{S_0} = e^{\\alpha t + \\beta B_t} \\\\\n\u0026 \\ln(\\frac{S_t}{S_0}) = \\alpha t + \\beta B_t \\quad \\text{You can think of the } \\alpha t \\text{ term as contributing to the mean and } \\beta B_t \\text{ as a normal distribution with mean } 0\\\\\n\u0026 \\text{Since, } B_t \\sim N(0, t) \\\\\n\u0026 \\alpha t + \\beta B_t \\text{ is normally distributed, but we want to know it's mean and variance} \\\\\n\u0026 \\alpha t + \\beta B_t \\sim N(\\alpha t, \\beta^2t) \\quad | \\text{ Since } Var(x) = a \\implies Var(kx) = k^2a\\\\\n\u0026 \\implies \\ln(\\frac{S_t}{S_0}) \\sim N(\\alpha t, \\beta^2 t)\n\\end{align}\n$$\nThis is what is known as log-normal. In other words, the ratio of the share prices at time $t$ to the share price at the beginning is a log-normal distribution. The log part essentially just skews the curve. \n![Pasted image 20240312095342](/images/pasted-image-20240312095342.png)\n\n- [Log-Normal Distribution: Definition, Uses, and How To Calculate - Investopedia](https://www.investopedia.com/terms/l/log-normal-distribution.asp)\n# Phase 2 - The Black-Scholes-Merton Equation\nThorpe wasn't satisfied with Bachelier's model for pricing options. For one thing, stock prices aren't entirely random. They can increase over time if the business is doing well or fall if it isn't. Bachelier's model ignores this. So Thorpe came up with a more accurate model for pricing options, which took this drift into account. He used his model to gain an edge in the market and make a lot of money. Black-Scholes and Merton later independently came up with a way to price future contracts that would then revolutionize the trading industry forever. Their equation, like Thorpe's, was an improved version of Bachelier's model. \n\n## Dynamic Hedging\n### A Toy Example\nLet's say Bharat sells Arya a call option on a stock, and let's say the stock price has gone up. So it's now in the money for Arya. For every additional rupee that the stock price goes up from the strike price, Bharat will now lose a rupee. **BUT**, he can eliminate this risk by owning 1 unit of stock. He would lose 1 rupee from the option, but gain that rupee back from the stock. And if the stock drops below the strike price, making the option go out of the money for Arya, he can just sell the stock at the strike price so he doesn't risk losing any money from that either. This is the idea behind dynamic hedging.\n\n### A Hedged Portfolio\nA hedged portfolio, at any one time, will offset an option $V$ with some amount ($\\Delta$) of stock $S$.  Let $\\Pi$ represent the portfolio, we have $\\Pi = V - \\Delta S$. It basically means you can sell something without taking the opposite side of the trade. You have a no-risk trade you could make profit from. However, this isn't very practical because the amount of stock to hold $\\Delta$, changes based on current stock prices.\n\n## Deriving Black-Scholes-Merton\nWe're essentially constructing a portfolio of a single option $V$, and a certain number of shares $\\Delta$ of $S$ that we're going to sell against the option to dynamically hedge against it. So the value of our portfolio is essentially $\\Pi = V(S, t) - \\Delta S$. We're interested in tracking the time evolution of our portfolio. This is difficult because again, the future cash-flow of our option is not easy to price in. So we use the principles from Brownian motion to essentially model the underlying asset as a stochastic process that follows geometric Brownian motion.  \n$$\n\\begin{align}\n\u0026 \\Pi = V(S, t) - \\Delta S \\\\\n\u0026 d\\Pi = dV - \\Delta dS \\quad | \\ \\text{Modelling } dS \\text{ using Geometric Brownian Motion,}\\\\\n\u0026 dS = \\mu Sdt + \\sigma S dW \\quad | \\ \\text{The first term accounts for drift. The second term accounts for volatility.}\\\\\n\u0026 \\text{You can then apply Ito's Lemma to get: } \\\\\n\u0026 dV = \\frac{\\partial V}{\\partial t}dt + \\frac{\\partial V}{\\partial S}dS + \\frac{1}{2}\\frac{\\partial^2 V}{\\partial S^2}dS^2 \\\\\n\u0026 dV = \\frac{\\partial V}{\\partial t}dt + \\frac{\\partial V}{\\partial S}dS + \\frac{1}{2} \\sigma^2S^2\\frac{\\partial^2V}{\\partial S^2}dt \\\\ \n\u0026 \\text{Substituting this back into the original portfolio formula,} \\\\\n\u0026 d\\Pi = (\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2V}{\\partial S^2})dt + (\\frac{\\partial V}{\\partial S} - \\Delta) dS \\\\\n\u0026 \\text{If we take } \\Delta = \\frac{\\partial V}{\\partial S} \\text{ as the hedge,} \\\\\n\u0026 d\\Pi = (\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2V}{\\partial S^2})dt \\\\ \n\\end{align}\n$$\nOur portfolio is now just a  $dt$ term which means that the portfolio is now deterministic, and as such, doesn't carry any risk. A risk-free portfolio should yield a risk-free rate ($r$), which let's us write a different equation for $d\\Pi$. \n$$\n\\begin{align}\n\u0026 d\\Pi = r\\Pi dt = (V - rS\\frac{\\partial V}{\\partial S})dt \\\\\n\u0026 \\text{By equating this to our previous formula, and re-grouping terms, we get the famous equation:} \\\\\n\u0026 \\frac{\\partial V}{\\partial t} + rS\\frac{\\partial V}{\\partial S} + \\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2V}{\\partial S^2}-rV = 0\n\\end{align}\n$$\nThe risk-free rate in the Black-Scholes formula represents the **theoretical return on an investment with no risk of default**. For example, government-bonds. \n\nWe can now set $V$ equal to a call option or a put option and then solve the differential equation to get a closed-form equation for the price of a call-option given:\n$$\n\\begin{align}\n\u0026 C = \\text{call option price} \\\\\n\u0026 N = \\text{cumulative distribution function of the normal distribution} \\\\\n\u0026 S_t = \\text{spot price of an asset} \\\\\n\u0026 K = \\text{strike price} \\\\\n\u0026 r = \\text{risk-free rate} \\\\\n\u0026 t = \\text{time to maturity} \\\\\n\u0026 \\sigma = \\text{volatility of asset} \\\\\n\u0026 \\\\\n\u0026 C = N(d_1)S_t - N(d_2)Ke^{-rt} \\\\ \n\u0026 \\text{where } d_1 = \\frac{\\ln(\\frac{S_t}{K}) + (r + \\frac{\\sigma^2}{2})t}{\\sigma\\sqrt t} \\\\\n\u0026 \\text{and } d_2 = d_1 - \\sigma\\sqrt t\n\\end{align}\n$$\n\n\n# References\n1. [The Trillion Dollar Equation](https://www.youtube.com/watch?v=A5w-dEgIU1M\u0026t=148s)\n2. [Bachelier Model Call Option Price Derivation](https://www.youtube.com/watch?v=J1sBj9K-BhE)\n3. [BROWNIAN MOTION - Galton UChicago](https://galton.uchicago.edu/~lalley/Courses/313/BrownianMotionCurrent.pdf)\n4. [Geometric Brownian Motion](https://www.youtube.com/watch?v=sIKD1tQryHg\u0026list=PLg5nrpKdkk2BZoZYAg2d6Ma3HZ5p9h35i\u0026index=5)\n5. [The Easiest Way to Derive the Black-Scholes Model](https://www.youtube.com/watch?v=NHvQ5CSSgw0\u0026t=70s)\n",
    "lastmodified": "2024-05-30T09:58:44.322025523+05:30",
    "tags": []
  },
  "/blog/the-economics-of-foreign-exchange-forex": {
    "title": "The Economics of Foreign Exchange (Forex)",
    "content": "# History\n## Globalization\nToday, we live in an increasingly globalized word. The usual growth path for a business today always involves adding more dependencies on global supply-chains to ensure that their products and services are produced in the best country possible. To boost profits, you might want to cut manufacturing costs, and where else to get cheap components and parts from than China? Or maybe you want high quality parts from Germany to better satisfy your customers. Regardless, internationalizing supply chains is a very important near-compulsory step for every business to increase their growth. But this wasn't the case 100 years ago. \"Imports\" didn't exist. You either built the entire product locally or you couldn't. Trade was minimal and owning foreign products was a luxury. Maybe it's one of the reasons why the words \"foreign goods\" still commands so much respect today. \n## Comparative Advantage\nLet's explain this concept through a simple case study of 2 countries, Germany and China. Germany is well-known for being a supplier of high quality, efficient and luxury parts. China on the other hand is well-known for being able to mass-produce cheap goods. \n### Germany \nLet's say we asked Germany to produce iPhones. They could do it pretty easily because they already have all the infrastructure and technological expertise required to mass-produce them from their individual components without much difficulty. However, Germany has very expensive, limited labor and their government also has very stringent rules regarding industrial waste disposal. Add to this, it's not geographically close to the mass-suppliers of the components used in iPhones. According to educated estimates, iPhones built in Germany might cost as much as $\\approx 4,000\\$$. \n### China\nNow let's ask China to build luxury sports cars. They could probably do it. China mass produces a lot of EVs, but their manufacturing is a lot more targeted to more medium-quality mass manufacturing. China's labor force is much larger than Germany and is also comparatively lower-skilled. Their government also has much looser environmental regulations than Germany. A lot of the major NAND and other similar component suppliers are also in or near China. This makes it extremely well suited to mass manufacturing iPhones, but not luxury cars. \n### Symbiosis\nGermany _could_ allocate some resources to building overpriced iPhones and China _could_ allocate resourced to build *somewhat decent* luxury cars, but this requires a lot of research, exploration and budget to be allocated to these projects. But instead of doing that, they could just agree to focus on their strengths. Germany makes cars, and China makes phones. Now to ensure their citizens can enjoy both cheap phones and good quality cars, Germany over-produces and exports its cars to China, and China does the same with iPhones for Germany. Both countries get the best of both worlds. \ndd ad\nThis is the because each country had a **comparative advantage** for the product they specialize it.  However, note that this is an extremely simplified example. We have evolved past the barter system, trade is not done by putting up 100 iPhones for a car, but through currency. However, German manufacturers don't want Yuan and Chinese manufacturers don't want Euros. Hence we trade *currencies*. This makes a secondary market, for Euros and Yen. The foreign exchange is created because global trade requires it. ([What is the Stock Market?](/blog/what-is-the-stock-market))\n## Bretton Woods Conference\nThe Bretton Woods conference, also known as the United Nations Monetary and Financial Conference, was a landmark international gathering held in July 1944 in Bretton Woods, USA. The conference was attended by 730 delegates from 44 representative countries. Countries wanted to establish multilateral economic cooperation to avoid the complications and painful situations faced during World War 2, the [Great Depression](/blog/great-depression) and the trade wars that spread the depression globally. \n\n\u003eThe seminal idea behind the Bretton Woods Conference was the notion of [open markets](https://en.wikipedia.org/wiki/Free_market \"Free market\"). In his closing remarks at the conference, its president, U.S. Treasury Secretary [Henry Morgenthau](https://en.wikipedia.org/wiki/Henry_Morgenthau_Jr. \"Henry Morgenthau Jr.\"), stated that the establishment of the IMF and the IBRD marked the end of [economic nationalism](https://en.wikipedia.org/wiki/Economic_nationalism \"Economic nationalism\"). This meant countries would maintain their national interest, but trade blocs and economic spheres of influence would no longer be their means. - [Bretton Woods Conference - Wikipedia](https://en.wikipedia.org/wiki/Bretton_Woods_Conference)\n\nTwo notable results of this conference include the establishment of the **International Monetary Fund (IMF)** and the **International Bank for Reconstruction and Development (IBRD)** which was later renamed to something you might recognize today, the **World Bank**. The other important result, was the establishment of an adjust-ably pegged foreign exchange market rate system, where exchange rates were pegged to gold. Governments were only allowed to alter the rates by at most $10\\%$ without the involvement of the IMF and only to correct a \"fundamental disequilibrium.\"\n\nThe system established a gold price of 35$ per ounce and participating currencies pegged their currency to the dollar. There was no \"market\" for currencies, just an somewhat adjustable fixed rate for currency conversion irrespective of supply and demand. This is simple, but completely disregards the fluctuating nature of supply and demand that economics tries to model \u0026 understand. \n\n\u003e For anybody that that knows anything about economics, you will know, that fighting the forced of supply and demand is like fighting the flow of water on a riverbank. Eventually, the continuous forces are going to win out. - [Economics Explained - YouTube](https://www.youtube.com/watch?v=ig_EO805rpA)\n\n# The End of Bretton Woods, Today's Forex Market\n\n\u003eThe first large crack in the system appeared in 1967, with a run on gold and an attack on the British Pound that lead to a 14.3% devaluation in the currency despite the efforts by the government to pull it back in line with it's predetermined level. Finally, president Nixon took the United States off the gold standard in 1971 (temporarily). And by late 1973, the system had collapsed and participating countries were allowed to float their currencies freely. This meant that currencies were traded on open markets just like shares or oil or beanie babies. The growth of global trade and the liberation of currencies to live their life at whatever value the market decided for them is why the market is so influential today - [Economics Explained - YouTube](https://www.youtube.com/watch?v=ig_EO805rpA)\n\n# Foreign Exchange Players\n- **General public:** Makes up a very tiny percentage of market transactions. Primarily from holiday travel currency conversions and retail shopping on sites which do not support automatic currency conversions. \n- **Institutional Investors:** Companies, Investment firms and Governments are big players in this market. Imagine company $A$ is based in country $a$ and wants to expand to country $b$, which uses $b'$ currency. Company $A$ now has to use its $a'$ currency revenue from country $a$ to buy up currency $b'$ through the exchange market and then use it as investment in country $b$ to expand there. However, remember that exchange rates are always fluctuating. Let's say company $A$ makes $50\\%$ profit in country $b$, but $b'$ has fallen in value harshly, completely negating the profit the business made in country $b$. This is a lot of risk that companies don't want to take. They would be willing to take risk in betting that their products and services would be a hit in country $b$, but they wouldn't want to deal with fluctuations in the exchange market which is outside their area of expertise. This is where other investment firms and speculative investors can help these businesses out by taking bets and helping the company by trading using [Derivatives - Options](/blog/derivatives-options), which give peace of mind to the company and risk-reward potential for the speculative investors. Similarly governments can buy up chunks of foreign currency to influence the market, play politics, etc. \n# References\n1. [Economics Explained - YouTube](https://www.youtube.com/watch?v=ig_EO805rpA)\n2. [Bretton Woods Conference - Wikipedia](https://en.wikipedia.org/wiki/Bretton_Woods_Conference)",
    "lastmodified": "2024-05-30T09:58:44.325358867+05:30",
    "tags": []
  },
  "/blog/the-fast-fourier-transform-fft": {
    "title": "The Fast Fourier Transform (FFT)",
    "content": "# FFT (Fast Fourier Transform)\n\n**The problem:** _Given two d-degree polynomials, compute their product_\n\nLet $A(x) = a_0 + a_1x + ... + a_dx^d \\ \\text{and} \\ B(x) = b_0 + b_1+...+b_dx^d$\n\nThen,\n\n$C(x) = A(x)\\times B(x) = c_0 + c_1x+...+ c_{2d}x^{2d}$ has coefficients $c_k = a_ob_k+a_1b_{k-1}+...+a_kb_0 = \\sum_{i=0}^ka_ib_{k-i}$\n\nThe naïve solution here would be to compute in $O(d^2)$ steps. There are $2d$ terms in the final expression and each of these terms requires order $O(d)$ multiplications to compute. The question is, _can we do better?_\n\nDivide and conquer is an approach that works well when we are able to introduce/identify some sort of overlap in subproblems. But for each coefficient, the multiplication terms **do not** have much overlap. Perhaps a different view is of order.\n\nThe **co-efficient** representation of polynomials is essentially an equation that can _uniquely_ identify some function on a graph. There are definitely other representations that will allow us to do the same.\n\nThe one we will be looking at today is the **value** representation of a function. Consider any function defined by some $d$ degree polynomial. Notice that such a function can always be _uniquely_ identified by any set of $d+1$ points that satisfy the equation (are on its graph).\n\n**Proof:** Say we have a $d$ degree polynomial $P$ and we evaluate it at $d + 1$ unique points. We end up with the set of points $\\{ (x_0, P(x_0)), (x_1, P(x_1), \\dots, (x_d, P(x_d) \\}$.\n\nIf $P(x) = p_dx^d + p_{d-1}x^{d-1}+\\dots+p_2x^2+p_1x^1+p_0$\n\nNotice that there are $d+1$ coefficients for each such $P(x)$. Writing our equation in matrix form,\n\n$$ \\begin{bmatrix} P(x_0) \\\\ P(x_1) \\\\ \\vdots \\\\ P(x_d) \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 x_0 \u0026 x_0^2 \u0026 \\dots \u0026 x_0^d \\\\ 1 \u0026 x_1 \u0026 x_1^2 \u0026 \\dots \u0026 x_1^d \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 1 \u0026 x_d \u0026 x_d^2 \u0026 \\dots \u0026 x_d^d \\end{bmatrix} \\begin{bmatrix} p_0 \\\\ p_1 \\\\ \\vdots \\\\ p_d \\end{bmatrix} $$\n\nNotice that there are $d+1$ variables and $d+1$ equations. If we had any lesser we would not be able to uniquely solve for this system. Hence we need at least $d+1$ points. Another way to visualize this is that our matrix of equations is invertible for unique points $x_0, x_1, \\dots, x_d$. This can be proved by solving for the determinant. This implies that we have a unique set of coefficients with which we can identify the polynomial.\n\nBut _why_ do we care about the value representation of a polynomial?\n\n## Value representation: The good and the bad\n\nNotice that if we have some polynomial $C(x)$ which is the result of multiplication of two $d$ degree polynomials $A(x)$ and $B(x)$, the degree of polynomial $C(x)$ must be $2d$. This means that it can be _uniquely_ identified by just $2d+1$ points.\n\nNow, for some point $x_0$, $C(x_0) = A(x_0)\\times B(x_0)$.\n\nThis means that, if we could pick and evaluate polynomials $A(x)$ and $B(x)$ at $2d+1$ points, we can generate $2d+1$ points to _uniquely_ identify $C(x_0)$ with in **linear time**.\n\nHowever, this is assuming that converting the polynomial from coefficient form to value form and back takes lesser than equal to $O(n)$. This is **not** true. We must evaluate a polynomial with $d$ terms at $2d+1$ points. This calculation is of the order $O(d^2)$ and hence, no better than the naïve method. This is where the idea of **FFTs** comes in.\n\n## Evaluating faster (Applying divide and conquer)\n\nThe problem we wish to solve is as follows. Given a polynomial function $A(x)$ and a set of points $X$, we wish to compute $A(x) \\ \\ \\forall x \\in X$.\n\nLet $A(x) = a_0+a_1x+\\dots+a_dx^{n-1}$\n\nNotice that we can divide our polynomial into two halves, one containing the even powers of $x$ and another containing the odd halves. Let's call them $A_e(x)$ and $A_o(x)$.\n\n$A_e(x) = \\sum_{k=0}^{\\frac{n}{2}-1}a_{2k}x^k$\n\n$A_o(x) = \\sum_{k=0}^{\\frac{n}{2}}a_{2k+1}x^k$\n\nNotice that we aren't raising $x$ to the power of their coefficients. And in doing so, we have effectively cut in half the degree of the polynomial. But in doing so, we have lost the original polynomial. We still require an algebraically correct way to merge these two divisions into the original polynomial.\n\nNotice that if we evaluate $A_e$ at $x^2$ instead of $x$, the algebra checks out. $(x^2)^k = x^{2k}$. Every polynomial term matches its counterpart in the original polynomial. Similarly, we can do the same for $A_0$, but we are now missing a $+1$ in the powers of every term. This can be easily corrected by simply multiplying a single $x$ to the whole polynomial. Similar to Horner's rule. This gives us our final equation,\n\n$$ A(x) = A_e(x^2)+xA_o(x^2) $$\n\nThis has allowed us to effectively calculate the value of $A(x)$ for some point using a technique that uses divide and conquer. But **is** it truly faster than any of the previous algorithms?\n\n### Analyzing time complexity\n\nNotice that\n\n$$ T(n) = 2T(\\frac{n}{2}, |X|)+O(n+|X|) $$\n\nThe $\\frac{n}{2}$ comes from dividing the input to each recurrence in half. We have 2 such recursive calls. These 2 factors account for the first term in the expression. Now, at each \"node\" of our recursive tree, we have do $O(n)$ computation for traversing the polynomial list and splitting it into two halves. And finally, $O(|X|)$ time for computing the polynomial at each $x\\in X$.\n\nTo solve this recurrence, let us imagine the recursion tree. The base case for this recursion is when $n=1$. When $n=1$, the answer is the value in the set itself. However, notice that at no point are we **ever** changing the size of the set $X$. The original size of $X$ was $n$, and it remains $n$ at every step of the algorithm. This will span out to be a binary tree of depth $log_2(n)$, with each node doing $O(n)$ computation.\n\n![fft-1](/images/fft-1.png)\n\n\nAt the bottom most level, notice that we still have order $n$ leaves, each of which are doing order $n$ computation. This will sadly give us a time complexity of $O(n \\times n) = O(n^2)$.\n\nThe reason why every node must do $O(n)$ computation is because we haven't been able to change the size of the set $X$ like we have managed to with $n$. If we could somehow half the size of $X$ just like we did with $n$, we would get a much simpler recurrence. $T(n) = T(\\frac{n}{2})+O(n)$ which evaluates to just $O(nlogn)$. But how can we reduce the size of the set of all points we need to evaluate our polynomial at?\n\n### The final piece of the puzzle\n\nLet's take a look at our equation again\n\n$$ A(x) = A_e(x^2)+xA_o(x^2) $$\n\nIn the recursive call to $A_e$ and $A_o$, we have so far managed to _reduce_ the value of $n$ (no. of terms in the polynomial), by half. But we haven't managed to half the size of $X$, the set of all points we require to evaluate our polynomial at. So let's take our attention off $n$ and think about $x$.\n\nAt every step, or \"node\" of our algorithm, notice that we are passing the value of $x^2$. Another key realization is that, we are **free** to choose any $X$ we want as long as all the points in $X$ are unique.\n\nThis has allowed us to transform the problem of reducing the size of $X$ into a simpler question, _\"Does there exist some $x^2$ for which there are multiple unique roots $x_0$ and $x_1$?\"_\n\nNotice that at least in the real plane, the answer is **no.** Well, it might work for the first \"root node\" of our recursion tree. Every real number except zero satisfies the property that $x^2=(-x)^2$. Hence we can just evaluate the polynomial at some set of points $x$ and $-x$. But in the second level of our recursion, we have a huge problem. $x^2$ will **always** be a _positive_ value. This means, we no longer have positive-negative pairs to work with. Our set $X$ is no longer free to choose. It has the constraint on it that it **must** be all positive. Without our $\\pm x$ pairs, we cannot proceed.\n\n**Breaking out of the real plane**\n\nHere comes the last piece of our puzzle. While the above was true for real numbers, it is **not** true for complex numbers. Let's assume our set at the final depth of its recursion was $X = \\{ 1 \\}$.\n\nFor the set to be halved in the level just above, we require **two** values of $x_0$ and $x_1$ such that $x_0^2=x_1^2=1$.\n\nTwo such values are $-1$ and $+1$. Let's try thinking one level above this. We would require two values $x_0$ and $x_1$ such that $x_0^2=x_1^2=-1$. Two values that fit this equation are $i$ and $-i$.\n\nNotice that we can keep doing this at every step of our recursion, and we would just keep picking the $k^{th}$ roots of $1$ and every level.\n\nThis is the key realization to solving the problem of reducing the size of $X$. By choosing our set $X$ as the set of all the $k^{th}$ roots of unity where $k \\gt log_2n$, we have effectively managed to half the size of $X$ along with $n$ at **every** step of our algorithm. Our recursion tree now looks more like this\n\n![fft-2](/images/fft-2.png)\n\n\nBy simply computing $A_e(x)$ and $A_o(x)$ at $\\frac{n}{2}$ intervals, we can compute the answer at $n$ points. The roots of unity always occur in $\\pm$ pairs and evaluate to the **same** value when squared. This means, we can write it as follows.\n\n$$ A(x) = A_e(x^2) \\pm xA_o(x^2) \\quad \\forall x\\in X , \\text{x is positive} $$\n\nThis has allowed us to transform our original equation for calculating time complexity into he following\n\n$$ T(n) = T(\\frac{n}{2}, \\frac{|X|}{2})+O(n) \\\\ = O(nlogn) $$\n\nWe have managed to come up with an algorithm that can compute the value of some polynomial function $A(x)$ with $n$ terms at every point in some set $X$ of size of the order $n$ in $O(nlogn)$ time.\n\n### Converting back to polynomial form [Interpolation]\n\nNow, we have an algorithm that can **almost** do it all. We can compute form polynomial representation to value representation in just $O(nlogn)$ complexity. We can compute the value of the product of the $2n$ terms and find the value representation of the polynomial product in $O(n)$ complexity. The only thing left is to convert the polynomial obtained back from value form to polynomial form.\n\nWith a little thought, we can use the same FFT algorithm we just came up with to interpolate our values back to give us our polynomial in coefficient form. Let us think about the _original_ equation that we managed to simplify and solve using FFT.\n\n$$ \\begin{bmatrix} P(x_0) \\\\ P(x_1) \\\\ \\vdots \\\\ P(x_d) \\end{bmatrix} = \\begin{bmatrix} 1 \u0026 x_0 \u0026 x_0^2 \u0026 \\dots \u0026 x_0^d \\\\ 1 \u0026 x_1 \u0026 x_1^2 \u0026 \\dots \u0026 x_1^d \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 1 \u0026 x_d \u0026 x_d^2 \u0026 \\dots \u0026 x_d^d \\end{bmatrix} \\begin{bmatrix} p_0 \\\\ p_1 \\\\ \\vdots \\\\ p_d \\end{bmatrix} $$\n\nWe chose a value of $x_i$ such that every $x_i \\in X$, is some $k^{th}$ root of unity. To get our original vector back, we only need to left-multiply the matrix of the $k^{th}$ roots of unity with it's inverse and use FFT to compute the product of the inverse matrix and the values vector.\n\nThis was our choice for the $X$ matrix,\n\n$$ M_n(\\omega) =\\begin{bmatrix}1 \u0026 1 \u0026 1 \u0026 \\dots\u0026 1\\\\1 \u0026\\omega \u0026 \\omega^2 \u0026 \\dots \u0026\\omega^{n-1}\\\\1 \u0026 \\omega^2\u0026 \\omega^4\u0026 \\dots \u0026\\omega^{2(n-1)}\\\\\u0026\u0026\\vdots\\\\1 \u0026 \\omega^j \u0026 \\omega^{2j} \u0026\\dots\u0026\\omega^{(n-1)j} \\\\\u0026\u0026\\vdots\\\\1 \u0026 \\omega ^{n-1}\u0026 \\omega^{2(n-1)} \u0026 \\dots \u0026\\omega ^{(n-1)(n-1)}\\end{bmatrix} $$\n\n$M_n(\\omega)$ is a Vandermonde matrix with the following property that it is invertible only if every choice of $x_i$ is unique. This is true in our case and hence $M_n(\\omega)$ is invertible. Once this proof has been done for the sake of proving correctness, we have a **complete** solution to solve the problem of polynomial multiplication in just $O(nlogn)$ time.\n\n$$ \\text{Compute values of A(x) and B(X) at } 2d+1 \\text{ points using FFT. Multiply the corresponding points with each other to obtain value representation of the product } C(x) \\text{at 2d+1 points. Use reverse FFT to compute the value of the coefficients of } C(x) \\text{ for each of its } 2d+1 \\text{ terms.} $$\n\nThe overall time complexity is $O(nlogn)+O(n)+O(nlogn) = O(nlogn)$\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H\n2. [Divide \u0026 Conquer: FFT - MIT 6.046J OCW - Erik Demaine](https://youtu.be/iTMn0Kt18tg?si=MkE2euxhcF1whuSR)\n",
    "lastmodified": "2024-05-30T09:58:44.328692211+05:30",
    "tags": []
  },
  "/blog/what-is-the-stock-market": {
    "title": "What Is the Stock Market?",
    "content": "# Origins\n## Why Stock Market - A Toy Example\nThe understand the origins of the stock market, we need to understand why there was a need for the stock market in the first place. Let's say you're the owner of a coffee company, you've got a local shop set up and your business is booming. Your method of sourcing the right coffee beans and technique of preparation works! You start thinking bigger and want to expand your local outlet to shops across the country. You need money for this, and so you look for investors (this is too risky for bank loans!). Let's say you get some initial investors Arya, Bharat and Chandan. They give you some amount of capital $C$ in exchange for $S$ shares in your company. Shares are just an written/electronic object that represents an unit of ownership in a company. Each share of your company is initially valued by them at Rs. $C/S$. If there are $S'$ shares of your company in total, your company is valued at Rs. $S' \\times C$. \n\nThings go well, but you realize you need more capital to expand. Your company can now choose to do what is known as an **IPO** (Initial Public Offering) on the public stock market. Now, **any company or individual** who believes your business could be profitable might buy some shares. These shares allow investors on the public market to become partial owners in your business. Their investment also helps your company to grow, and as it generates more profit, more buyers might see potential and start buying your stock. As the demand for you company's shares increases, since the supply is fixed, the price increases. This raises the value of the company's stock that people already own, making a lot of profit for your earlier investors. For your company, increased public exposure on the market serves as publicity and also helps you fund new initiatives. \n\nHowever, the converse is also true. Let's suppose you cut costs at one of your coffee shops, decreasing the quality of coffee and the public get to know about it. The negative news might convince investors that your company might not make much profits in the future, expect your company's share value to decline and to sell their holdings. As stocks are sold and demand for your stock goes down, the price falls, and with it, the company's market value. This could leave investors with big losses, unless the company starts looking profitable again.\n\nThis see-saw of supply and demand is what is responsible for the ever fluctuating prices on the stock market. Companies are under the unavoidable influence of fluctuating market forces, such as the cost of materials, labor, or unexpected new laws, bad publicity or changes in leadership. All these variables cause day to day noise in the market, which can appear to make companies appear more or less successful. The sad reality is that often *appearing to lose value*, often leads to losing investors which in turn makes your company lose actual value. Human confidence in the market has the power to trigger everything from economic booms to financial crisis. \n## The Actual Origin Story\nIn the 1600s, European companies and government had an operation going where the company would set up a colony in a different country, take valuable goods for free or at cheap prices and ship them back home to sell for massive profits. However, there were many situations back then where bad weather, pirates, etc. could take down the ship, incurring huge losses for the ship owners. To fund this whole (sometimes risky) operation, ship owners would find rich investors to pay for the cost of the voyage. If the ship returned, everyone would get a share in the loot and reap profits. If the ship didn't, everyone would have the lost the invested money and made no profits. Oftentimes, the investors would invest in multiple ships at once to spread their risk. \n\nSoon, the Dutch East India Company took this concept to the next level. Instead of doing the investment on a ship-by-ship basis, they began to sell shares of their company. Since they had a fleet of ships under their command, they essentially bypassed the invest-in-multiple ships part of the process for investors. Investors invested in the company, and the company gained access to more funds to carry out more expeditions to India and hence make more profits. Investors were able to sell their shares in the company at local bars, public gathering spots, etc. or just reap **dividends** from the company. Dividends are payouts made by the company to investors (these payments are not mandatory, but can help boost the investor sentiment for the company). This is how the basis for the first major stock market was created. \n\n\u003e The Amsterdam stock exchange is considered to be the oldest '**modern**' securities market in the world. Created shortly after the establishment of the Dutch East India Company in 1602 when equities began trading on a regular basis as a secondary market to trade its shares. - [Wikipedia - Euronext Amsterdam](https://en.wikipedia.org/wiki/Euronext_Amsterdam)\n\n# Is the Stock Market a Force for Good?\n## The Good Stuff\n- Back in the day, just one person used to call all the shots of a business. Vanderbilt and Rockefeller are famous names that come to mind. They exercised tight control over their businesses and built huge empires. But this all began to change in the beginning of the 20th century. Companies like General Motors (GM) and General Electric, etc. started. Companies realized that they could grow a **lot faster** if they opened up the company to the public and had public investments to accelerate growth. \n- Shareholders in the company want to make money, and if they see the company going in the wrong direction, they'll sell their shares. This will lead to more selling and the share price dropping. But risky ambitious ideas might also encourage people to invest and see the stock price go up. This is sort of the whole idea of the stock market as a force for good. It drives companies to **make more profitable decisions** =\u003e They have more money to give back to shareholders \u0026 more money to **create jobs** and **grow the business further**. This is good for everybody.\n- It serves as a vehicle for providing investment opportunities for not just the rich and wealthy investors, but to all people. People's strong belief in the market leads them to invest in the market, which causes more businesses to get the funding they need to create more jobs and make their business better, thereby directly impacting the people who invested by providing them with more job opportunities and better services / quality of life and even profits via dividends / increase in share price. Very similar idea to that described in  [How the Economic Machine Works by Ray Dalio](/blog/how-the-economic-machine-works-by-ray-dalio).\n## The Bad Stuff\n- It's often not the **real value** of companies that drive their stock prices. It's often the most popular stories that people **believe** about those companies. Sometimes these stories are backed by facts, but sometimes it's all just hype or misinformation. There's often no way to calibrate or contain the spread of hype/misinformation. A famous example is the time when the dotcom bubble exploded. There was crazy hype about internet companies, and then when it exploded, there were harsh repercussions. Shareholders lost a lot of money, companies collapsed, leading to a huge loss of jobs and the Great Economic Recession. Essentially the polar opposite of the good cycle. \n- Because corporations are owned by shareholders, and because most of the stock market runs on greed, the **only obligation of businesses is to make profits.** Often, the top executive's (CEO, CTO, etc.) pay is linked to the share price performance. For example, 80% of their pay could be via stock grants. This drives them to make sure that the share price goes up. This could lead the top decision makers in these companies to take decisions like cutting costs, buying back their own shares, etc. to increase the stock prices in the short term to artificially bump up the price. Between 2007-2016, $55\\%$ of companies in the S\u0026P 500 spent their earnings on stock buybacks, $39\\%$ on dividends to investors and only $6\\%$ on jobs and growing the economy. Things which are actually beneficial for company growth and thereby the country's economy. We've evolved to have a shorter term view on shareholder rights versus a longer term view on stakeholder responsibilities. Laying off jobs, cutting costs to deliver inferior products, reducing wages, etc. negatively affects the economy but could be great for driving up a company's profits in the shorter term. But this is what the stock market encourages companies to optimize.\n- As the stock market has grown, so has inequality. In the USA, in 1970 the average CEO made about $22\\times$ more than the average worker. In 2016, this number had grown to become $271\\times$ more. \n\n\u003eWhen the stock market is booming, we're made to believe the economy is booming. In America, the stock market has been booming for nearly 40 years. But if we add up all the good and services bought and sold in the United States (the **actual** economy), that number isn't growing as quickly as it used to $(\\lt 3\\%)$. Wages have hardly budged in decades ($\\$20.19$ in 1965 to $\\$22.49$ 2020) and the average American family's net worth still hasn't really recovered from the Great Recession. ($\\$119$k in 2007 to $\\$78$k in 2016). So what exactly is the stock market measuring? \n\u003e- Quoted from [Explained | The Stock Market | FULL EPISODE | Netflix](https://www.youtube.com/watch?v=ZCFkWDdmXG8\u0026t=4s)\n# References\n1. [How does the stock market work? - Oliver Elfenbaum](https://www.youtube.com/watch?v=p7HKvqRI_Bo)\n2. [Explained | The Stock Market | FULL EPISODE | Netflix](https://www.youtube.com/watch?v=ZCFkWDdmXG8\u0026t=4s)\n3. [Wikipedia - Euronext Amsterdam](https://en.wikipedia.org/wiki/Euronext_Amsterdam)\n4. [How The Economic Machine Works by Ray Dalio](https://www.youtube.com/watch?v=PHe0bXAIuk0)\n",
    "lastmodified": "2024-05-30T09:58:44.332025554+05:30",
    "tags": []
  },
  "/blog/wilson-s-theorem-fermat-s-little-theorem-euler-s-totient-function": {
    "title": "Wilson's Theorem, Fermat's Little Theorem \u0026 Euler's Totient Function",
    "content": "Last time, we covered the [Extended Euclidean Algorithm](/blog/extended-euclidean-algorithm). Now, we'll delve into some cooler number theory. \n# Wilson's Theorem\nWilson's Theorem states that for any number $p$, the following congruence holds $\\iff p$ is prime:\n$$(p-1)! \\equiv -1 \\pmod{p}$$\n## Proof\n### Proof for composite numbers\nWe can prove that this statement does not hold for any composite $p$ easily. Let $p$ be a composite number $\\gt 2$. Then $p$ can be represented as the product of two numbers $a \\cdot b = p$ for some $1 \\leq a \\leq b \\lt p$. Note that this means that $a \\mid (p-1)!$, hence $(p-1)! \\equiv 0 \\pmod{a}$. But if $a \\mid p$ and $(p-1)! \\equiv 0 \\pmod{a}$, then $(p-1)! \\equiv -1 \\pmod{p}$ cannot be true. $(p-1)! \\pmod{p}$ must be 0. This is a contradiction. \nTherefore, if $p$ is composite, $(p-1)! \\not \\equiv 0 \\pmod{p}$. Similarly, if the equivalence is -1, then $p$ cannot be composite. \n### Proof for prime numbers\nLet's prove the case for $p = 2$ first. $(2-1)! = 1! \\equiv -1 \\pmod{2}$ is seen trivially. Now we will prove for all odd primes $p$. \nNote that in $Z_{p+} = \\{1, 2, 3, \\ldots, p-1\\}$, $\\forall x \\in Z_{p+}, \\ \\exists \\ x' \\mid x \\cdot x' \\equiv 1 \\pmod{p}$. This is essentially the existence of an inverse. Also note that the inverse must always be unique for each $x \\in Z_{p+}$. Now, there are two possible cases, $x = x'$ or $x \\neq x'$. \n\nLet's assume $x = x'$.  Then, \n$$\n\\begin{aligned}\nx \\cdot x' \\equiv 1 \\pmod{p} \\\\\nx^2 \\equiv 1 \\pmod{p} \\\\\n\\implies x \\equiv \\pm1 \\pmod{p} \\\\ \n\\implies x = 1 \\ \\lor \\ x = p-1\n\\end{aligned}\n$$\nTherefore, the only two elements in this field with inverses equivalent to themselves are $1$ and $p-1$. Now, let's consider the entire product of $(p-1)!$. \n$$\n\\begin{aligned}\n(p-1)! \\pmod{p} \\equiv (p-1)\\cdot(p-2)\\cdot(p-3)\\cdots1 \\pmod{p} \\\\\n\\text{Pairing off all the other elements with their unique inverses gives us} \\\\\n(p-1)! \\equiv 1\\cdot (p-1) \\pmod{p} \\\\\n\\implies (p-1)! \\equiv -1 \\pmod{p}\n\\end{aligned}\n$$\nHence we have proved Wilson's theorem. \n# Fermat's Little Theorem\n\nFermat's little theorem states the following:\n\n_If $p$ is a prime number, then for any integer $a$, the number $a^p -a$ is an integer multiple of $p$._\n\nIn other words,\n\n$$ a^p\\equiv a(mod \\ p) $$\n\nFurther, if $a$ is not divisible by $p$, then\n\n$$ a^{p-1} \\equiv 1(mod \\ p) $$\n\n\u003e Fun fact, this theorem is used to come up with a **very** accurate probabilistic [Randomization, Primality Testing Algorithms](/blog/randomization-primality-testing-algorithms)!\n## Proof\nThe proof is as follows: Consider the set $Z_p = \\{1, 2, 3, \\ldots, p-1\\}$, which contains all the non-zero integers modulo $p$. Let's construct the following equation and work on rearranging / substituting terms. \n$$\n\\begin{aligned} \n(a \\cdot 1)(a \\cdot 2)(a \\cdot 3) \\cdots (a \\cdot (p-1)) \u0026\\equiv a^{p-1} \\cdot (1 \\cdot 2 \\cdot 3 \\cdots (p-1)) \\pmod{p} \\\\ \u0026\\equiv a^{p-1} \\cdot (p-1)! \\pmod{p} \n\\end{aligned}$$ By Wilson's Theorem, we know that $(p-1)! \\equiv -1 \\pmod{p}$ for any prime $p$. Substituting this, we get: $$a^{p-1} \\cdot (p-1)! \\equiv a^{p-1} \\cdot (-1) \\pmod{p}$$ Therefore, we have: $$a^{p-1} \\cdot (-1) \\equiv -a^{p-1} \\pmod{p}$$ Rearranging the terms, we get: $$a^{p} - 1 \\equiv 0 \\pmod{p}$$ This can be rewritten as: $$a^{p} \\equiv a \\pmod{p}$$ Thus, we have proved Fermat's Little Theorem.\n# Euler's Totient Function\n\nEuler came along later and gave a more generalized version of Fermat's little theorem. He stated that for _any_ modulus $n$ and any integer $a$ co-prime to $n$, the following holds true.\n\n$$ a^{\\phi(n)} \\equiv 1 (mod \\ n) $$\n\nHere, $\\phi(n)$ is known as **Euler's Totient function.** It counts the number of integers between 1 and $n$ inclusive, which are co-prime to n. Or in simpler words, it is equivalent to the number of numbers less than $n$ that do not share any divisors with $n$.\n\n**Some interesting properties:**\n\n1. Notice that for any prime number $p$, $\\phi (p) = p-1$. By virtue of being prime, $p$ does not share any factor with any number less than itself.\n    \n2. The totient function is a **multiplicative function**. This is not a trivial thing to see and follows from the Chinese remainder theorem. This [stack link](https://math.stackexchange.com/questions/192452/whats-the-proof-that-the-euler-totient-function-is-multiplicative) has a really nice write up of the proof. This property essentially means that for relatively prime $a$ and $b$,\n    \n    $$ \\phi (ab) = \\phi(a)\\cdot\\phi(b) $$\n    \n\nNotice that Fermat's is indeed a special case of this theorem. When $n$ is prime, we get Fermat's little theorem.\n\nFurther, just like factorization, computing the value of $\\phi(n)$ is a **hard** problem. However, notice that if the factorization of $n$ is known, we can compute the value easily. We can simply write $n$ in terms of its prime products and write $\\phi(n) = \\phi(p_1) \\cdot \\phi(p_2)\\cdots \\phi(p_n)$. And it is easy to compute $\\phi(p)$ where $p$ is prime.\n\n# References\nThese notes are old and I did not rigorously horde references back then. If some part of this content is your's or you know where it's from then do reach out to me and I'll update it. \n1. Professor [Kannan Srinathan's](https://www.iiit.ac.in/people/faculty/srinathan/) course on Algorithm Analysis \u0026 Design in IIIT-H",
    "lastmodified": "2024-05-30T09:58:44.332025554+05:30",
    "tags": []
  },
  "/interests": {
    "title": "interests",
    "content": "+++\ntitle = 'Interests'\ndate = 2023-01-01T08:00:00-07:00\n+++\n\n# Academic\n\n## Competitive Programming\nI started competitive programming when I joined IIIT in 2020. I actively participated in contests during my college years, and was peak rated Master on Codeforces ([akcube](https://codeforces.com/profile/akcube) - 2114) (active India rank ~50). I also participated in [ICPC](https://icpc.global/) during this time. My team *fightFight* qualified for the Asia-West continent finals in 2022, where we ranked 19th. We lost World Finals due to missing a single \"-1\" in code, but that's something I'll keep crying about :) We've also placed 9th in the Amritapuri regional in 2023.\n\n![fightFight.jpg](/images/fightFight.jpg)\n\nI have also been the [IIIT-H Programming Club](https://iiit-programming-club.github.io/) coordinator for 4 years. During this time, I designed and hosted the programming club website. I also was part of the team that set 2 contests on Codeforces ([Codeforces Round 940 (Div. 2) and CodeCraft-23](https://codeforces.com/contest/1957) and [CodeCraft-22 and Codeforces Round 795 (Div. 2)](https://codeforces.com/contest/1691) which had around ~20k and ~30k participants respectively. Apart from this, I also founded the [Indian Competitive Programming Camp](https://iiit-programming-club.github.io/ICPC-Camp/What-is-camp) and successfully completed it's first iteration, garnering over 2k participants across the country's best institutes. \n\n## High Performance Computing\nI was also one of the members of the [IIIT-H Theory Group](https://iiittheorygroup.github.io/) where I primarily pursued my interests in HPC \u0026 systems. I'm a huge fan of C++, and have contributed to implementing `par_unseq` and `unseq` execution policies for a `stdlib` implementation during GSoC - [Project Outline](https://docs.google.com/document/d/1GH7ryQ8q1yo4xq-kMCB_oCAOZxEkc7ky_sitcAyZrmQ/edit?usp=sharing). I also worked on heavily optimizing my own BLAS implementation, a journey which I documented [here](https://www.notion.so/Optimizing-kBLAS-f7d754020cc2452f8fdfdbc760e458e1) (Will hopefully move this out of notion soon!). Apart from this, I've also worked on optimizing algorithms in the Bio-Informatics space for efficient execution using SIMD. One example is my work with Vidit Jain on achieving a 33x performance increase from row-optimized Needleman-Wunsch, which you can find [here](https://github.com/akcube/Sewing-machine/). Am currently working on optimizing the [STAR](https://github.com/alexdobin/STAR) DNA Sequence Aligner with [Intel](https://intel.com/).\n\n## Teaching\nI first heard this from [Anurudh Peduri](https://anurudhp.github.io/), \n\n\u003e “Those who can, do; those who can’t, teach.” - George Bernard Shaw\n\nJokes aside, I do love teaching. I've taken multiple lectures on topics ranging from C++ STL to Network Flow algorithms to audiences of 50-100+ people as part of Programming Club. Some of these meets may have been recorded / for a national audience as part of the ICPC Camp I conducted. Ex: [Centroid Decomposition, Auxillary Trees - Kishore Kumar](https://www.youtube.com/live/9lNo4PjvezA?si=TcFuKn6KwhsOOMbu). Apart from that, I've also taken a few meets on Bloom Filters, [HPC](https://drive.google.com/file/d/1lK6ndOp11hi1p3X1eWsFyTcvg0xQafMs/view), etc.  \n\n## Trading / ML\nNew interest. I've been trading for a couple of years now and done pretty decent. Exploring some famous problems in this space (asset allocation, options theory, etc.) to see if I can build something cool. Very much a WIP.\n\n# Hobbies\n\n## Gaming\nI love playing Counter Strike \u0026 Rocket League. Started FPS journey with a game called Black Squad. I total some 2000 hours across FPS games. My peak rank was Nova-3 in CS:GO and Diamond 2 in Rocket League. Am considerably worse in both at the moment :( I know mobile gaming isn't gaming, but I did hit Champion 2 in RL Sideswipe! Apart from that, recently started playing chess. I'm terrible at it, but might grind for a bit.\n\n## Sports\nI played football quite actively in school and was one of the goalkeepers for my school cluster team. In college, I was a part of our house team and we won the house cup in my second year. I dropped off after that. I've tried volleyball and quite like it, but don't pursue it competitively. I don't watch sports actively but do keep tabs on my favorite teams: Real Madrid (Football), CSK (IPL) \u0026 India (World Cup / Test). \n\n## Dance\nI love dancing. I find it hard to allocate time for this sadly, but love performing on a big stage given the opportunity. Here's the first and only time I really got to go all out on a stage. That's Manav on the left, huge props to him, I'm not trained by any means so he played a huge part in making this happen.  \n\n\u003ciframe title=\"IIIT - Navrang Performance\" src=\"https://drive.google.com/file/d/10kRnPMCiEbJaTRam09QHafovPyNq4IEl/preview\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 35%; height: 35%;\"\u003e\u003c/iframe\u003e\n\n",
    "lastmodified": "2024-05-30T05:07:16.543635129+05:30",
    "tags": []
  }
}